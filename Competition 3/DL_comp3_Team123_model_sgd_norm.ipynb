{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLab Cup 3: Reverse Image Caption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow the tutorial in this page:\n",
    "# https://www.endtoend.ai/tutorial/how-to-download-kaggle-datasets-on-ubuntu/\n",
    "!cat ~/.kaggle/kaggle.json\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "!kaggle competitions download -c datalab-cup3-reverse-image-caption-2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "# zipfile example\n",
    "def zip_list(file_path):\n",
    "    zf = zipfile.ZipFile(file_path, 'r')\n",
    "    zf.extractall('./')\n",
    "\n",
    "file_path = './datalab-cup3-reverse-image-caption-2021.zip'\n",
    "zip_list(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and Packages Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # disable warnings, info and errors \n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Dec 30 21:39:40 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 465.19.01    Driver Version: 465.19.01    CUDA Version: 11.3     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\n",
      "| 48%   57C    P8    17W / 250W |     21MiB / 11176MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:02:00.0 Off |                  N/A |\n",
      "|  0%   50C    P8    13W / 250W |   2413MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1140      G   /usr/lib/xorg/Xorg                 16MiB |\n",
      "|    1   N/A  N/A     12164      C   ...evin/anaconda3/bin/python     2409MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the first GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        #for gpu in gpus:\n",
    "        #    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 5427 vocabularies in total\n",
      "Word to id mapping, for example: flower -> 1\n",
      "Id to word mapping, for example: 1 -> flower\n",
      "Tokens: <PAD>: 5427; <RARE>: 5428\n"
     ]
    }
   ],
   "source": [
    "dictionary_path = './dictionary'\n",
    "vocab = np.load(dictionary_path + '/vocab.npy')\n",
    "print('there are {} vocabularies in total'.format(len(vocab)))\n",
    "\n",
    "word2Id_dict = dict(np.load(dictionary_path + '/word2Id.npy'))\n",
    "id2word_dict = dict(np.load(dictionary_path + '/id2Word.npy'))\n",
    "print('Word to id mapping, for example: %s -> %s' % ('flower', word2Id_dict['flower']))\n",
    "print('Id to word mapping, for example: %s -> %s' % ('1', id2word_dict['1']))\n",
    "print('Tokens: <PAD>: %s; <RARE>: %s' % (word2Id_dict['<PAD>'], word2Id_dict['<RARE>']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the flower shown has yellow anther red pistil and bright red petals.\n",
      "['9', '1', '82', '5', '11', '70', '20', '31', '3', '29', '20', '2', '5427', '5427', '5427', '5427', '5427', '5427', '5427', '5427']\n"
     ]
    }
   ],
   "source": [
    "def sent2IdList(line, MAX_SEQ_LENGTH=20):\n",
    "    MAX_SEQ_LIMIT = MAX_SEQ_LENGTH\n",
    "    padding = 0\n",
    "    \n",
    "    # data preprocessing, remove all puntuation in the texts\n",
    "    prep_line = re.sub('[%s]' % re.escape(string.punctuation), ' ', line.rstrip())\n",
    "    prep_line = prep_line.replace('-', ' ')\n",
    "    prep_line = prep_line.replace('-', ' ')\n",
    "    prep_line = prep_line.replace('  ', ' ')\n",
    "    prep_line = prep_line.replace('.', '')\n",
    "    tokens = prep_line.split(' ')\n",
    "    tokens = [\n",
    "        tokens[i] for i in range(len(tokens))\n",
    "        if tokens[i] != ' ' and tokens[i] != ''\n",
    "    ]\n",
    "    l = len(tokens)\n",
    "    padding = MAX_SEQ_LIMIT - l\n",
    "    \n",
    "    # make sure length of each text is equal to MAX_SEQ_LENGTH, and replace the less common word with <RARE> token\n",
    "    for i in range(padding):\n",
    "        tokens.append('<PAD>')\n",
    "    line = [\n",
    "        word2Id_dict[tokens[k]]\n",
    "        if tokens[k] in word2Id_dict else word2Id_dict['<RARE>']\n",
    "        for k in range(len(tokens))\n",
    "    ]\n",
    "\n",
    "    return line\n",
    "\n",
    "text = \"the flower shown has yellow anther red pistil and bright red petals.\"\n",
    "print(text)\n",
    "print(sent2IdList(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7370 image in training data\n"
     ]
    }
   ],
   "source": [
    "data_path = './dataset'\n",
    "df = pd.read_pickle(data_path + '/text2ImgData.pkl')\n",
    "num_training_sample = len(df)\n",
    "n_images_train = num_training_sample\n",
    "print('There are %d image in training data' % (n_images_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Captions</th>\n",
       "      <th>ImagePath</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6734</th>\n",
       "      <td>[[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...</td>\n",
       "      <td>./102flowers/image_06734.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6736</th>\n",
       "      <td>[[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...</td>\n",
       "      <td>./102flowers/image_06736.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6737</th>\n",
       "      <td>[[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...</td>\n",
       "      <td>./102flowers/image_06737.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6738</th>\n",
       "      <td>[[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...</td>\n",
       "      <td>./102flowers/image_06738.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6739</th>\n",
       "      <td>[[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...</td>\n",
       "      <td>./102flowers/image_06739.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Captions  \\\n",
       "ID                                                        \n",
       "6734  [[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...   \n",
       "6736  [[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...   \n",
       "6737  [[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...   \n",
       "6738  [[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...   \n",
       "6739  [[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...   \n",
       "\n",
       "                         ImagePath  \n",
       "ID                                  \n",
       "6734  ./102flowers/image_06734.jpg  \n",
       "6736  ./102flowers/image_06736.jpg  \n",
       "6737  ./102flowers/image_06737.jpg  \n",
       "6738  ./102flowers/image_06738.jpg  \n",
       "6739  ./102flowers/image_06739.jpg  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this competition, you have to generate image in size 64x64x3\n",
    "IMAGE_HEIGHT = 64\n",
    "IMAGE_WIDTH = 64\n",
    "IMAGE_CHANNEL = 3\n",
    "MAPPING_FUNC = None\n",
    "\n",
    "def vertical_flip(tf_img):\n",
    "    return tf.image.flip_left_right(tf_img)\n",
    "\n",
    "def horizontal_flip(tf_img):\n",
    "    return tf.image.flip_up_down(tf_img)\n",
    "\n",
    "def brightness(tf_img):\n",
    "    return tf.image.random_brightness(tf_img, 0.2, 2)\n",
    "\n",
    "def data_generator(caption, image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.cast(img, tf.float32)\n",
    "    img = (img - 127.5) / 127.5\n",
    "    # img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # img = tf.image.per_image_standardization(img)\n",
    "    img.set_shape([None, None, 3])\n",
    "    \n",
    "    if MAPPING_FUNC is not None:\n",
    "        img = MAPPING_FUNC(img)\n",
    "\n",
    "    img = tf.image.resize(img, size=[IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "    img.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL])\n",
    "    caption = tf.cast(caption, tf.int32)    \n",
    "    return img, caption\n",
    "\n",
    "def transform_dataset(caption, image_path):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((caption, image_path))\n",
    "    dataset = dataset.map(data_generator, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.shuffle(len(caption))\n",
    "    return dataset\n",
    "\n",
    "def dataset_generator(filenames, batch_size):\n",
    "    # load the training data into two NumPy arrays\n",
    "    df = pd.read_pickle(filenames)\n",
    "    captions = df['Captions'].values\n",
    "    caption = []\n",
    "    # each image has 1 to 10 corresponding captions\n",
    "    # we choose one of them randomly for training\n",
    "    for i in range(len(captions)):\n",
    "        caption.append(random.choice(captions[i]))\n",
    "    caption = np.asarray(caption)\n",
    "    caption = caption.astype(np.int64)\n",
    "    image_path = df['ImagePath'].values\n",
    "    \n",
    "    # assume that each row of features corresponds to the same row as labels.\n",
    "    assert caption.shape[0] == image_path.shape[0]\n",
    "    \n",
    "    original_dataset = transform_dataset(caption, image_path)\n",
    "\n",
    "    MAPPING_FUNC = vertical_flip\n",
    "    vertical_dataset = transform_dataset(caption, image_path)\n",
    "\n",
    "    MAPPING_FUNC = horizontal_flip\n",
    "    horizontal_dataset = transform_dataset(caption, image_path)\n",
    "\n",
    "    MAPPING_FUNC = brightness\n",
    "    brightness_dataset = transform_dataset(caption, image_path)\n",
    "    \n",
    "    dataset = original_dataset.concatenate(vertical_dataset)\n",
    "    dataset = dataset.concatenate(horizontal_dataset)\n",
    "    dataset = dataset.concatenate(brightness_dataset)\n",
    "    dataset = dataset.shuffle(len(caption)).batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "dataset = dataset_generator(data_path + '/text2ImgData.pkl', BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparas = {\n",
    "    'MAX_SEQ_LENGTH': 20,                     # maximum sequence length\n",
    "    'EMBED_DIM': 256,                         # word embedding dimension\n",
    "    'VOCAB_SIZE': len(word2Id_dict),          # size of dictionary of captions\n",
    "    'RNN_HIDDEN_SIZE': 128,                   # number of RNN neurons\n",
    "    'Z_DIM': 512,                             # random noise z dimension\n",
    "    'DENSE_DIM': 128,                         # number of neurons in dense layer\n",
    "    'IMAGE_SIZE': [64, 64, 3],                # render image size\n",
    "    'BATCH_SIZE': 64,\n",
    "    'LR': 1e-4,\n",
    "    'LR_DECAY': 0.5,\n",
    "    'BETA_1': 0.5,\n",
    "    'N_EPOCH': 1000,\n",
    "    'N_SAMPLE': num_training_sample,          # size of training data\n",
    "    'CHECKPOINTS_DIR': './checkpoints/demo',  # checkpoint path\n",
    "    'PRINT_FREQ': 1                           # printing frequency of loss\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditioinal GAN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Encode text (a caption) into hidden representation\n",
    "    input: text, which is a list of ids\n",
    "    output: embedding, or hidden representation of input text in dimension of RNN_HIDDEN_SIZE\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        self.batch_size = self.hparas['BATCH_SIZE']\n",
    "        \n",
    "        # embedding with tensorflow API\n",
    "        self.embedding = layers.Embedding(self.hparas['VOCAB_SIZE'], self.hparas['EMBED_DIM'])\n",
    "        # RNN, here we use GRU cell, another common RNN cell similar to LSTM\n",
    "        self.gru = layers.GRU(self.hparas['RNN_HIDDEN_SIZE'],\n",
    "                              return_sequences=True,\n",
    "                              return_state=True,\n",
    "                              recurrent_initializer='glorot_uniform')\n",
    "\n",
    "        # self.bidirect = layers.Bidirectional(layers.LSTM(128))\n",
    "        # self.d1 = layers.Dense(128, activation=\"relu\")\n",
    "    \n",
    "    def call(self, text, hidden):\n",
    "        text = self.embedding(text)\n",
    "        output, state = self.gru(text, initial_state = hidden)\n",
    "        # output = self.bidirect(text)\n",
    "        # output = self.d1(output)\n",
    "        return output[:, -1, :], state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.hparas['BATCH_SIZE'], self.hparas['RNN_HIDDEN_SIZE']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Generate fake image based on given text(hidden representation) and noise z\n",
    "    input: text and noise\n",
    "    output: fake image with size 64*64*3\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas):\n",
    "        super(Generator, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.d1 = tf.keras.layers.Dense(self.hparas['DENSE_DIM'])\n",
    "        self.d2 = tf.keras.layers.Dense(64*64*3)\n",
    "\n",
    "        self.d3 = layers.Dense(8*8*512, use_bias=False)\n",
    "        self.bn1= layers.BatchNormalization()\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.bn3 = layers.BatchNormalization()\n",
    "        self.bn4 = layers.BatchNormalization()\n",
    "        self.dropout = layers.Dropout(0.5)\n",
    "\n",
    "        self.Conv2DTrans1 = layers.Conv2DTranspose(256, (5, 5), strides=(1, 1), padding=\"same\", use_bias=False)\n",
    "        self.Conv2DTrans2 = layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding=\"same\", use_bias=False)\n",
    "        self.Conv2DTrans3 = layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding=\"same\", use_bias=False)\n",
    "        self.Conv2DTrans4 = layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding=\"same\", use_bias=False)\n",
    "        \n",
    "    def call(self, text, noise_z):\n",
    "        text = self.flatten(text)\n",
    "        text = self.d1(text)\n",
    "        text = tf.nn.leaky_relu(text)\n",
    "        \n",
    "        # concatenate input text and random noise\n",
    "        text_concat = tf.concat([noise_z, text], axis=1)\n",
    "        text_concat = self.d3(text_concat)\n",
    "        text_concat = self.bn1(text_concat)\n",
    "        text_concat = tf.reshape(text_concat, [-1, 8, 8, 512])\n",
    "        \n",
    "        text_concat = self.Conv2DTrans1(text_concat)\n",
    "        text_concat = self.bn2(text_concat)\n",
    "        text_concat = tf.nn.leaky_relu(text_concat)\n",
    "        text_concat = self.dropout(text_concat)\n",
    "\n",
    "        text_concat = self.Conv2DTrans2(text_concat)\n",
    "        text_concat = self.bn3(text_concat)\n",
    "        text_concat = tf.nn.leaky_relu(text_concat)\n",
    "        text_concat = self.dropout(text_concat)\n",
    "\n",
    "        text_concat = self.Conv2DTrans3(text_concat)\n",
    "        text_concat = self.bn4(text_concat)\n",
    "        text_concat = tf.nn.leaky_relu(text_concat)\n",
    "        text_concat = self.dropout(text_concat)\n",
    "\n",
    "        text_concat = self.Conv2DTrans4(text_concat)\n",
    "\n",
    "        logits = tf.reshape(text_concat, [-1, 64, 64, 3])\n",
    "        output = tf.nn.tanh(logits)\n",
    "\n",
    "        return logits, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Differentiate the real and fake image\n",
    "    input: image and corresponding text\n",
    "    output: labels, the real image should be 1, while the fake should be 0\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.d_text = tf.keras.layers.Dense(self.hparas['DENSE_DIM'])\n",
    "        self.d_img = tf.keras.layers.Dense(self.hparas['DENSE_DIM'])\n",
    "        self.d = tf.keras.layers.Dense(1)\n",
    "\n",
    "        self.Conv2D1 = layers.Conv2D(64, (5, 5), strides=(2, 2), padding=\"same\")\n",
    "        self.bn = layers.BatchNormalization()\n",
    "        self.dropout = layers.Dropout(0.3)\n",
    "\n",
    "        self.Conv2D2 = layers.Conv2D(128, (5, 5), strides=(2, 2), padding=\"same\")\n",
    "        self.Conv2D3 = layers.Conv2D(256, (5, 5), strides=(2, 2), padding=\"same\")\n",
    "\n",
    "    def call(self, img, text):\n",
    "        text = self.flatten(text)\n",
    "        text = self.d_text(text)\n",
    "        text = tf.nn.leaky_relu(text)\n",
    "        \n",
    "        img = self.Conv2D1(img)\n",
    "        img = tf.nn.leaky_relu(img)\n",
    "        img = self.dropout(img)\n",
    "\n",
    "        img = self.Conv2D2(img)\n",
    "        img = tf.nn.leaky_relu(img)\n",
    "        img = self.dropout(img)\n",
    "\n",
    "        img = self.Conv2D3(img)\n",
    "        img = tf.nn.leaky_relu(img)\n",
    "        img = self.dropout(img)\n",
    "\n",
    "        img = self.flatten(img)\n",
    "        img = self.d_img(img)\n",
    "        \n",
    "        # concatenate image with paired text\n",
    "        img_text = tf.concat([text, img], axis=1)\n",
    "        \n",
    "        logits = self.d(img_text)\n",
    "        output = tf.nn.sigmoid(logits)\n",
    "        \n",
    "        return logits, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = TextEncoder(hparas)\n",
    "generator = Generator(hparas)\n",
    "discriminator = Discriminator(hparas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_logits, fake_logits):\n",
    "    # output value of real image should be 1\n",
    "    real_loss = cross_entropy(tf.ones_like(real_logits), real_logits)\n",
    "    # output value of fake image should be 0\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_logits), fake_logits)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    # output value of fake image should be 0\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use seperated optimizers for training generator and discriminator\n",
    "generator_optimizer = tf.keras.optimizers.Adam(hparas['LR'])\n",
    "discriminator_optimizer = tf.keras.optimizers.SGD(hparas['LR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one benefit of tf.train.Checkpoint() API is we can save everything seperately\n",
    "checkpoint_dir = hparas['CHECKPOINTS_DIR']\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 text_encoder=text_encoder,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def DC_DTrain(image, caption, hidden, noise):\n",
    "    # z = tf.random.normal(hparas['BZ'])\n",
    "    noise_g = tf.random.normal(shape=[hparas['BATCH_SIZE'], hparas['Z_DIM']], mean=0.0, stddev=1.0)\n",
    "\n",
    "    with tf.GradientTape() as tp:\n",
    "        with tf.GradientTape() as tp_gp:\n",
    "            text_embed, hidden = text_encoder(caption, hidden)\n",
    "            # _, fake_image = generator(text_embed, noise)\n",
    "            # real_logits, real_output = discriminator(image, text_embed)\n",
    "            # fake_logits, fake_output = discriminator(fake_image, text_embed)\n",
    "\n",
    "            _, x_bar = generator(text_embed, noise_g, training = True)\n",
    "            epsilon = tf.random.uniform([BATCH_SIZE,1,1,1])\n",
    "            x = image\n",
    "            x_hat = epsilon * x + (1. - epsilon) * x_bar\n",
    "\n",
    "            x_bar = x_bar + noise * tf.random.normal(x_bar.shape)\n",
    "            x = x + noise * tf.random.normal(x.shape)\n",
    "            x_hat = x_hat + noise * tf.random.normal(x_hat.shape)\n",
    "\n",
    "            z0, _ = discriminator(x_bar, text_embed, training = True)\n",
    "            z1, _ = discriminator(x, text_embed, training = True)\n",
    "            z2, _ = discriminator(x_hat, text_embed, training = True)\n",
    "\n",
    "            gradient_penalty = tp_gp.gradient(z2,x_hat)\n",
    "            gradient_penalty = tf.sqrt(tf.reduce_sum(tf.math.square(gradient_penalty),axis=[1,2,3]))\n",
    "            loss = z0 - z1 + 10. * tf.math.square((gradient_penalty - 1.))\n",
    "            ld = tf.reduce_mean(loss)\n",
    "            lg = - tf.reduce_mean(z0)\n",
    "\n",
    "    gradient_d = tp.gradient(ld, discriminator.trainable_variables)\n",
    "\n",
    "    discriminator_optimizer.apply_gradients(zip(gradient_d, discriminator.trainable_variables))\n",
    "\n",
    "    return lg, ld\n",
    "\n",
    "@tf.function\n",
    "def DC_GTrain(image, caption, hidden, noise):\n",
    "    # z = tf.random.normal(hparas['BZ'])\n",
    "    noise_g = tf.random.normal(shape=[hparas['BATCH_SIZE'], hparas['Z_DIM']], mean=0.0, stddev=1.0)\n",
    "\n",
    "    with tf.GradientTape() as tp:\n",
    "        with tf.GradientTape() as tp_gp:\n",
    "            text_embed, hidden = text_encoder(caption, hidden)\n",
    "            # _, fake_image = generator(text_embed, noise)\n",
    "            # real_logits, real_output = discriminator(image, text_embed)\n",
    "            # fake_logits, fake_output = discriminator(fake_image, text_embed)\n",
    "\n",
    "            _, x_bar = generator(text_embed, noise_g, training = True)\n",
    "            epsilon = tf.random.uniform([BATCH_SIZE,1,1,1])\n",
    "            x = image\n",
    "            x_hat = epsilon * x + (1. - epsilon) * x_bar\n",
    "\n",
    "            x_bar = x_bar + noise * tf.random.normal(x_bar.shape)\n",
    "            x = x + noise * tf.random.normal(x.shape)\n",
    "            x_hat = x_hat + noise * tf.random.normal(x_hat.shape)\n",
    "\n",
    "            z0, _ = discriminator(x_bar, text_embed, training = True)\n",
    "            z1, _ = discriminator(x, text_embed, training = True)\n",
    "            z2, _ = discriminator(x_hat, text_embed, training = True)\n",
    "\n",
    "            gradient_penalty = tp_gp.gradient(z2,x_hat)\n",
    "            gradient_penalty = tf.sqrt(tf.reduce_sum(tf.math.square(gradient_penalty),axis=[1,2,3]))\n",
    "            loss = z0 - z1 + 10. * tf.math.square((gradient_penalty - 1.))\n",
    "            ld = tf.reduce_mean(loss)\n",
    "            lg = - tf.reduce_mean(z0)\n",
    "\n",
    "    gradient_g = tp.gradient(lg, generator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradient_g, generator.trainable_variables))\n",
    "\n",
    "    return lg, ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(caption, noise, hidden):\n",
    "    text_embed, hidden = text_encoder(caption, hidden)\n",
    "    _, fake_image = generator(text_embed, noise)\n",
    "    return fake_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "DC_Train = (\n",
    "    DC_DTrain,\n",
    "    DC_DTrain,\n",
    "    DC_DTrain,\n",
    "    DC_DTrain,\n",
    "    DC_DTrain,\n",
    "    DC_GTrain,\n",
    ")\n",
    "\n",
    "DC_Critic = len(DC_Train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visulization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1], 3))\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w, :] = image\n",
    "    return img\n",
    "\n",
    "def imsave(images, size, path):\n",
    "    # getting the pixel values between [0, 1] to save it\n",
    "    return plt.imsave(path, merge(images, size)*0.5 + 0.5)\n",
    "\n",
    "def save_images(images, size, image_path):\n",
    "    return imsave(images, size, image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_generator(caption, batch_size):\n",
    "    caption = np.asarray(caption)\n",
    "    caption = caption.astype(np.int64)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(caption)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ni = int(np.ceil(np.sqrt(hparas['BATCH_SIZE'])))\n",
    "sample_size = hparas['BATCH_SIZE']\n",
    "sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, hparas['Z_DIM'])).astype(np.float32)\n",
    "sample_sentence = [\"the flower shown has yellow anther red pistil and bright red petals.\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower has petals that are yellow, white and purple and has dark lines\"] * int(sample_size/ni) + \\\n",
    "                  [\"the petals on this flower are white with a yellow center\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower has a lot of small round pink petals.\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower is orange in color, and has petals that are ruffled and rounded.\"] * int(sample_size/ni) + \\\n",
    "                  [\"the flower has yellow petals and the center of it is brown.\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower has petals that are blue and white.\"] * int(sample_size/ni) +\\\n",
    "                  [\"these white flowers have petals that start off white in color and end in a white towards the tips.\"] * int(sample_size/ni)\n",
    "\n",
    "for i, sent in enumerate(sample_sentence):\n",
    "    sample_sentence[i] = sent2IdList(sent)\n",
    "sample_sentence = sample_generator(sample_sentence, hparas['BATCH_SIZE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('samples/demo'):\n",
    "    os.makedirs('samples/demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    # hidden state of RNN\n",
    "    hidden = text_encoder.initialize_hidden_state()\n",
    "    steps_per_epoch = int(hparas['N_SAMPLE']/hparas['BATCH_SIZE'])\n",
    "    \n",
    "    ctr = 0\n",
    "    for epoch in range(hparas['N_EPOCH']):\n",
    "        g_total_loss = 0\n",
    "        d_total_loss = 0\n",
    "        start = time.time()\n",
    "        \n",
    "        if epoch < 200:\n",
    "            noise = 1.0 / float(epoch + 1)\n",
    "        else:\n",
    "            noise = 0.0\n",
    "        \n",
    "        for image, caption in dataset:\n",
    "            lg, ld = DC_Train[ctr](image, caption, hidden, noise) # (real_img, embed, noise_decay)\n",
    "            ctr += 1\n",
    "            g_total_loss += lg.numpy()\n",
    "            d_total_loss += ld.numpy()\n",
    "            if ctr == DC_Critic : ctr = 0\n",
    "            \n",
    "        time_tuple = time.localtime()\n",
    "        time_string = time.strftime(\"%m/%d/%Y, %H:%M:%S\", time_tuple)\n",
    "            \n",
    "        print(\"Epoch {}, gen_loss: {:.4f}, disc_loss: {:.4f}\".format(epoch+1,\n",
    "                                                                     g_total_loss/steps_per_epoch,\n",
    "                                                                     d_total_loss/steps_per_epoch))\n",
    "        print('Time for epoch {} is {:.4f} sec'.format(epoch+1, time.time()-start))\n",
    "        \n",
    "        # save the model\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "        \n",
    "        # visualization\n",
    "        if (epoch + 1) % hparas['PRINT_FREQ'] == 0:\n",
    "            for caption in sample_sentence:\n",
    "                fake_image = test_step(caption, sample_seed, hidden)\n",
    "            save_images(fake_image, [ni, ni], 'samples/demo/train_{:02d}.jpg'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, gen_loss: -0.8106, disc_loss: 26.2114\n",
      "Time for epoch 1 is 54.2361 sec\n",
      "Epoch 2, gen_loss: -1.0746, disc_loss: 22.9979\n",
      "Time for epoch 2 is 50.8075 sec\n",
      "Epoch 3, gen_loss: -0.7176, disc_loss: 21.4154\n",
      "Time for epoch 3 is 49.3004 sec\n",
      "Epoch 4, gen_loss: 0.7307, disc_loss: 17.7248\n",
      "Time for epoch 4 is 49.5353 sec\n",
      "Epoch 5, gen_loss: -1.6419, disc_loss: 16.3114\n",
      "Time for epoch 5 is 51.0460 sec\n",
      "Epoch 6, gen_loss: -1.2308, disc_loss: 17.0351\n",
      "Time for epoch 6 is 50.8477 sec\n",
      "Epoch 7, gen_loss: -1.5467, disc_loss: 18.1825\n",
      "Time for epoch 7 is 50.4101 sec\n",
      "Epoch 8, gen_loss: 108.1327, disc_loss: -30.1398\n",
      "Time for epoch 8 is 50.2835 sec\n",
      "Epoch 9, gen_loss: 101.1627, disc_loss: -23.4229\n",
      "Time for epoch 9 is 50.7894 sec\n",
      "Epoch 10, gen_loss: 110.7641, disc_loss: -33.8412\n",
      "Time for epoch 10 is 50.0714 sec\n",
      "Epoch 11, gen_loss: 173.6095, disc_loss: -37.9558\n",
      "Time for epoch 11 is 52.1786 sec\n",
      "Epoch 12, gen_loss: 189.2504, disc_loss: -32.3199\n",
      "Time for epoch 12 is 50.8197 sec\n",
      "Epoch 13, gen_loss: 171.4704, disc_loss: -33.3280\n",
      "Time for epoch 13 is 50.8117 sec\n",
      "Epoch 14, gen_loss: 159.3362, disc_loss: -33.4905\n",
      "Time for epoch 14 is 49.4740 sec\n",
      "Epoch 15, gen_loss: 177.2408, disc_loss: -32.1157\n",
      "Time for epoch 15 is 59.9187 sec\n",
      "Epoch 16, gen_loss: 156.3120, disc_loss: -40.9203\n",
      "Time for epoch 16 is 82.3470 sec\n",
      "Epoch 17, gen_loss: 143.1541, disc_loss: -36.3126\n",
      "Time for epoch 17 is 62.1906 sec\n",
      "Epoch 18, gen_loss: 144.5961, disc_loss: -27.1671\n",
      "Time for epoch 18 is 79.8154 sec\n",
      "Epoch 19, gen_loss: 157.3982, disc_loss: -26.8355\n",
      "Time for epoch 19 is 60.3466 sec\n",
      "Epoch 20, gen_loss: 144.5795, disc_loss: -35.2356\n",
      "Time for epoch 20 is 78.6988 sec\n",
      "Epoch 21, gen_loss: 164.8666, disc_loss: -32.2988\n",
      "Time for epoch 21 is 61.8991 sec\n",
      "Epoch 22, gen_loss: 140.1614, disc_loss: -31.2356\n",
      "Time for epoch 22 is 76.6563 sec\n",
      "Epoch 23, gen_loss: 197.9686, disc_loss: -29.7384\n",
      "Time for epoch 23 is 60.1660 sec\n",
      "Epoch 24, gen_loss: 175.1779, disc_loss: -26.3365\n",
      "Time for epoch 24 is 77.5643 sec\n",
      "Epoch 25, gen_loss: 201.4719, disc_loss: -28.1103\n",
      "Time for epoch 25 is 60.0420 sec\n",
      "Epoch 26, gen_loss: 165.7308, disc_loss: -25.9596\n",
      "Time for epoch 26 is 78.2168 sec\n",
      "Epoch 27, gen_loss: 125.7646, disc_loss: -21.1904\n",
      "Time for epoch 27 is 61.5606 sec\n",
      "Epoch 28, gen_loss: 185.4863, disc_loss: -28.0906\n",
      "Time for epoch 28 is 76.8914 sec\n",
      "Epoch 29, gen_loss: 181.1022, disc_loss: -30.0264\n",
      "Time for epoch 29 is 61.0704 sec\n",
      "Epoch 30, gen_loss: 113.1787, disc_loss: -26.8129\n",
      "Time for epoch 30 is 76.7005 sec\n",
      "Epoch 31, gen_loss: 95.4179, disc_loss: -24.1989\n",
      "Time for epoch 31 is 59.3223 sec\n",
      "Epoch 32, gen_loss: 136.5550, disc_loss: -20.3582\n",
      "Time for epoch 32 is 78.6344 sec\n",
      "Epoch 33, gen_loss: 189.6135, disc_loss: -25.6459\n",
      "Time for epoch 33 is 59.4205 sec\n",
      "Epoch 34, gen_loss: 142.7990, disc_loss: -31.0143\n",
      "Time for epoch 34 is 78.9806 sec\n",
      "Epoch 35, gen_loss: 102.3821, disc_loss: -27.6902\n",
      "Time for epoch 35 is 58.8500 sec\n",
      "Epoch 36, gen_loss: 117.0548, disc_loss: -28.1006\n",
      "Time for epoch 36 is 79.7061 sec\n",
      "Epoch 37, gen_loss: 119.1322, disc_loss: -29.3235\n",
      "Time for epoch 37 is 59.3968 sec\n",
      "Epoch 38, gen_loss: 33.6598, disc_loss: -25.7794\n",
      "Time for epoch 38 is 78.4821 sec\n",
      "Epoch 39, gen_loss: 105.9040, disc_loss: -23.8333\n",
      "Time for epoch 39 is 60.0724 sec\n",
      "Epoch 40, gen_loss: 59.1174, disc_loss: -26.9043\n",
      "Time for epoch 40 is 78.3480 sec\n",
      "Epoch 41, gen_loss: 62.6371, disc_loss: -24.4277\n",
      "Time for epoch 41 is 60.2302 sec\n",
      "Epoch 42, gen_loss: 81.5040, disc_loss: -24.0697\n",
      "Time for epoch 42 is 78.6439 sec\n",
      "Epoch 43, gen_loss: 32.2661, disc_loss: -26.4651\n",
      "Time for epoch 43 is 81.9264 sec\n",
      "Epoch 44, gen_loss: -3.3383, disc_loss: -21.5667\n",
      "Time for epoch 44 is 65.7161 sec\n",
      "Epoch 45, gen_loss: 78.9476, disc_loss: -24.3803\n",
      "Time for epoch 45 is 78.5049 sec\n",
      "Epoch 46, gen_loss: 79.4850, disc_loss: -28.1259\n",
      "Time for epoch 46 is 65.3444 sec\n",
      "Epoch 47, gen_loss: 9.4155, disc_loss: -21.3799\n",
      "Time for epoch 47 is 79.0166 sec\n",
      "Epoch 48, gen_loss: 49.6114, disc_loss: -18.3790\n",
      "Time for epoch 48 is 63.6659 sec\n",
      "Epoch 49, gen_loss: 38.4613, disc_loss: -19.6699\n",
      "Time for epoch 49 is 79.9692 sec\n",
      "Epoch 50, gen_loss: 29.6829, disc_loss: -21.9493\n",
      "Time for epoch 50 is 63.1884 sec\n",
      "Epoch 51, gen_loss: 64.7217, disc_loss: -23.9335\n",
      "Time for epoch 51 is 81.8939 sec\n",
      "Epoch 52, gen_loss: 37.2523, disc_loss: -18.4800\n",
      "Time for epoch 52 is 61.6728 sec\n",
      "Epoch 53, gen_loss: 65.6451, disc_loss: -23.1464\n",
      "Time for epoch 53 is 81.0934 sec\n",
      "Epoch 54, gen_loss: 130.3400, disc_loss: -20.6341\n",
      "Time for epoch 54 is 62.5716 sec\n",
      "Epoch 55, gen_loss: 61.7249, disc_loss: -21.0727\n",
      "Time for epoch 55 is 80.0691 sec\n",
      "Epoch 56, gen_loss: 42.0030, disc_loss: -22.3594\n",
      "Time for epoch 56 is 63.0630 sec\n",
      "Epoch 57, gen_loss: 165.1251, disc_loss: -22.6119\n",
      "Time for epoch 57 is 83.8416 sec\n",
      "Epoch 58, gen_loss: 12.9851, disc_loss: -20.1893\n",
      "Time for epoch 58 is 60.0308 sec\n",
      "Epoch 59, gen_loss: 77.9994, disc_loss: -23.1767\n",
      "Time for epoch 59 is 81.8743 sec\n",
      "Epoch 60, gen_loss: -29.7537, disc_loss: -21.6518\n",
      "Time for epoch 60 is 62.1596 sec\n",
      "Epoch 61, gen_loss: 75.2955, disc_loss: -20.0724\n",
      "Time for epoch 61 is 80.0826 sec\n",
      "Epoch 62, gen_loss: 33.8568, disc_loss: -24.3232\n",
      "Time for epoch 62 is 63.4115 sec\n",
      "Epoch 63, gen_loss: -8.2150, disc_loss: -20.0462\n",
      "Time for epoch 63 is 80.1634 sec\n",
      "Epoch 64, gen_loss: 107.7896, disc_loss: -19.8303\n",
      "Time for epoch 64 is 63.2149 sec\n",
      "Epoch 65, gen_loss: -23.1350, disc_loss: -22.1979\n",
      "Time for epoch 65 is 80.4114 sec\n",
      "Epoch 66, gen_loss: 43.6385, disc_loss: -19.3741\n",
      "Time for epoch 66 is 63.7677 sec\n",
      "Epoch 67, gen_loss: 24.6129, disc_loss: -23.3917\n",
      "Time for epoch 67 is 81.0904 sec\n",
      "Epoch 68, gen_loss: 52.5315, disc_loss: -22.6361\n",
      "Time for epoch 68 is 63.7590 sec\n",
      "Epoch 69, gen_loss: 23.5587, disc_loss: -16.3662\n",
      "Time for epoch 69 is 81.9214 sec\n",
      "Epoch 70, gen_loss: -8.8169, disc_loss: -21.2372\n",
      "Time for epoch 70 is 64.4233 sec\n",
      "Epoch 71, gen_loss: 1.7417, disc_loss: -17.9406\n",
      "Time for epoch 71 is 85.1633 sec\n",
      "Epoch 72, gen_loss: -3.6128, disc_loss: -20.1581\n",
      "Time for epoch 72 is 58.4832 sec\n",
      "Epoch 73, gen_loss: -55.7297, disc_loss: -18.0779\n",
      "Time for epoch 73 is 82.0797 sec\n",
      "Epoch 74, gen_loss: 3.1647, disc_loss: -17.2359\n",
      "Time for epoch 74 is 61.4248 sec\n",
      "Epoch 75, gen_loss: -5.0583, disc_loss: -22.1888\n",
      "Time for epoch 75 is 80.7296 sec\n",
      "Epoch 76, gen_loss: 14.3286, disc_loss: -22.0007\n",
      "Time for epoch 76 is 62.8979 sec\n",
      "Epoch 77, gen_loss: -77.5308, disc_loss: -23.8037\n",
      "Time for epoch 77 is 80.1379 sec\n",
      "Epoch 78, gen_loss: 23.7793, disc_loss: -19.8431\n",
      "Time for epoch 78 is 63.0467 sec\n",
      "Epoch 79, gen_loss: 9.3619, disc_loss: -19.5896\n",
      "Time for epoch 79 is 80.1996 sec\n",
      "Epoch 80, gen_loss: 39.9166, disc_loss: -26.2735\n",
      "Time for epoch 80 is 63.2020 sec\n",
      "Epoch 81, gen_loss: -48.3090, disc_loss: -20.7717\n",
      "Time for epoch 81 is 80.5513 sec\n",
      "Epoch 82, gen_loss: 119.5553, disc_loss: -19.2082\n",
      "Time for epoch 82 is 62.9295 sec\n",
      "Epoch 83, gen_loss: -0.0499, disc_loss: -19.0223\n",
      "Time for epoch 83 is 80.0680 sec\n",
      "Epoch 84, gen_loss: 0.0628, disc_loss: -21.4976\n",
      "Time for epoch 84 is 62.9436 sec\n",
      "Epoch 85, gen_loss: 41.1288, disc_loss: -6.7030\n",
      "Time for epoch 85 is 79.6226 sec\n",
      "Epoch 86, gen_loss: -30.2167, disc_loss: -18.7685\n",
      "Time for epoch 86 is 63.6667 sec\n",
      "Epoch 87, gen_loss: -45.1639, disc_loss: -20.6191\n",
      "Time for epoch 87 is 85.0821 sec\n",
      "Epoch 88, gen_loss: 64.0713, disc_loss: -13.0997\n",
      "Time for epoch 88 is 58.4543 sec\n",
      "Epoch 89, gen_loss: -113.7822, disc_loss: -19.5567\n",
      "Time for epoch 89 is 82.3231 sec\n",
      "Epoch 90, gen_loss: 39.3504, disc_loss: -17.8616\n",
      "Time for epoch 90 is 62.5839 sec\n",
      "Epoch 91, gen_loss: 93.0371, disc_loss: -12.9725\n",
      "Time for epoch 91 is 82.5111 sec\n",
      "Epoch 92, gen_loss: -43.9083, disc_loss: -20.9753\n",
      "Time for epoch 92 is 61.6793 sec\n",
      "Epoch 93, gen_loss: 28.9039, disc_loss: -24.9218\n",
      "Time for epoch 93 is 82.0747 sec\n",
      "Epoch 94, gen_loss: 45.0239, disc_loss: -18.8526\n",
      "Time for epoch 94 is 65.6479 sec\n",
      "Epoch 95, gen_loss: 7.6007, disc_loss: -14.9853\n",
      "Time for epoch 95 is 80.2811 sec\n",
      "Epoch 96, gen_loss: -90.6240, disc_loss: -18.3359\n",
      "Time for epoch 96 is 63.1298 sec\n",
      "Epoch 97, gen_loss: 13.6576, disc_loss: -18.3105\n",
      "Time for epoch 97 is 80.5263 sec\n",
      "Epoch 98, gen_loss: -82.2927, disc_loss: -16.9976\n",
      "Time for epoch 98 is 62.8241 sec\n",
      "Epoch 99, gen_loss: -47.9438, disc_loss: -18.5067\n",
      "Time for epoch 99 is 80.0179 sec\n",
      "Epoch 100, gen_loss: -47.0857, disc_loss: -12.9582\n",
      "Time for epoch 100 is 63.3705 sec\n",
      "Epoch 101, gen_loss: 3.7419, disc_loss: -17.6212\n",
      "Time for epoch 101 is 82.0762 sec\n",
      "Epoch 102, gen_loss: -59.8867, disc_loss: -17.5973\n",
      "Time for epoch 102 is 61.5241 sec\n",
      "Epoch 103, gen_loss: 67.2110, disc_loss: -18.6664\n",
      "Time for epoch 103 is 80.9028 sec\n",
      "Epoch 104, gen_loss: 7.2093, disc_loss: -16.0074\n",
      "Time for epoch 104 is 62.7322 sec\n",
      "Epoch 105, gen_loss: -37.7793, disc_loss: -16.3328\n",
      "Time for epoch 105 is 80.1840 sec\n",
      "Epoch 106, gen_loss: 70.4112, disc_loss: -14.1696\n",
      "Time for epoch 106 is 62.9695 sec\n",
      "Epoch 107, gen_loss: -81.5808, disc_loss: -19.1327\n",
      "Time for epoch 107 is 86.4795 sec\n",
      "Epoch 108, gen_loss: -56.9461, disc_loss: -20.1533\n",
      "Time for epoch 108 is 57.0513 sec\n",
      "Epoch 109, gen_loss: 76.6818, disc_loss: -12.4711\n",
      "Time for epoch 109 is 82.4367 sec\n",
      "Epoch 110, gen_loss: -38.1099, disc_loss: -20.6456\n",
      "Time for epoch 110 is 61.0704 sec\n",
      "Epoch 111, gen_loss: -23.8255, disc_loss: -13.5295\n",
      "Time for epoch 111 is 80.5014 sec\n",
      "Epoch 112, gen_loss: 42.8015, disc_loss: -11.7862\n",
      "Time for epoch 112 is 62.9780 sec\n",
      "Epoch 113, gen_loss: 6.3218, disc_loss: -13.3489\n",
      "Time for epoch 113 is 80.0288 sec\n",
      "Epoch 114, gen_loss: -26.1186, disc_loss: -19.4134\n",
      "Time for epoch 114 is 62.8742 sec\n",
      "Epoch 115, gen_loss: 1.4689, disc_loss: -11.4212\n",
      "Time for epoch 115 is 78.2372 sec\n",
      "Epoch 116, gen_loss: 18.3420, disc_loss: -19.3841\n",
      "Time for epoch 116 is 64.8127 sec\n",
      "Epoch 117, gen_loss: 16.8811, disc_loss: -12.0799\n",
      "Time for epoch 117 is 78.7118 sec\n",
      "Epoch 118, gen_loss: -30.8707, disc_loss: -21.8756\n",
      "Time for epoch 118 is 64.0963 sec\n",
      "Epoch 119, gen_loss: 11.0161, disc_loss: -12.8093\n",
      "Time for epoch 119 is 79.9222 sec\n",
      "Epoch 120, gen_loss: 10.7643, disc_loss: -16.6752\n",
      "Time for epoch 120 is 63.0133 sec\n",
      "Epoch 121, gen_loss: 18.2902, disc_loss: -16.6656\n",
      "Time for epoch 121 is 79.6632 sec\n",
      "Epoch 122, gen_loss: -65.9673, disc_loss: -11.9567\n",
      "Time for epoch 122 is 63.2043 sec\n",
      "Epoch 123, gen_loss: 60.2391, disc_loss: -18.6860\n",
      "Time for epoch 123 is 79.5850 sec\n",
      "Epoch 124, gen_loss: -90.2309, disc_loss: -13.3069\n",
      "Time for epoch 124 is 63.1608 sec\n",
      "Epoch 125, gen_loss: -4.1302, disc_loss: -18.5516\n",
      "Time for epoch 125 is 80.6263 sec\n",
      "Epoch 126, gen_loss: -50.9648, disc_loss: -10.2972\n",
      "Time for epoch 126 is 62.8500 sec\n",
      "Epoch 127, gen_loss: -136.7944, disc_loss: -25.3533\n",
      "Time for epoch 127 is 81.9339 sec\n",
      "Epoch 128, gen_loss: 59.2093, disc_loss: -12.7096\n",
      "Time for epoch 128 is 61.4533 sec\n",
      "Epoch 129, gen_loss: -19.7494, disc_loss: -19.5989\n",
      "Time for epoch 129 is 80.4064 sec\n",
      "Epoch 130, gen_loss: -283.8131, disc_loss: -21.5062\n",
      "Time for epoch 130 is 62.9727 sec\n",
      "Epoch 131, gen_loss: 4.0051, disc_loss: -14.4360\n",
      "Time for epoch 131 is 89.6399 sec\n",
      "Epoch 132, gen_loss: -1.0317, disc_loss: -15.3748\n",
      "Time for epoch 132 is 54.2515 sec\n",
      "Epoch 133, gen_loss: -58.8717, disc_loss: -17.0463\n",
      "Time for epoch 133 is 81.9600 sec\n",
      "Epoch 134, gen_loss: 29.3207, disc_loss: -15.0303\n",
      "Time for epoch 134 is 57.7099 sec\n",
      "Epoch 135, gen_loss: -106.3076, disc_loss: -17.5660\n",
      "Time for epoch 135 is 81.8363 sec\n",
      "Epoch 136, gen_loss: -58.9409, disc_loss: -15.1365\n",
      "Time for epoch 136 is 61.6853 sec\n",
      "Epoch 137, gen_loss: 81.7350, disc_loss: -14.9843\n",
      "Time for epoch 137 is 80.0621 sec\n",
      "Epoch 138, gen_loss: -151.5257, disc_loss: -13.6448\n",
      "Time for epoch 138 is 63.2976 sec\n",
      "Epoch 139, gen_loss: -0.7723, disc_loss: -20.2516\n",
      "Time for epoch 139 is 79.6929 sec\n",
      "Epoch 140, gen_loss: 146.3135, disc_loss: -13.3562\n",
      "Time for epoch 140 is 63.2911 sec\n",
      "Epoch 141, gen_loss: -39.1648, disc_loss: -18.3885\n",
      "Time for epoch 141 is 79.5255 sec\n",
      "Epoch 142, gen_loss: -102.9111, disc_loss: -12.1634\n",
      "Time for epoch 142 is 63.2973 sec\n",
      "Epoch 143, gen_loss: 84.8027, disc_loss: -14.5140\n",
      "Time for epoch 143 is 79.1194 sec\n",
      "Epoch 144, gen_loss: 31.4363, disc_loss: -15.9232\n",
      "Time for epoch 144 is 63.5641 sec\n",
      "Epoch 145, gen_loss: -98.1034, disc_loss: -17.7416\n",
      "Time for epoch 145 is 79.6572 sec\n",
      "Epoch 146, gen_loss: -12.6407, disc_loss: -13.6791\n",
      "Time for epoch 146 is 63.2251 sec\n",
      "Epoch 147, gen_loss: 47.9614, disc_loss: -15.6992\n",
      "Time for epoch 147 is 79.2909 sec\n",
      "Epoch 148, gen_loss: -140.3349, disc_loss: -12.7918\n",
      "Time for epoch 148 is 63.5155 sec\n",
      "Epoch 149, gen_loss: -100.5597, disc_loss: -15.5774\n",
      "Time for epoch 149 is 79.5899 sec\n",
      "Epoch 150, gen_loss: 109.2460, disc_loss: -12.6375\n",
      "Time for epoch 150 is 62.8550 sec\n",
      "Epoch 151, gen_loss: -153.6962, disc_loss: -16.7398\n",
      "Time for epoch 151 is 49.1108 sec\n",
      "Epoch 152, gen_loss: -95.6522, disc_loss: -15.2258\n",
      "Time for epoch 152 is 49.1089 sec\n",
      "Epoch 153, gen_loss: -143.6951, disc_loss: -14.7073\n",
      "Time for epoch 153 is 49.1866 sec\n",
      "Epoch 154, gen_loss: 48.2945, disc_loss: -16.6896\n",
      "Time for epoch 154 is 49.0565 sec\n",
      "Epoch 155, gen_loss: -95.8472, disc_loss: -15.6640\n",
      "Time for epoch 155 is 49.1288 sec\n",
      "Epoch 156, gen_loss: 1.0168, disc_loss: -18.3878\n",
      "Time for epoch 156 is 49.1340 sec\n",
      "Epoch 157, gen_loss: 127.9657, disc_loss: -14.1330\n",
      "Time for epoch 157 is 49.1660 sec\n",
      "Epoch 158, gen_loss: -134.3619, disc_loss: -16.3971\n",
      "Time for epoch 158 is 49.0944 sec\n",
      "Epoch 159, gen_loss: 9.4176, disc_loss: -14.1816\n",
      "Time for epoch 159 is 49.0551 sec\n",
      "Epoch 160, gen_loss: -11.5891, disc_loss: -12.7470\n",
      "Time for epoch 160 is 50.9132 sec\n",
      "Epoch 161, gen_loss: 11.1661, disc_loss: -15.0622\n",
      "Time for epoch 161 is 49.1903 sec\n",
      "Epoch 162, gen_loss: -31.0719, disc_loss: -14.5866\n",
      "Time for epoch 162 is 49.1388 sec\n",
      "Epoch 163, gen_loss: 13.5657, disc_loss: -13.8627\n",
      "Time for epoch 163 is 49.1509 sec\n",
      "Epoch 164, gen_loss: 41.1214, disc_loss: -13.8200\n",
      "Time for epoch 164 is 49.1324 sec\n",
      "Epoch 165, gen_loss: 4.0552, disc_loss: -13.0401\n",
      "Time for epoch 165 is 49.1517 sec\n",
      "Epoch 166, gen_loss: -56.8528, disc_loss: -14.0379\n",
      "Time for epoch 166 is 49.1521 sec\n",
      "Epoch 167, gen_loss: 150.2107, disc_loss: -14.2988\n",
      "Time for epoch 167 is 49.1846 sec\n",
      "Epoch 168, gen_loss: -76.0438, disc_loss: -14.6080\n",
      "Time for epoch 168 is 49.1015 sec\n",
      "Epoch 169, gen_loss: 44.6046, disc_loss: -13.6970\n",
      "Time for epoch 169 is 49.1292 sec\n",
      "Epoch 170, gen_loss: -23.4299, disc_loss: -15.4057\n",
      "Time for epoch 170 is 49.1640 sec\n",
      "Epoch 171, gen_loss: 26.5244, disc_loss: -12.5759\n",
      "Time for epoch 171 is 49.0990 sec\n",
      "Epoch 172, gen_loss: 95.0473, disc_loss: -14.4186\n",
      "Time for epoch 172 is 49.1908 sec\n",
      "Epoch 173, gen_loss: -25.6770, disc_loss: -17.6788\n",
      "Time for epoch 173 is 49.1017 sec\n",
      "Epoch 174, gen_loss: -18.5847, disc_loss: -11.7802\n",
      "Time for epoch 174 is 49.1110 sec\n",
      "Epoch 175, gen_loss: 182.4398, disc_loss: -12.2666\n",
      "Time for epoch 175 is 49.1676 sec\n",
      "Epoch 176, gen_loss: -68.9970, disc_loss: -15.2392\n",
      "Time for epoch 176 is 49.1007 sec\n",
      "Epoch 177, gen_loss: 76.1504, disc_loss: -16.5291\n",
      "Time for epoch 177 is 49.3232 sec\n",
      "Epoch 178, gen_loss: 174.6988, disc_loss: -10.9159\n",
      "Time for epoch 178 is 49.0676 sec\n",
      "Epoch 179, gen_loss: 67.6443, disc_loss: -13.7061\n",
      "Time for epoch 179 is 49.1437 sec\n",
      "Epoch 180, gen_loss: 255.9119, disc_loss: -16.2874\n",
      "Time for epoch 180 is 49.1597 sec\n",
      "Epoch 181, gen_loss: 184.6229, disc_loss: -12.3727\n",
      "Time for epoch 181 is 49.1205 sec\n",
      "Epoch 182, gen_loss: 34.9281, disc_loss: -12.1913\n",
      "Time for epoch 182 is 49.1506 sec\n",
      "Epoch 183, gen_loss: 215.0539, disc_loss: -13.3292\n",
      "Time for epoch 183 is 49.0844 sec\n",
      "Epoch 184, gen_loss: 10.1500, disc_loss: -10.6888\n",
      "Time for epoch 184 is 49.0863 sec\n",
      "Epoch 185, gen_loss: 6.3063, disc_loss: -11.8334\n",
      "Time for epoch 185 is 49.0742 sec\n",
      "Epoch 186, gen_loss: 39.2967, disc_loss: -10.9963\n",
      "Time for epoch 186 is 49.1313 sec\n",
      "Epoch 187, gen_loss: 53.3947, disc_loss: -14.2364\n",
      "Time for epoch 187 is 49.1113 sec\n",
      "Epoch 188, gen_loss: 66.8869, disc_loss: -12.0065\n",
      "Time for epoch 188 is 49.1539 sec\n",
      "Epoch 189, gen_loss: -71.1974, disc_loss: -13.7838\n",
      "Time for epoch 189 is 49.1038 sec\n",
      "Epoch 190, gen_loss: 67.6591, disc_loss: -12.8676\n",
      "Time for epoch 190 is 49.0430 sec\n",
      "Epoch 191, gen_loss: 145.5391, disc_loss: -13.5230\n",
      "Time for epoch 191 is 49.1346 sec\n",
      "Epoch 192, gen_loss: -78.7617, disc_loss: -14.6656\n",
      "Time for epoch 192 is 49.1058 sec\n",
      "Epoch 193, gen_loss: 134.8844, disc_loss: -14.9454\n",
      "Time for epoch 193 is 49.0829 sec\n",
      "Epoch 194, gen_loss: 99.3380, disc_loss: -11.4623\n",
      "Time for epoch 194 is 49.1083 sec\n",
      "Epoch 195, gen_loss: 62.7722, disc_loss: -14.8990\n",
      "Time for epoch 195 is 51.2946 sec\n",
      "Epoch 196, gen_loss: 12.7793, disc_loss: -14.3127\n",
      "Time for epoch 196 is 49.1135 sec\n",
      "Epoch 197, gen_loss: 145.5438, disc_loss: -12.9751\n",
      "Time for epoch 197 is 49.1105 sec\n",
      "Epoch 198, gen_loss: -18.7520, disc_loss: -13.8856\n",
      "Time for epoch 198 is 49.1280 sec\n",
      "Epoch 199, gen_loss: 46.8837, disc_loss: -10.7470\n",
      "Time for epoch 199 is 49.0821 sec\n",
      "Epoch 200, gen_loss: -59.1288, disc_loss: -11.2651\n",
      "Time for epoch 200 is 49.1352 sec\n",
      "Epoch 201, gen_loss: 134.6871, disc_loss: -9.5657\n",
      "Time for epoch 201 is 49.0306 sec\n",
      "Epoch 202, gen_loss: 70.6885, disc_loss: -11.2263\n",
      "Time for epoch 202 is 47.0524 sec\n",
      "Epoch 203, gen_loss: 114.5084, disc_loss: -13.7442\n",
      "Time for epoch 203 is 47.0583 sec\n",
      "Epoch 204, gen_loss: 145.9530, disc_loss: -8.6242\n",
      "Time for epoch 204 is 47.0876 sec\n",
      "Epoch 205, gen_loss: 126.2407, disc_loss: -12.5050\n",
      "Time for epoch 205 is 47.0527 sec\n",
      "Epoch 206, gen_loss: 138.9927, disc_loss: -12.3153\n",
      "Time for epoch 206 is 47.0801 sec\n",
      "Epoch 207, gen_loss: 166.1993, disc_loss: -10.0265\n",
      "Time for epoch 207 is 47.0273 sec\n",
      "Epoch 208, gen_loss: -2.6232, disc_loss: -11.9390\n",
      "Time for epoch 208 is 47.0348 sec\n",
      "Epoch 209, gen_loss: 171.3074, disc_loss: -13.4128\n",
      "Time for epoch 209 is 47.0864 sec\n",
      "Epoch 210, gen_loss: 99.4236, disc_loss: -13.4754\n",
      "Time for epoch 210 is 47.0485 sec\n",
      "Epoch 211, gen_loss: 285.2748, disc_loss: -12.2741\n",
      "Time for epoch 211 is 47.0669 sec\n",
      "Epoch 212, gen_loss: 55.1763, disc_loss: -13.2556\n",
      "Time for epoch 212 is 47.0429 sec\n",
      "Epoch 213, gen_loss: -9.7667, disc_loss: -9.6283\n",
      "Time for epoch 213 is 47.0431 sec\n",
      "Epoch 214, gen_loss: 198.2823, disc_loss: -12.0593\n",
      "Time for epoch 214 is 46.9934 sec\n",
      "Epoch 215, gen_loss: -91.7245, disc_loss: -15.4360\n",
      "Time for epoch 215 is 47.0903 sec\n",
      "Epoch 216, gen_loss: -94.9309, disc_loss: -9.0131\n",
      "Time for epoch 216 is 47.0399 sec\n",
      "Epoch 217, gen_loss: 158.1277, disc_loss: -11.0486\n",
      "Time for epoch 217 is 47.0377 sec\n",
      "Epoch 218, gen_loss: -116.1837, disc_loss: -14.0806\n",
      "Time for epoch 218 is 47.0612 sec\n",
      "Epoch 219, gen_loss: -42.0263, disc_loss: -10.2730\n",
      "Time for epoch 219 is 47.0619 sec\n",
      "Epoch 220, gen_loss: 62.1588, disc_loss: -10.1065\n",
      "Time for epoch 220 is 47.0635 sec\n",
      "Epoch 221, gen_loss: -31.0168, disc_loss: -11.9164\n",
      "Time for epoch 221 is 47.0392 sec\n",
      "Epoch 222, gen_loss: 223.9453, disc_loss: -13.5080\n",
      "Time for epoch 222 is 47.0681 sec\n",
      "Epoch 223, gen_loss: 110.3109, disc_loss: -11.6702\n",
      "Time for epoch 223 is 47.0780 sec\n",
      "Epoch 224, gen_loss: -9.9503, disc_loss: -9.6593\n",
      "Time for epoch 224 is 47.0286 sec\n",
      "Epoch 225, gen_loss: 110.4957, disc_loss: -13.2655\n",
      "Time for epoch 225 is 47.0660 sec\n",
      "Epoch 226, gen_loss: 256.7625, disc_loss: -11.2605\n",
      "Time for epoch 226 is 47.0204 sec\n",
      "Epoch 227, gen_loss: 68.9961, disc_loss: -15.1471\n",
      "Time for epoch 227 is 47.0799 sec\n",
      "Epoch 228, gen_loss: 59.1118, disc_loss: -10.3077\n",
      "Time for epoch 228 is 47.0444 sec\n",
      "Epoch 229, gen_loss: 293.2719, disc_loss: -8.8123\n",
      "Time for epoch 229 is 47.0599 sec\n",
      "Epoch 230, gen_loss: 51.4254, disc_loss: -12.3952\n",
      "Time for epoch 230 is 47.0877 sec\n",
      "Epoch 231, gen_loss: 103.9377, disc_loss: -12.6276\n",
      "Time for epoch 231 is 47.2161 sec\n",
      "Epoch 232, gen_loss: 153.0357, disc_loss: -14.4615\n",
      "Time for epoch 232 is 47.2516 sec\n",
      "Epoch 233, gen_loss: 184.8169, disc_loss: -12.8575\n",
      "Time for epoch 233 is 47.4790 sec\n",
      "Epoch 234, gen_loss: 84.6341, disc_loss: -10.2473\n",
      "Time for epoch 234 is 47.1880 sec\n",
      "Epoch 235, gen_loss: 179.0799, disc_loss: -8.1185\n",
      "Time for epoch 235 is 47.3855 sec\n",
      "Epoch 236, gen_loss: 132.5034, disc_loss: -10.5176\n",
      "Time for epoch 236 is 47.0645 sec\n",
      "Epoch 237, gen_loss: 174.0598, disc_loss: -11.4865\n",
      "Time for epoch 237 is 47.0853 sec\n",
      "Epoch 238, gen_loss: 239.0468, disc_loss: -12.2152\n",
      "Time for epoch 238 is 47.8752 sec\n",
      "Epoch 239, gen_loss: 147.0836, disc_loss: -11.6407\n",
      "Time for epoch 239 is 49.3199 sec\n",
      "Epoch 240, gen_loss: 113.0066, disc_loss: -11.3072\n",
      "Time for epoch 240 is 48.4280 sec\n",
      "Epoch 241, gen_loss: 205.7689, disc_loss: -11.4003\n",
      "Time for epoch 241 is 47.4505 sec\n",
      "Epoch 242, gen_loss: 196.2881, disc_loss: -9.2766\n",
      "Time for epoch 242 is 47.2285 sec\n",
      "Epoch 243, gen_loss: 107.6236, disc_loss: -10.7742\n",
      "Time for epoch 243 is 47.5901 sec\n",
      "Epoch 244, gen_loss: 176.3171, disc_loss: -10.1026\n",
      "Time for epoch 244 is 47.8035 sec\n",
      "Epoch 245, gen_loss: 181.7403, disc_loss: -12.3097\n",
      "Time for epoch 245 is 48.0942 sec\n",
      "Epoch 246, gen_loss: 28.0480, disc_loss: -10.4208\n",
      "Time for epoch 246 is 47.1516 sec\n",
      "Epoch 247, gen_loss: 146.6486, disc_loss: -11.9530\n",
      "Time for epoch 247 is 48.5052 sec\n",
      "Epoch 248, gen_loss: 38.8263, disc_loss: -14.3961\n",
      "Time for epoch 248 is 49.1808 sec\n",
      "Epoch 249, gen_loss: 35.6023, disc_loss: -13.5053\n",
      "Time for epoch 249 is 49.2840 sec\n",
      "Epoch 250, gen_loss: -19.7114, disc_loss: -10.9915\n",
      "Time for epoch 250 is 49.8830 sec\n",
      "Epoch 251, gen_loss: -96.3519, disc_loss: -10.7754\n",
      "Time for epoch 251 is 48.1060 sec\n",
      "Epoch 252, gen_loss: 64.4277, disc_loss: -12.5057\n",
      "Time for epoch 252 is 47.7179 sec\n",
      "Epoch 253, gen_loss: -24.0580, disc_loss: -11.1911\n",
      "Time for epoch 253 is 47.3898 sec\n",
      "Epoch 254, gen_loss: -11.7039, disc_loss: -9.2269\n",
      "Time for epoch 254 is 48.2734 sec\n",
      "Epoch 255, gen_loss: 69.1313, disc_loss: -11.1947\n",
      "Time for epoch 255 is 49.2389 sec\n",
      "Epoch 256, gen_loss: 53.0879, disc_loss: -9.9304\n",
      "Time for epoch 256 is 48.7430 sec\n",
      "Epoch 257, gen_loss: 23.6793, disc_loss: -11.0988\n",
      "Time for epoch 257 is 48.7946 sec\n",
      "Epoch 258, gen_loss: -50.3063, disc_loss: -10.5963\n",
      "Time for epoch 258 is 48.5866 sec\n",
      "Epoch 259, gen_loss: 18.7098, disc_loss: -9.2105\n",
      "Time for epoch 259 is 48.6540 sec\n",
      "Epoch 260, gen_loss: 177.8326, disc_loss: -13.4528\n",
      "Time for epoch 260 is 48.9674 sec\n",
      "Epoch 261, gen_loss: 172.4136, disc_loss: -3.1355\n",
      "Time for epoch 261 is 49.0791 sec\n",
      "Epoch 262, gen_loss: 7.2106, disc_loss: -14.9739\n",
      "Time for epoch 262 is 48.9657 sec\n",
      "Epoch 263, gen_loss: 95.8256, disc_loss: -11.5891\n",
      "Time for epoch 263 is 47.8761 sec\n",
      "Epoch 264, gen_loss: 184.1255, disc_loss: -9.6417\n",
      "Time for epoch 264 is 48.6878 sec\n",
      "Epoch 265, gen_loss: -79.9094, disc_loss: -14.2255\n",
      "Time for epoch 265 is 47.9622 sec\n",
      "Epoch 266, gen_loss: 32.6214, disc_loss: -11.9988\n",
      "Time for epoch 266 is 48.6912 sec\n",
      "Epoch 267, gen_loss: 152.2558, disc_loss: -11.2757\n",
      "Time for epoch 267 is 47.8669 sec\n",
      "Epoch 268, gen_loss: 50.3016, disc_loss: -8.1266\n",
      "Time for epoch 268 is 47.7761 sec\n",
      "Epoch 269, gen_loss: 159.6428, disc_loss: -10.7458\n",
      "Time for epoch 269 is 47.3260 sec\n",
      "Epoch 270, gen_loss: 13.8643, disc_loss: -8.7448\n",
      "Time for epoch 270 is 48.4945 sec\n",
      "Epoch 271, gen_loss: 32.9467, disc_loss: -7.9721\n",
      "Time for epoch 271 is 47.3456 sec\n",
      "Epoch 272, gen_loss: 64.3968, disc_loss: -9.6124\n",
      "Time for epoch 272 is 47.6524 sec\n",
      "Epoch 273, gen_loss: -39.0182, disc_loss: -9.3973\n",
      "Time for epoch 273 is 48.1431 sec\n",
      "Epoch 274, gen_loss: 119.0121, disc_loss: -12.0981\n",
      "Time for epoch 274 is 48.8481 sec\n",
      "Epoch 275, gen_loss: -92.4893, disc_loss: -8.0638\n",
      "Time for epoch 275 is 48.9988 sec\n",
      "Epoch 276, gen_loss: 95.7844, disc_loss: -11.7262\n",
      "Time for epoch 276 is 48.6873 sec\n",
      "Epoch 277, gen_loss: 70.2459, disc_loss: -9.1703\n",
      "Time for epoch 277 is 48.6095 sec\n",
      "Epoch 278, gen_loss: -86.2482, disc_loss: -8.6663\n",
      "Time for epoch 278 is 47.6323 sec\n",
      "Epoch 279, gen_loss: 239.3122, disc_loss: -11.9431\n",
      "Time for epoch 279 is 47.6341 sec\n",
      "Epoch 280, gen_loss: 12.5604, disc_loss: -10.6734\n",
      "Time for epoch 280 is 47.4878 sec\n",
      "Epoch 281, gen_loss: 10.9340, disc_loss: -12.3632\n",
      "Time for epoch 281 is 48.7738 sec\n",
      "Epoch 282, gen_loss: 110.7197, disc_loss: -8.6695\n",
      "Time for epoch 282 is 48.4562 sec\n",
      "Epoch 283, gen_loss: -77.2722, disc_loss: -11.9225\n",
      "Time for epoch 283 is 47.1725 sec\n",
      "Epoch 284, gen_loss: 86.3746, disc_loss: -8.4226\n",
      "Time for epoch 284 is 47.3417 sec\n",
      "Epoch 285, gen_loss: 17.3356, disc_loss: -10.3238\n",
      "Time for epoch 285 is 47.2204 sec\n",
      "Epoch 286, gen_loss: 22.4846, disc_loss: -13.1880\n",
      "Time for epoch 286 is 47.1938 sec\n",
      "Epoch 287, gen_loss: 55.1032, disc_loss: -9.1833\n",
      "Time for epoch 287 is 65.9722 sec\n",
      "Epoch 288, gen_loss: -101.2217, disc_loss: -11.0498\n",
      "Time for epoch 288 is 67.9343 sec\n",
      "Epoch 289, gen_loss: -0.1867, disc_loss: -8.5853\n",
      "Time for epoch 289 is 81.9204 sec\n",
      "Epoch 290, gen_loss: -5.8502, disc_loss: -9.6118\n",
      "Time for epoch 290 is 60.8237 sec\n",
      "Epoch 291, gen_loss: -60.1833, disc_loss: -11.7876\n",
      "Time for epoch 291 is 68.8947 sec\n",
      "Epoch 292, gen_loss: -27.1988, disc_loss: -9.3615\n",
      "Time for epoch 292 is 67.0523 sec\n",
      "Epoch 293, gen_loss: -84.7011, disc_loss: -10.9998\n",
      "Time for epoch 293 is 66.0253 sec\n",
      "Epoch 294, gen_loss: -55.4070, disc_loss: -10.0786\n",
      "Time for epoch 294 is 69.0189 sec\n",
      "Epoch 295, gen_loss: 6.9676, disc_loss: -9.5631\n",
      "Time for epoch 295 is 58.6941 sec\n",
      "Epoch 296, gen_loss: -79.4111, disc_loss: -12.5087\n",
      "Time for epoch 296 is 74.5827 sec\n",
      "Epoch 297, gen_loss: -36.7291, disc_loss: -10.7214\n",
      "Time for epoch 297 is 55.3394 sec\n",
      "Epoch 298, gen_loss: -93.0451, disc_loss: -11.0771\n",
      "Time for epoch 298 is 77.8967 sec\n",
      "Epoch 299, gen_loss: -24.6474, disc_loss: -9.6999\n",
      "Time for epoch 299 is 49.4077 sec\n",
      "Epoch 300, gen_loss: 72.7259, disc_loss: -13.2334\n",
      "Time for epoch 300 is 76.6922 sec\n",
      "Epoch 301, gen_loss: 62.4561, disc_loss: -14.7791\n",
      "Time for epoch 301 is 50.0818 sec\n",
      "Epoch 302, gen_loss: -31.0819, disc_loss: -8.7653\n",
      "Time for epoch 302 is 77.2007 sec\n",
      "Epoch 303, gen_loss: 54.8239, disc_loss: -11.2625\n",
      "Time for epoch 303 is 59.2301 sec\n",
      "Epoch 304, gen_loss: -26.1358, disc_loss: -13.4036\n",
      "Time for epoch 304 is 73.5789 sec\n",
      "Epoch 305, gen_loss: -80.2148, disc_loss: -9.5726\n",
      "Time for epoch 305 is 62.7361 sec\n",
      "Epoch 306, gen_loss: -70.9029, disc_loss: -9.8964\n",
      "Time for epoch 306 is 66.6305 sec\n",
      "Epoch 307, gen_loss: -32.1419, disc_loss: -12.1463\n",
      "Time for epoch 307 is 68.3061 sec\n",
      "Epoch 308, gen_loss: 10.0485, disc_loss: -10.3372\n",
      "Time for epoch 308 is 62.8608 sec\n",
      "Epoch 309, gen_loss: -80.8930, disc_loss: -11.3036\n",
      "Time for epoch 309 is 73.4314 sec\n",
      "Epoch 310, gen_loss: 61.9742, disc_loss: -9.9334\n",
      "Time for epoch 310 is 57.5394 sec\n",
      "Epoch 311, gen_loss: -68.8556, disc_loss: -11.0655\n",
      "Time for epoch 311 is 75.7304 sec\n",
      "Epoch 312, gen_loss: -133.9840, disc_loss: -6.2952\n",
      "Time for epoch 312 is 54.6621 sec\n",
      "Epoch 313, gen_loss: 12.9118, disc_loss: -11.6223\n",
      "Time for epoch 313 is 77.5238 sec\n",
      "Epoch 314, gen_loss: -81.6678, disc_loss: -10.0276\n",
      "Time for epoch 314 is 49.2766 sec\n",
      "Epoch 315, gen_loss: -51.5533, disc_loss: -11.4507\n",
      "Time for epoch 315 is 75.5234 sec\n",
      "Epoch 316, gen_loss: 78.6642, disc_loss: -10.9482\n",
      "Time for epoch 316 is 55.4063 sec\n",
      "Epoch 317, gen_loss: -156.5325, disc_loss: -9.6848\n",
      "Time for epoch 317 is 74.7213 sec\n",
      "Epoch 318, gen_loss: -57.2460, disc_loss: -11.8884\n",
      "Time for epoch 318 is 62.2481 sec\n",
      "Epoch 319, gen_loss: 28.5002, disc_loss: -8.8555\n",
      "Time for epoch 319 is 81.9204 sec\n",
      "Epoch 320, gen_loss: -94.5367, disc_loss: -11.8036\n",
      "Time for epoch 320 is 58.4763 sec\n",
      "Epoch 321, gen_loss: -36.9837, disc_loss: -9.4431\n",
      "Time for epoch 321 is 72.6637 sec\n",
      "Epoch 322, gen_loss: 27.3952, disc_loss: -6.6666\n",
      "Time for epoch 322 is 63.6447 sec\n",
      "Epoch 323, gen_loss: -44.4666, disc_loss: -10.8055\n",
      "Time for epoch 323 is 64.7315 sec\n",
      "Epoch 324, gen_loss: -16.7755, disc_loss: -10.0534\n",
      "Time for epoch 324 is 68.4941 sec\n",
      "Epoch 325, gen_loss: 47.4197, disc_loss: -8.9453\n",
      "Time for epoch 325 is 59.2668 sec\n",
      "Epoch 326, gen_loss: -111.2430, disc_loss: -13.1574\n",
      "Time for epoch 326 is 73.8649 sec\n",
      "Epoch 327, gen_loss: 1.9258, disc_loss: -10.4126\n",
      "Time for epoch 327 is 57.1569 sec\n",
      "Epoch 328, gen_loss: 31.0797, disc_loss: -8.1920\n",
      "Time for epoch 328 is 76.5439 sec\n",
      "Epoch 329, gen_loss: -55.2759, disc_loss: -10.7832\n",
      "Time for epoch 329 is 49.5392 sec\n",
      "Epoch 330, gen_loss: 81.4109, disc_loss: -9.5964\n",
      "Time for epoch 330 is 76.2069 sec\n",
      "Epoch 331, gen_loss: 10.0508, disc_loss: -9.4245\n",
      "Time for epoch 331 is 81.9562 sec\n",
      "Epoch 332, gen_loss: -33.0280, disc_loss: -10.6418\n",
      "Time for epoch 332 is 72.3279 sec\n",
      "Epoch 333, gen_loss: -46.1010, disc_loss: -10.9259\n",
      "Time for epoch 333 is 58.4056 sec\n",
      "Epoch 334, gen_loss: 27.9063, disc_loss: -7.9496\n",
      "Time for epoch 334 is 74.5824 sec\n",
      "Epoch 335, gen_loss: 58.8191, disc_loss: -10.6222\n",
      "Time for epoch 335 is 52.9800 sec\n",
      "Epoch 336, gen_loss: 38.9820, disc_loss: -9.8945\n",
      "Time for epoch 336 is 78.2782 sec\n",
      "Epoch 337, gen_loss: 53.6629, disc_loss: -7.5278\n",
      "Time for epoch 337 is 49.0079 sec\n",
      "Epoch 338, gen_loss: 3.5009, disc_loss: -11.7670\n",
      "Time for epoch 338 is 76.5951 sec\n",
      "Epoch 339, gen_loss: -27.3070, disc_loss: -11.9609\n",
      "Time for epoch 339 is 57.0463 sec\n",
      "Epoch 340, gen_loss: -31.1479, disc_loss: -10.6973\n",
      "Time for epoch 340 is 73.1289 sec\n",
      "Epoch 341, gen_loss: -50.9236, disc_loss: -10.2261\n",
      "Time for epoch 341 is 63.0809 sec\n",
      "Epoch 342, gen_loss: -37.1201, disc_loss: -9.1602\n",
      "Time for epoch 342 is 65.9593 sec\n",
      "Epoch 343, gen_loss: -11.9924, disc_loss: -11.5345\n",
      "Time for epoch 343 is 67.8216 sec\n",
      "Epoch 344, gen_loss: -30.3360, disc_loss: -10.4047\n",
      "Time for epoch 344 is 62.6003 sec\n",
      "Epoch 345, gen_loss: -75.5670, disc_loss: -9.8474\n",
      "Time for epoch 345 is 73.6007 sec\n",
      "Epoch 346, gen_loss: 107.6391, disc_loss: -8.2239\n",
      "Time for epoch 346 is 59.7603 sec\n",
      "Epoch 347, gen_loss: -18.5189, disc_loss: -10.9084\n",
      "Time for epoch 347 is 74.3628 sec\n",
      "Epoch 348, gen_loss: -44.9349, disc_loss: -8.8070\n",
      "Time for epoch 348 is 52.8604 sec\n",
      "Epoch 349, gen_loss: 18.8534, disc_loss: -10.1636\n",
      "Time for epoch 349 is 78.4044 sec\n",
      "Epoch 350, gen_loss: -50.6421, disc_loss: -6.7425\n",
      "Time for epoch 350 is 48.9500 sec\n",
      "Epoch 351, gen_loss: -19.5209, disc_loss: -11.6722\n",
      "Time for epoch 351 is 75.7371 sec\n",
      "Epoch 352, gen_loss: -19.9981, disc_loss: -12.3138\n",
      "Time for epoch 352 is 56.0027 sec\n",
      "Epoch 353, gen_loss: 50.6828, disc_loss: -8.5327\n",
      "Time for epoch 353 is 74.7660 sec\n",
      "Epoch 354, gen_loss: -1.0857, disc_loss: -10.5616\n",
      "Time for epoch 354 is 61.9693 sec\n",
      "Epoch 355, gen_loss: -6.8707, disc_loss: -9.3427\n",
      "Time for epoch 355 is 47.5530 sec\n",
      "Epoch 356, gen_loss: -87.4538, disc_loss: -10.2148\n",
      "Time for epoch 356 is 60.5101 sec\n",
      "Epoch 357, gen_loss: -135.4517, disc_loss: -11.2246\n",
      "Time for epoch 357 is 74.9286 sec\n",
      "Epoch 358, gen_loss: -28.6460, disc_loss: -8.0550\n",
      "Time for epoch 358 is 61.0762 sec\n",
      "Epoch 359, gen_loss: -109.3171, disc_loss: -9.9712\n",
      "Time for epoch 359 is 74.1133 sec\n",
      "Epoch 360, gen_loss: 126.9672, disc_loss: -9.6066\n",
      "Time for epoch 360 is 53.0729 sec\n",
      "Epoch 361, gen_loss: 131.3188, disc_loss: -6.2635\n",
      "Time for epoch 361 is 78.9039 sec\n",
      "Epoch 362, gen_loss: -5.2258, disc_loss: -10.4276\n",
      "Time for epoch 362 is 48.8885 sec\n",
      "Epoch 363, gen_loss: 64.9420, disc_loss: -8.5954\n",
      "Time for epoch 363 is 76.7388 sec\n",
      "Epoch 364, gen_loss: 70.2188, disc_loss: -9.0051\n",
      "Time for epoch 364 is 57.2371 sec\n",
      "Epoch 365, gen_loss: -101.3110, disc_loss: -9.3140\n",
      "Time for epoch 365 is 73.0498 sec\n",
      "Epoch 366, gen_loss: 29.5879, disc_loss: -9.5263\n",
      "Time for epoch 366 is 63.1592 sec\n",
      "Epoch 367, gen_loss: -23.2240, disc_loss: -10.3010\n",
      "Time for epoch 367 is 65.5326 sec\n",
      "Epoch 368, gen_loss: 39.9331, disc_loss: -9.2739\n",
      "Time for epoch 368 is 67.8860 sec\n",
      "Epoch 369, gen_loss: 74.5431, disc_loss: -7.8802\n",
      "Time for epoch 369 is 61.1666 sec\n",
      "Epoch 370, gen_loss: 7.6389, disc_loss: -8.5709\n",
      "Time for epoch 370 is 74.7463 sec\n",
      "Epoch 371, gen_loss: 86.0388, disc_loss: -11.4240\n",
      "Time for epoch 371 is 58.0601 sec\n",
      "Epoch 372, gen_loss: 64.0526, disc_loss: -9.7030\n",
      "Time for epoch 372 is 75.4526 sec\n",
      "Epoch 373, gen_loss: -94.1662, disc_loss: -10.0732\n",
      "Time for epoch 373 is 49.2517 sec\n",
      "Epoch 374, gen_loss: -69.4211, disc_loss: -8.6534\n",
      "Time for epoch 374 is 75.9440 sec\n",
      "Epoch 375, gen_loss: -28.8398, disc_loss: -8.6706\n",
      "Time for epoch 375 is 54.2695 sec\n",
      "Epoch 376, gen_loss: 16.9115, disc_loss: -9.1182\n",
      "Time for epoch 376 is 76.6072 sec\n",
      "Epoch 377, gen_loss: -15.4906, disc_loss: -8.7563\n",
      "Time for epoch 377 is 61.0058 sec\n",
      "Epoch 378, gen_loss: 88.2286, disc_loss: -8.7754\n",
      "Time for epoch 378 is 66.5533 sec\n",
      "Epoch 379, gen_loss: 33.4036, disc_loss: -10.6772\n",
      "Time for epoch 379 is 66.8186 sec\n",
      "Epoch 380, gen_loss: -4.3943, disc_loss: -8.3515\n",
      "Time for epoch 380 is 60.3964 sec\n",
      "Epoch 381, gen_loss: 62.6832, disc_loss: -10.1239\n",
      "Time for epoch 381 is 74.4877 sec\n",
      "Epoch 382, gen_loss: 112.7814, disc_loss: -8.4496\n",
      "Time for epoch 382 is 58.3808 sec\n",
      "Epoch 383, gen_loss: 83.8463, disc_loss: -8.7391\n",
      "Time for epoch 383 is 76.2756 sec\n",
      "Epoch 384, gen_loss: 55.7854, disc_loss: -8.4269\n",
      "Time for epoch 384 is 49.9606 sec\n",
      "Epoch 385, gen_loss: 14.2877, disc_loss: -7.5038\n",
      "Time for epoch 385 is 76.3862 sec\n",
      "Epoch 386, gen_loss: 4.0372, disc_loss: -8.6403\n",
      "Time for epoch 386 is 52.5147 sec\n",
      "Epoch 387, gen_loss: -59.1724, disc_loss: -8.7708\n",
      "Time for epoch 387 is 78.5945 sec\n",
      "Epoch 388, gen_loss: -83.2128, disc_loss: -9.3957\n",
      "Time for epoch 388 is 60.4070 sec\n",
      "Epoch 389, gen_loss: -80.9812, disc_loss: -6.6655\n",
      "Time for epoch 389 is 69.0790 sec\n",
      "Epoch 390, gen_loss: -21.5114, disc_loss: -10.1060\n",
      "Time for epoch 390 is 66.8311 sec\n",
      "Epoch 391, gen_loss: -3.0840, disc_loss: -9.6181\n",
      "Time for epoch 391 is 68.6193 sec\n",
      "Epoch 392, gen_loss: -37.2016, disc_loss: -6.5148\n",
      "Time for epoch 392 is 68.0344 sec\n",
      "Epoch 393, gen_loss: 5.0730, disc_loss: -9.8512\n",
      "Time for epoch 393 is 59.9439 sec\n",
      "Epoch 394, gen_loss: 33.6812, disc_loss: -9.6379\n",
      "Time for epoch 394 is 73.4990 sec\n",
      "Epoch 395, gen_loss: -18.5489, disc_loss: -11.4596\n",
      "Time for epoch 395 is 53.3619 sec\n",
      "Epoch 396, gen_loss: 58.3968, disc_loss: -9.9071\n",
      "Time for epoch 396 is 78.0758 sec\n",
      "Epoch 397, gen_loss: 16.3827, disc_loss: -8.7591\n",
      "Time for epoch 397 is 48.9011 sec\n",
      "Epoch 398, gen_loss: 7.4414, disc_loss: -10.4437\n",
      "Time for epoch 398 is 77.3903 sec\n",
      "Epoch 399, gen_loss: 20.9719, disc_loss: -9.3619\n",
      "Time for epoch 399 is 57.7665 sec\n",
      "Epoch 400, gen_loss: -79.5465, disc_loss: -10.2498\n",
      "Time for epoch 400 is 72.2706 sec\n",
      "Epoch 401, gen_loss: -32.8526, disc_loss: -8.8735\n",
      "Time for epoch 401 is 61.5443 sec\n",
      "Epoch 402, gen_loss: -45.5285, disc_loss: -10.5807\n",
      "Time for epoch 402 is 66.1137 sec\n",
      "Epoch 403, gen_loss: -99.1914, disc_loss: -8.2222\n",
      "Time for epoch 403 is 67.1935 sec\n",
      "Epoch 404, gen_loss: -8.0788, disc_loss: -9.3224\n",
      "Time for epoch 404 is 63.2315 sec\n",
      "Epoch 405, gen_loss: -81.4478, disc_loss: -8.3612\n",
      "Time for epoch 405 is 72.9161 sec\n",
      "Epoch 406, gen_loss: -129.3622, disc_loss: -11.9571\n",
      "Time for epoch 406 is 56.8723 sec\n",
      "Epoch 407, gen_loss: -117.2200, disc_loss: -8.0473\n",
      "Time for epoch 407 is 75.9693 sec\n",
      "Epoch 408, gen_loss: -89.1401, disc_loss: -10.2534\n",
      "Time for epoch 408 is 51.4115 sec\n",
      "Epoch 409, gen_loss: -132.1575, disc_loss: -9.5174\n",
      "Time for epoch 409 is 76.3904 sec\n",
      "Epoch 410, gen_loss: -100.9846, disc_loss: -9.4327\n",
      "Time for epoch 410 is 51.1094 sec\n",
      "Epoch 411, gen_loss: -128.0239, disc_loss: -8.9206\n",
      "Time for epoch 411 is 77.7135 sec\n",
      "Epoch 412, gen_loss: -86.1055, disc_loss: -11.5147\n",
      "Time for epoch 412 is 60.5548 sec\n",
      "Epoch 413, gen_loss: -93.7360, disc_loss: -8.4356\n",
      "Time for epoch 413 is 69.7840 sec\n",
      "Epoch 414, gen_loss: -37.0217, disc_loss: -8.2207\n",
      "Time for epoch 414 is 66.1048 sec\n",
      "Epoch 415, gen_loss: -14.6364, disc_loss: -9.4285\n",
      "Time for epoch 415 is 66.9637 sec\n",
      "Epoch 416, gen_loss: 65.4605, disc_loss: -8.4985\n",
      "Time for epoch 416 is 68.3317 sec\n",
      "Epoch 417, gen_loss: 18.7060, disc_loss: -9.6840\n",
      "Time for epoch 417 is 58.7492 sec\n",
      "Epoch 418, gen_loss: 63.8774, disc_loss: -8.1299\n",
      "Time for epoch 418 is 74.0113 sec\n",
      "Epoch 419, gen_loss: 1.9432, disc_loss: -10.0258\n",
      "Time for epoch 419 is 54.8583 sec\n",
      "Epoch 420, gen_loss: 56.8420, disc_loss: -9.1483\n",
      "Time for epoch 420 is 77.5661 sec\n",
      "Epoch 421, gen_loss: -18.6970, disc_loss: -8.6820\n",
      "Time for epoch 421 is 49.1980 sec\n",
      "Epoch 422, gen_loss: -30.8153, disc_loss: -8.6471\n",
      "Time for epoch 422 is 76.3410 sec\n",
      "Epoch 423, gen_loss: -29.4736, disc_loss: -8.0779\n",
      "Time for epoch 423 is 55.2144 sec\n",
      "Epoch 424, gen_loss: -26.2783, disc_loss: -7.9537\n",
      "Time for epoch 424 is 73.5291 sec\n",
      "Epoch 425, gen_loss: -19.8360, disc_loss: -10.2761\n",
      "Time for epoch 425 is 62.6827 sec\n",
      "Epoch 426, gen_loss: 9.7595, disc_loss: -8.7264\n",
      "Time for epoch 426 is 65.2317 sec\n",
      "Epoch 427, gen_loss: -34.9357, disc_loss: -9.0765\n",
      "Time for epoch 427 is 68.4574 sec\n",
      "Epoch 428, gen_loss: -69.5612, disc_loss: -7.8045\n",
      "Time for epoch 428 is 59.1430 sec\n",
      "Epoch 429, gen_loss: -72.3724, disc_loss: -8.4202\n",
      "Time for epoch 429 is 74.3598 sec\n",
      "Epoch 430, gen_loss: -100.5611, disc_loss: -8.0797\n",
      "Time for epoch 430 is 54.4667 sec\n",
      "Epoch 431, gen_loss: -94.4138, disc_loss: -9.5295\n",
      "Time for epoch 431 is 77.8528 sec\n",
      "Epoch 432, gen_loss: -80.2743, disc_loss: -8.2118\n",
      "Time for epoch 432 is 49.0232 sec\n",
      "Epoch 433, gen_loss: -104.3786, disc_loss: -8.4952\n",
      "Time for epoch 433 is 75.7270 sec\n",
      "Epoch 434, gen_loss: -109.5844, disc_loss: -8.5609\n",
      "Time for epoch 434 is 55.9692 sec\n",
      "Epoch 435, gen_loss: -153.7463, disc_loss: -10.0203\n",
      "Time for epoch 435 is 73.8154 sec\n",
      "Epoch 436, gen_loss: -110.6249, disc_loss: -9.3394\n",
      "Time for epoch 436 is 62.0765 sec\n",
      "Epoch 437, gen_loss: -119.1838, disc_loss: -8.8210\n",
      "Time for epoch 437 is 47.1556 sec\n",
      "Epoch 438, gen_loss: -124.0508, disc_loss: -8.6738\n",
      "Time for epoch 438 is 47.0776 sec\n",
      "Epoch 439, gen_loss: -61.8437, disc_loss: -9.8015\n",
      "Time for epoch 439 is 47.1263 sec\n",
      "Epoch 440, gen_loss: -86.2091, disc_loss: -9.0646\n",
      "Time for epoch 440 is 47.0946 sec\n",
      "Epoch 441, gen_loss: -104.2979, disc_loss: -9.7604\n",
      "Time for epoch 441 is 47.1112 sec\n",
      "Epoch 442, gen_loss: -80.7558, disc_loss: -10.5143\n",
      "Time for epoch 442 is 47.1304 sec\n",
      "Epoch 443, gen_loss: -30.2238, disc_loss: -7.7914\n",
      "Time for epoch 443 is 47.1366 sec\n",
      "Epoch 444, gen_loss: -81.0944, disc_loss: -8.1398\n",
      "Time for epoch 444 is 47.1287 sec\n",
      "Epoch 445, gen_loss: -44.1383, disc_loss: -8.6633\n",
      "Time for epoch 445 is 47.0861 sec\n",
      "Epoch 446, gen_loss: -33.2713, disc_loss: -10.4806\n",
      "Time for epoch 446 is 47.0713 sec\n",
      "Epoch 447, gen_loss: -29.3159, disc_loss: -9.2837\n",
      "Time for epoch 447 is 47.0832 sec\n",
      "Epoch 448, gen_loss: -91.1857, disc_loss: -8.4500\n",
      "Time for epoch 448 is 47.0627 sec\n",
      "Epoch 449, gen_loss: -59.0829, disc_loss: -10.7345\n",
      "Time for epoch 449 is 47.0987 sec\n",
      "Epoch 450, gen_loss: -67.3312, disc_loss: -11.4864\n",
      "Time for epoch 450 is 47.1030 sec\n",
      "Epoch 451, gen_loss: -135.9576, disc_loss: -7.8005\n",
      "Time for epoch 451 is 47.0957 sec\n",
      "Epoch 452, gen_loss: -85.8521, disc_loss: -9.0653\n",
      "Time for epoch 452 is 47.0923 sec\n",
      "Epoch 453, gen_loss: -109.5426, disc_loss: -11.3351\n",
      "Time for epoch 453 is 47.1091 sec\n",
      "Epoch 454, gen_loss: -111.8280, disc_loss: -9.2413\n",
      "Time for epoch 454 is 47.0602 sec\n",
      "Epoch 455, gen_loss: -89.7966, disc_loss: -9.1239\n",
      "Time for epoch 455 is 47.1034 sec\n",
      "Epoch 456, gen_loss: -99.9696, disc_loss: -11.1344\n",
      "Time for epoch 456 is 47.1033 sec\n",
      "Epoch 457, gen_loss: -114.1398, disc_loss: -8.2055\n",
      "Time for epoch 457 is 47.0974 sec\n",
      "Epoch 458, gen_loss: -96.6233, disc_loss: -9.4535\n",
      "Time for epoch 458 is 47.1273 sec\n",
      "Epoch 459, gen_loss: -90.5578, disc_loss: -9.7911\n",
      "Time for epoch 459 is 47.1095 sec\n",
      "Epoch 460, gen_loss: -56.0437, disc_loss: -7.4887\n",
      "Time for epoch 460 is 47.1397 sec\n",
      "Epoch 461, gen_loss: -158.0539, disc_loss: -7.8656\n",
      "Time for epoch 461 is 47.1098 sec\n",
      "Epoch 462, gen_loss: -67.0791, disc_loss: -8.9581\n",
      "Time for epoch 462 is 47.1295 sec\n",
      "Epoch 463, gen_loss: -48.7809, disc_loss: -10.3005\n",
      "Time for epoch 463 is 47.1535 sec\n",
      "Epoch 464, gen_loss: -32.3534, disc_loss: -8.7112\n",
      "Time for epoch 464 is 47.1517 sec\n",
      "Epoch 465, gen_loss: 2.0985, disc_loss: -9.5055\n",
      "Time for epoch 465 is 47.1166 sec\n",
      "Epoch 466, gen_loss: 20.0146, disc_loss: -8.5884\n",
      "Time for epoch 466 is 47.1011 sec\n",
      "Epoch 467, gen_loss: -11.4273, disc_loss: -9.1346\n",
      "Time for epoch 467 is 47.0661 sec\n",
      "Epoch 468, gen_loss: -48.3806, disc_loss: -8.4032\n",
      "Time for epoch 468 is 47.0744 sec\n",
      "Epoch 469, gen_loss: -91.4850, disc_loss: -6.9371\n",
      "Time for epoch 469 is 47.0961 sec\n",
      "Epoch 470, gen_loss: 30.1678, disc_loss: -13.2008\n",
      "Time for epoch 470 is 47.0824 sec\n",
      "Epoch 471, gen_loss: -36.7248, disc_loss: -8.2998\n",
      "Time for epoch 471 is 47.1071 sec\n",
      "Epoch 472, gen_loss: -134.7609, disc_loss: -6.8872\n",
      "Time for epoch 472 is 47.0950 sec\n",
      "Epoch 473, gen_loss: -124.4514, disc_loss: -9.8283\n",
      "Time for epoch 473 is 47.2068 sec\n",
      "Epoch 474, gen_loss: -60.3201, disc_loss: -9.5342\n",
      "Time for epoch 474 is 47.0498 sec\n",
      "Epoch 475, gen_loss: -130.7250, disc_loss: -9.5485\n",
      "Time for epoch 475 is 47.1114 sec\n",
      "Epoch 476, gen_loss: -48.4703, disc_loss: -10.2756\n",
      "Time for epoch 476 is 47.0772 sec\n",
      "Epoch 477, gen_loss: -48.5944, disc_loss: -8.5005\n",
      "Time for epoch 477 is 47.0884 sec\n",
      "Epoch 478, gen_loss: -0.8267, disc_loss: -9.6215\n",
      "Time for epoch 478 is 47.0990 sec\n",
      "Epoch 479, gen_loss: -67.8260, disc_loss: -7.9251\n",
      "Time for epoch 479 is 47.0869 sec\n",
      "Epoch 480, gen_loss: -80.8912, disc_loss: -11.0164\n",
      "Time for epoch 480 is 47.0714 sec\n",
      "Epoch 481, gen_loss: -138.9458, disc_loss: -8.8711\n",
      "Time for epoch 481 is 47.0778 sec\n",
      "Epoch 482, gen_loss: -78.2780, disc_loss: -9.0134\n",
      "Time for epoch 482 is 47.1084 sec\n",
      "Epoch 483, gen_loss: -47.8236, disc_loss: -10.3832\n",
      "Time for epoch 483 is 47.1259 sec\n",
      "Epoch 484, gen_loss: -6.7100, disc_loss: -9.9039\n",
      "Time for epoch 484 is 47.0744 sec\n",
      "Epoch 485, gen_loss: 80.4438, disc_loss: -10.7981\n",
      "Time for epoch 485 is 47.1034 sec\n",
      "Epoch 486, gen_loss: -30.7447, disc_loss: -9.2262\n",
      "Time for epoch 486 is 47.0942 sec\n",
      "Epoch 487, gen_loss: 29.6405, disc_loss: -6.5241\n",
      "Time for epoch 487 is 47.1197 sec\n",
      "Epoch 488, gen_loss: 5.4586, disc_loss: -8.8430\n",
      "Time for epoch 488 is 47.1141 sec\n",
      "Epoch 489, gen_loss: -10.3729, disc_loss: -9.3769\n",
      "Time for epoch 489 is 47.0833 sec\n",
      "Epoch 490, gen_loss: -47.5511, disc_loss: -7.6322\n",
      "Time for epoch 490 is 47.1326 sec\n",
      "Epoch 491, gen_loss: -120.5596, disc_loss: -8.0586\n",
      "Time for epoch 491 is 47.0996 sec\n",
      "Epoch 492, gen_loss: -106.6372, disc_loss: -9.1683\n",
      "Time for epoch 492 is 47.1365 sec\n",
      "Epoch 493, gen_loss: -46.4498, disc_loss: -8.3730\n",
      "Time for epoch 493 is 47.1211 sec\n",
      "Epoch 494, gen_loss: -69.6337, disc_loss: -7.7213\n",
      "Time for epoch 494 is 47.0986 sec\n",
      "Epoch 495, gen_loss: -93.1542, disc_loss: -7.2179\n",
      "Time for epoch 495 is 47.0985 sec\n",
      "Epoch 496, gen_loss: -78.8133, disc_loss: -8.8593\n",
      "Time for epoch 496 is 47.1031 sec\n",
      "Epoch 497, gen_loss: -76.4952, disc_loss: -8.3931\n",
      "Time for epoch 497 is 47.1735 sec\n",
      "Epoch 498, gen_loss: -131.9491, disc_loss: -7.4477\n",
      "Time for epoch 498 is 47.1351 sec\n",
      "Epoch 499, gen_loss: -93.9937, disc_loss: -7.8085\n",
      "Time for epoch 499 is 47.1280 sec\n",
      "Epoch 500, gen_loss: -77.3754, disc_loss: -9.7232\n",
      "Time for epoch 500 is 47.1322 sec\n",
      "Epoch 501, gen_loss: -84.9132, disc_loss: -7.2114\n",
      "Time for epoch 501 is 47.0806 sec\n",
      "Epoch 502, gen_loss: -99.5285, disc_loss: -8.8492\n",
      "Time for epoch 502 is 47.2405 sec\n",
      "Epoch 503, gen_loss: -194.0941, disc_loss: -9.5523\n",
      "Time for epoch 503 is 47.1348 sec\n",
      "Epoch 504, gen_loss: -71.4550, disc_loss: -7.9352\n",
      "Time for epoch 504 is 47.1511 sec\n",
      "Epoch 505, gen_loss: -76.0815, disc_loss: -9.1468\n",
      "Time for epoch 505 is 47.1590 sec\n",
      "Epoch 506, gen_loss: 34.1732, disc_loss: -8.4450\n",
      "Time for epoch 506 is 47.1827 sec\n",
      "Epoch 507, gen_loss: -77.5931, disc_loss: -9.3049\n",
      "Time for epoch 507 is 47.3266 sec\n",
      "Epoch 508, gen_loss: -112.7867, disc_loss: -9.4672\n",
      "Time for epoch 508 is 47.1917 sec\n",
      "Epoch 509, gen_loss: -9.9872, disc_loss: -7.6903\n",
      "Time for epoch 509 is 47.1413 sec\n",
      "Epoch 510, gen_loss: -109.4803, disc_loss: -9.5913\n",
      "Time for epoch 510 is 47.2193 sec\n",
      "Epoch 511, gen_loss: -75.1796, disc_loss: -8.9343\n",
      "Time for epoch 511 is 47.1565 sec\n",
      "Epoch 512, gen_loss: 9.7295, disc_loss: -7.9617\n",
      "Time for epoch 512 is 47.1464 sec\n",
      "Epoch 513, gen_loss: -19.2109, disc_loss: -7.8476\n",
      "Time for epoch 513 is 47.1313 sec\n",
      "Epoch 514, gen_loss: -22.6909, disc_loss: -11.0406\n",
      "Time for epoch 514 is 47.1676 sec\n",
      "Epoch 515, gen_loss: 40.7413, disc_loss: -9.4368\n",
      "Time for epoch 515 is 47.1969 sec\n",
      "Epoch 516, gen_loss: 83.2135, disc_loss: -5.1937\n",
      "Time for epoch 516 is 47.1615 sec\n",
      "Epoch 517, gen_loss: 94.8194, disc_loss: -9.0206\n",
      "Time for epoch 517 is 47.1769 sec\n",
      "Epoch 518, gen_loss: 5.9958, disc_loss: -7.4778\n",
      "Time for epoch 518 is 47.1383 sec\n",
      "Epoch 519, gen_loss: 4.3802, disc_loss: -7.2501\n",
      "Time for epoch 519 is 47.1314 sec\n",
      "Epoch 520, gen_loss: 88.7106, disc_loss: -9.3102\n",
      "Time for epoch 520 is 47.1667 sec\n",
      "Epoch 521, gen_loss: -0.8110, disc_loss: -8.2982\n",
      "Time for epoch 521 is 47.1748 sec\n",
      "Epoch 522, gen_loss: 29.5633, disc_loss: -9.2046\n",
      "Time for epoch 522 is 47.1341 sec\n",
      "Epoch 523, gen_loss: 67.3653, disc_loss: -6.9869\n",
      "Time for epoch 523 is 47.1547 sec\n",
      "Epoch 524, gen_loss: 85.2317, disc_loss: -8.1725\n",
      "Time for epoch 524 is 47.1468 sec\n",
      "Epoch 525, gen_loss: 42.6815, disc_loss: -7.8451\n",
      "Time for epoch 525 is 47.1447 sec\n",
      "Epoch 526, gen_loss: -3.5377, disc_loss: -7.1975\n",
      "Time for epoch 526 is 47.1443 sec\n",
      "Epoch 527, gen_loss: 20.2846, disc_loss: -8.0130\n",
      "Time for epoch 527 is 47.1618 sec\n",
      "Epoch 528, gen_loss: -26.1292, disc_loss: -9.5791\n",
      "Time for epoch 528 is 47.1536 sec\n",
      "Epoch 529, gen_loss: -126.7652, disc_loss: -8.4417\n",
      "Time for epoch 529 is 47.1739 sec\n",
      "Epoch 530, gen_loss: -54.6785, disc_loss: -8.1572\n",
      "Time for epoch 530 is 47.1083 sec\n",
      "Epoch 531, gen_loss: -8.0696, disc_loss: -7.9435\n",
      "Time for epoch 531 is 47.1629 sec\n",
      "Epoch 532, gen_loss: -38.1212, disc_loss: -7.1941\n",
      "Time for epoch 532 is 47.1704 sec\n",
      "Epoch 533, gen_loss: -46.8400, disc_loss: -8.2851\n",
      "Time for epoch 533 is 47.1546 sec\n",
      "Epoch 534, gen_loss: -49.4220, disc_loss: -7.4556\n",
      "Time for epoch 534 is 47.1308 sec\n",
      "Epoch 535, gen_loss: -20.8251, disc_loss: -6.8338\n",
      "Time for epoch 535 is 47.1576 sec\n",
      "Epoch 536, gen_loss: -26.7052, disc_loss: -8.3873\n",
      "Time for epoch 536 is 47.1379 sec\n",
      "Epoch 537, gen_loss: -30.1870, disc_loss: -8.0656\n",
      "Time for epoch 537 is 47.1564 sec\n",
      "Epoch 538, gen_loss: -37.4332, disc_loss: -8.2356\n",
      "Time for epoch 538 is 47.1102 sec\n",
      "Epoch 539, gen_loss: -124.1025, disc_loss: -7.1256\n",
      "Time for epoch 539 is 47.1488 sec\n",
      "Epoch 540, gen_loss: -87.7580, disc_loss: -8.6973\n",
      "Time for epoch 540 is 47.1721 sec\n",
      "Epoch 541, gen_loss: -102.8482, disc_loss: -8.7756\n",
      "Time for epoch 541 is 47.1571 sec\n",
      "Epoch 542, gen_loss: -64.9022, disc_loss: -7.8959\n",
      "Time for epoch 542 is 47.1587 sec\n",
      "Epoch 543, gen_loss: -45.5804, disc_loss: -8.6505\n",
      "Time for epoch 543 is 47.1556 sec\n",
      "Epoch 544, gen_loss: -39.1304, disc_loss: -7.4905\n",
      "Time for epoch 544 is 47.1331 sec\n",
      "Epoch 545, gen_loss: -70.0366, disc_loss: -9.3318\n",
      "Time for epoch 545 is 47.1553 sec\n",
      "Epoch 546, gen_loss: -52.0505, disc_loss: -9.1530\n",
      "Time for epoch 546 is 47.1907 sec\n",
      "Epoch 547, gen_loss: -35.2827, disc_loss: -7.3520\n",
      "Time for epoch 547 is 47.1690 sec\n",
      "Epoch 548, gen_loss: -33.5542, disc_loss: -8.3249\n",
      "Time for epoch 548 is 47.1460 sec\n",
      "Epoch 549, gen_loss: -53.2154, disc_loss: -8.9486\n",
      "Time for epoch 549 is 47.1268 sec\n",
      "Epoch 550, gen_loss: -34.9203, disc_loss: -7.8956\n",
      "Time for epoch 550 is 47.1428 sec\n",
      "Epoch 551, gen_loss: -58.5790, disc_loss: -8.4242\n",
      "Time for epoch 551 is 47.1671 sec\n",
      "Epoch 552, gen_loss: -113.9226, disc_loss: -7.3001\n",
      "Time for epoch 552 is 47.1325 sec\n",
      "Epoch 553, gen_loss: -45.5570, disc_loss: -9.1703\n",
      "Time for epoch 553 is 47.1388 sec\n",
      "Epoch 554, gen_loss: -99.9928, disc_loss: -7.8591\n",
      "Time for epoch 554 is 47.1610 sec\n",
      "Epoch 555, gen_loss: -121.3420, disc_loss: -7.9164\n",
      "Time for epoch 555 is 47.1203 sec\n",
      "Epoch 556, gen_loss: -100.7826, disc_loss: -7.7824\n",
      "Time for epoch 556 is 47.1480 sec\n",
      "Epoch 557, gen_loss: -136.5804, disc_loss: -8.2091\n",
      "Time for epoch 557 is 47.1392 sec\n",
      "Epoch 558, gen_loss: -92.2765, disc_loss: -9.8870\n",
      "Time for epoch 558 is 47.1381 sec\n",
      "Epoch 559, gen_loss: -98.1119, disc_loss: -7.5588\n",
      "Time for epoch 559 is 47.0902 sec\n",
      "Epoch 560, gen_loss: -72.2936, disc_loss: -9.0793\n",
      "Time for epoch 560 is 47.1328 sec\n",
      "Epoch 561, gen_loss: -15.3278, disc_loss: -7.8405\n",
      "Time for epoch 561 is 47.1384 sec\n",
      "Epoch 562, gen_loss: -6.9823, disc_loss: -7.0421\n",
      "Time for epoch 562 is 47.1329 sec\n",
      "Epoch 563, gen_loss: -1.4593, disc_loss: -9.5898\n",
      "Time for epoch 563 is 47.1679 sec\n",
      "Epoch 564, gen_loss: 28.8480, disc_loss: -8.1552\n",
      "Time for epoch 564 is 47.1087 sec\n",
      "Epoch 565, gen_loss: -27.3874, disc_loss: -8.3408\n",
      "Time for epoch 565 is 47.1073 sec\n",
      "Epoch 566, gen_loss: -129.9031, disc_loss: -8.9541\n",
      "Time for epoch 566 is 47.1178 sec\n",
      "Epoch 567, gen_loss: -78.8002, disc_loss: -7.8105\n",
      "Time for epoch 567 is 47.1318 sec\n",
      "Epoch 568, gen_loss: -100.9042, disc_loss: -7.9085\n",
      "Time for epoch 568 is 47.1488 sec\n",
      "Epoch 569, gen_loss: -17.3591, disc_loss: -7.1505\n",
      "Time for epoch 569 is 47.1100 sec\n",
      "Epoch 570, gen_loss: -31.0358, disc_loss: -8.6562\n",
      "Time for epoch 570 is 47.1259 sec\n",
      "Epoch 571, gen_loss: -35.5987, disc_loss: -8.7446\n",
      "Time for epoch 571 is 47.1247 sec\n",
      "Epoch 572, gen_loss: 10.1586, disc_loss: -7.6504\n",
      "Time for epoch 572 is 47.1569 sec\n",
      "Epoch 573, gen_loss: -97.4755, disc_loss: -7.8323\n",
      "Time for epoch 573 is 47.1056 sec\n",
      "Epoch 574, gen_loss: -62.1414, disc_loss: -7.9686\n",
      "Time for epoch 574 is 47.1514 sec\n",
      "Epoch 575, gen_loss: -53.2464, disc_loss: -8.2564\n",
      "Time for epoch 575 is 47.1032 sec\n",
      "Epoch 576, gen_loss: -57.4142, disc_loss: -9.4027\n",
      "Time for epoch 576 is 47.1250 sec\n",
      "Epoch 577, gen_loss: -76.1219, disc_loss: -9.1954\n",
      "Time for epoch 577 is 47.1473 sec\n",
      "Epoch 578, gen_loss: -70.1675, disc_loss: -8.9424\n",
      "Time for epoch 578 is 47.1310 sec\n",
      "Epoch 579, gen_loss: -56.9420, disc_loss: -8.6373\n",
      "Time for epoch 579 is 47.1546 sec\n",
      "Epoch 580, gen_loss: -81.9422, disc_loss: -9.2718\n",
      "Time for epoch 580 is 47.1339 sec\n",
      "Epoch 581, gen_loss: -25.5185, disc_loss: -9.3113\n",
      "Time for epoch 581 is 47.1214 sec\n",
      "Epoch 582, gen_loss: -27.6334, disc_loss: -7.3756\n",
      "Time for epoch 582 is 47.0753 sec\n",
      "Epoch 583, gen_loss: 7.8176, disc_loss: -8.6595\n",
      "Time for epoch 583 is 47.1300 sec\n",
      "Epoch 584, gen_loss: -11.4981, disc_loss: -7.6208\n",
      "Time for epoch 584 is 47.1117 sec\n",
      "Epoch 585, gen_loss: -19.8714, disc_loss: -8.1350\n",
      "Time for epoch 585 is 47.1068 sec\n",
      "Epoch 586, gen_loss: -39.5747, disc_loss: -9.0164\n",
      "Time for epoch 586 is 47.0972 sec\n",
      "Epoch 587, gen_loss: -69.6509, disc_loss: -7.6230\n",
      "Time for epoch 587 is 47.1207 sec\n",
      "Epoch 588, gen_loss: -15.3141, disc_loss: -8.1421\n",
      "Time for epoch 588 is 47.1129 sec\n",
      "Epoch 589, gen_loss: 20.6964, disc_loss: -7.3961\n",
      "Time for epoch 589 is 47.1392 sec\n",
      "Epoch 590, gen_loss: -86.2754, disc_loss: -8.0454\n",
      "Time for epoch 590 is 47.1338 sec\n",
      "Epoch 591, gen_loss: -68.5660, disc_loss: -8.3976\n",
      "Time for epoch 591 is 47.1210 sec\n",
      "Epoch 592, gen_loss: -91.5534, disc_loss: -7.8693\n",
      "Time for epoch 592 is 47.1110 sec\n",
      "Epoch 593, gen_loss: -110.9973, disc_loss: -8.7138\n",
      "Time for epoch 593 is 47.1357 sec\n",
      "Epoch 594, gen_loss: -132.1608, disc_loss: -7.4600\n",
      "Time for epoch 594 is 47.1647 sec\n",
      "Epoch 595, gen_loss: -104.4121, disc_loss: -7.3917\n",
      "Time for epoch 595 is 47.1040 sec\n",
      "Epoch 596, gen_loss: -143.6991, disc_loss: -8.1628\n",
      "Time for epoch 596 is 47.1540 sec\n",
      "Epoch 597, gen_loss: -107.5318, disc_loss: -8.4908\n",
      "Time for epoch 597 is 47.1120 sec\n",
      "Epoch 598, gen_loss: -8.9638, disc_loss: -9.2764\n",
      "Time for epoch 598 is 47.1368 sec\n",
      "Epoch 599, gen_loss: -26.4928, disc_loss: -7.1283\n",
      "Time for epoch 599 is 47.1446 sec\n",
      "Epoch 600, gen_loss: -48.8302, disc_loss: -9.1399\n",
      "Time for epoch 600 is 47.0963 sec\n",
      "Epoch 601, gen_loss: -130.6246, disc_loss: -8.5064\n",
      "Time for epoch 601 is 47.1529 sec\n",
      "Epoch 602, gen_loss: -92.2946, disc_loss: -6.8495\n",
      "Time for epoch 602 is 47.1447 sec\n",
      "Epoch 603, gen_loss: -74.4274, disc_loss: -8.9144\n",
      "Time for epoch 603 is 47.1189 sec\n",
      "Epoch 604, gen_loss: -87.4024, disc_loss: -7.1217\n",
      "Time for epoch 604 is 47.1576 sec\n",
      "Epoch 605, gen_loss: -49.9909, disc_loss: -9.0741\n",
      "Time for epoch 605 is 47.1256 sec\n",
      "Epoch 606, gen_loss: -26.6276, disc_loss: -8.1374\n",
      "Time for epoch 606 is 47.1150 sec\n",
      "Epoch 607, gen_loss: -34.5055, disc_loss: -5.1560\n",
      "Time for epoch 607 is 47.1322 sec\n",
      "Epoch 608, gen_loss: -65.5807, disc_loss: -7.7749\n",
      "Time for epoch 608 is 47.0716 sec\n",
      "Epoch 609, gen_loss: -3.6987, disc_loss: -9.6974\n",
      "Time for epoch 609 is 47.1379 sec\n",
      "Epoch 610, gen_loss: -77.4831, disc_loss: -8.7553\n",
      "Time for epoch 610 is 47.1362 sec\n",
      "Epoch 611, gen_loss: -70.0192, disc_loss: -8.9755\n",
      "Time for epoch 611 is 47.0864 sec\n",
      "Epoch 612, gen_loss: -21.4982, disc_loss: -10.7901\n",
      "Time for epoch 612 is 47.0961 sec\n",
      "Epoch 613, gen_loss: -112.5352, disc_loss: -7.2190\n",
      "Time for epoch 613 is 47.1533 sec\n",
      "Epoch 614, gen_loss: -128.0263, disc_loss: -8.8596\n",
      "Time for epoch 614 is 47.1291 sec\n",
      "Epoch 615, gen_loss: 2.9041, disc_loss: -8.0924\n",
      "Time for epoch 615 is 47.1104 sec\n",
      "Epoch 616, gen_loss: -47.1516, disc_loss: -8.1448\n",
      "Time for epoch 616 is 47.1345 sec\n",
      "Epoch 617, gen_loss: -94.2586, disc_loss: -8.8539\n",
      "Time for epoch 617 is 47.1718 sec\n",
      "Epoch 618, gen_loss: -147.9779, disc_loss: -7.6837\n",
      "Time for epoch 618 is 47.1411 sec\n",
      "Epoch 619, gen_loss: -138.5471, disc_loss: -6.9159\n",
      "Time for epoch 619 is 47.0777 sec\n",
      "Epoch 620, gen_loss: -29.2379, disc_loss: -9.2137\n",
      "Time for epoch 620 is 47.1248 sec\n",
      "Epoch 621, gen_loss: -135.0929, disc_loss: -8.1326\n",
      "Time for epoch 621 is 47.1033 sec\n",
      "Epoch 622, gen_loss: -117.5547, disc_loss: -5.8759\n",
      "Time for epoch 622 is 47.1550 sec\n",
      "Epoch 623, gen_loss: -76.7902, disc_loss: -8.9477\n",
      "Time for epoch 623 is 47.1409 sec\n",
      "Epoch 624, gen_loss: -108.4624, disc_loss: -9.2992\n",
      "Time for epoch 624 is 47.1610 sec\n",
      "Epoch 625, gen_loss: -23.4516, disc_loss: -8.0218\n",
      "Time for epoch 625 is 47.1088 sec\n",
      "Epoch 626, gen_loss: -0.2393, disc_loss: -6.5567\n",
      "Time for epoch 626 is 47.1214 sec\n",
      "Epoch 627, gen_loss: -102.0147, disc_loss: -7.7632\n",
      "Time for epoch 627 is 47.0657 sec\n",
      "Epoch 628, gen_loss: -64.1965, disc_loss: -9.0298\n",
      "Time for epoch 628 is 47.0908 sec\n",
      "Epoch 629, gen_loss: -20.1648, disc_loss: -6.1836\n",
      "Time for epoch 629 is 47.1063 sec\n",
      "Epoch 630, gen_loss: -94.1495, disc_loss: -8.7749\n",
      "Time for epoch 630 is 47.1111 sec\n",
      "Epoch 631, gen_loss: -25.3851, disc_loss: -6.5950\n",
      "Time for epoch 631 is 47.0605 sec\n",
      "Epoch 632, gen_loss: -37.2096, disc_loss: -7.1786\n",
      "Time for epoch 632 is 47.0382 sec\n",
      "Epoch 633, gen_loss: -116.2629, disc_loss: -7.0443\n",
      "Time for epoch 633 is 47.0657 sec\n",
      "Epoch 634, gen_loss: -38.7707, disc_loss: -7.9189\n",
      "Time for epoch 634 is 47.1158 sec\n",
      "Epoch 635, gen_loss: -40.1687, disc_loss: -7.5316\n",
      "Time for epoch 635 is 47.0987 sec\n",
      "Epoch 636, gen_loss: -35.9327, disc_loss: -7.9467\n",
      "Time for epoch 636 is 47.1122 sec\n",
      "Epoch 637, gen_loss: -56.7010, disc_loss: -7.5147\n",
      "Time for epoch 637 is 47.1060 sec\n",
      "Epoch 638, gen_loss: -50.3950, disc_loss: -7.1608\n",
      "Time for epoch 638 is 47.0612 sec\n",
      "Epoch 639, gen_loss: -28.4514, disc_loss: -7.1064\n",
      "Time for epoch 639 is 47.0844 sec\n",
      "Epoch 640, gen_loss: -20.1194, disc_loss: -7.7843\n",
      "Time for epoch 640 is 47.1389 sec\n",
      "Epoch 641, gen_loss: -26.5211, disc_loss: -7.1504\n",
      "Time for epoch 641 is 47.1098 sec\n",
      "Epoch 642, gen_loss: -32.1564, disc_loss: -8.0513\n",
      "Time for epoch 642 is 47.0810 sec\n",
      "Epoch 643, gen_loss: -31.2276, disc_loss: -7.7604\n",
      "Time for epoch 643 is 47.1442 sec\n",
      "Epoch 644, gen_loss: -48.1674, disc_loss: -8.1832\n",
      "Time for epoch 644 is 47.1017 sec\n",
      "Epoch 645, gen_loss: -35.3650, disc_loss: -9.1524\n",
      "Time for epoch 645 is 47.1103 sec\n",
      "Epoch 646, gen_loss: -10.6476, disc_loss: -7.2609\n",
      "Time for epoch 646 is 47.1007 sec\n",
      "Epoch 647, gen_loss: -32.4061, disc_loss: -7.4019\n",
      "Time for epoch 647 is 47.0444 sec\n",
      "Epoch 648, gen_loss: -49.9206, disc_loss: -9.4331\n",
      "Time for epoch 648 is 47.1268 sec\n",
      "Epoch 649, gen_loss: -21.2561, disc_loss: -7.6645\n",
      "Time for epoch 649 is 47.1197 sec\n",
      "Epoch 650, gen_loss: -64.2054, disc_loss: -8.6022\n",
      "Time for epoch 650 is 47.1219 sec\n",
      "Epoch 651, gen_loss: -38.5497, disc_loss: -7.5460\n",
      "Time for epoch 651 is 47.1007 sec\n",
      "Epoch 652, gen_loss: -28.2893, disc_loss: -7.0148\n",
      "Time for epoch 652 is 47.1398 sec\n",
      "Epoch 653, gen_loss: -53.9516, disc_loss: -8.9797\n",
      "Time for epoch 653 is 47.1027 sec\n",
      "Epoch 654, gen_loss: -14.5813, disc_loss: -7.8010\n",
      "Time for epoch 654 is 47.1257 sec\n",
      "Epoch 655, gen_loss: -76.4149, disc_loss: -7.5766\n",
      "Time for epoch 655 is 47.1042 sec\n",
      "Epoch 656, gen_loss: -2.1138, disc_loss: -8.0862\n",
      "Time for epoch 656 is 47.0997 sec\n",
      "Epoch 657, gen_loss: -42.5686, disc_loss: -7.1606\n",
      "Time for epoch 657 is 47.1255 sec\n",
      "Epoch 658, gen_loss: -57.1651, disc_loss: -7.5299\n",
      "Time for epoch 658 is 47.0866 sec\n",
      "Epoch 659, gen_loss: 0.3374, disc_loss: -8.1420\n",
      "Time for epoch 659 is 47.0948 sec\n",
      "Epoch 660, gen_loss: -39.9748, disc_loss: -8.6555\n",
      "Time for epoch 660 is 47.0981 sec\n",
      "Epoch 661, gen_loss: -0.4745, disc_loss: -8.5364\n",
      "Time for epoch 661 is 47.0929 sec\n",
      "Epoch 662, gen_loss: 37.6085, disc_loss: -7.2154\n",
      "Time for epoch 662 is 47.0827 sec\n",
      "Epoch 663, gen_loss: -25.1542, disc_loss: -8.3866\n",
      "Time for epoch 663 is 47.0672 sec\n",
      "Epoch 664, gen_loss: -26.0794, disc_loss: -7.9790\n",
      "Time for epoch 664 is 47.1117 sec\n",
      "Epoch 665, gen_loss: -33.3306, disc_loss: -9.2601\n",
      "Time for epoch 665 is 47.0875 sec\n",
      "Epoch 666, gen_loss: -104.1619, disc_loss: -8.2489\n",
      "Time for epoch 666 is 47.0943 sec\n",
      "Epoch 667, gen_loss: -58.7260, disc_loss: -7.8744\n",
      "Time for epoch 667 is 47.0961 sec\n",
      "Epoch 668, gen_loss: 25.8475, disc_loss: -7.7216\n",
      "Time for epoch 668 is 47.1311 sec\n",
      "Epoch 669, gen_loss: -94.2503, disc_loss: -8.7780\n",
      "Time for epoch 669 is 47.0861 sec\n",
      "Epoch 670, gen_loss: 16.5602, disc_loss: -8.0173\n",
      "Time for epoch 670 is 47.1382 sec\n",
      "Epoch 671, gen_loss: -2.5859, disc_loss: -6.9944\n",
      "Time for epoch 671 is 47.0784 sec\n",
      "Epoch 672, gen_loss: -62.3872, disc_loss: -8.6498\n",
      "Time for epoch 672 is 47.0853 sec\n",
      "Epoch 673, gen_loss: 10.3445, disc_loss: -7.9070\n",
      "Time for epoch 673 is 47.1328 sec\n",
      "Epoch 674, gen_loss: -8.5749, disc_loss: -6.4111\n",
      "Time for epoch 674 is 47.0613 sec\n",
      "Epoch 675, gen_loss: -71.7530, disc_loss: -7.0568\n",
      "Time for epoch 675 is 47.1001 sec\n",
      "Epoch 676, gen_loss: -30.3525, disc_loss: -7.6609\n",
      "Time for epoch 676 is 47.0825 sec\n",
      "Epoch 677, gen_loss: -17.7026, disc_loss: -8.9959\n",
      "Time for epoch 677 is 47.1187 sec\n",
      "Epoch 678, gen_loss: -49.7947, disc_loss: -8.4167\n",
      "Time for epoch 678 is 47.0843 sec\n",
      "Epoch 679, gen_loss: 54.7525, disc_loss: -7.6467\n",
      "Time for epoch 679 is 47.1227 sec\n",
      "Epoch 680, gen_loss: 27.1801, disc_loss: -8.0978\n",
      "Time for epoch 680 is 47.1142 sec\n",
      "Epoch 681, gen_loss: 15.3410, disc_loss: -8.4147\n",
      "Time for epoch 681 is 47.0921 sec\n",
      "Epoch 682, gen_loss: -16.7775, disc_loss: -7.5951\n",
      "Time for epoch 682 is 47.1286 sec\n",
      "Epoch 683, gen_loss: -12.7930, disc_loss: -7.5306\n",
      "Time for epoch 683 is 47.1079 sec\n",
      "Epoch 684, gen_loss: -23.1226, disc_loss: -7.4977\n",
      "Time for epoch 684 is 47.0918 sec\n",
      "Epoch 685, gen_loss: -39.4497, disc_loss: -7.3589\n",
      "Time for epoch 685 is 47.0887 sec\n",
      "Epoch 686, gen_loss: -20.7130, disc_loss: -7.9040\n",
      "Time for epoch 686 is 47.0929 sec\n",
      "Epoch 687, gen_loss: -28.2816, disc_loss: -8.7254\n",
      "Time for epoch 687 is 47.0992 sec\n",
      "Epoch 688, gen_loss: -24.4852, disc_loss: -7.6194\n",
      "Time for epoch 688 is 47.0895 sec\n",
      "Epoch 689, gen_loss: -30.7164, disc_loss: -6.7893\n",
      "Time for epoch 689 is 47.0919 sec\n",
      "Epoch 690, gen_loss: -39.4662, disc_loss: -8.2584\n",
      "Time for epoch 690 is 47.0799 sec\n",
      "Epoch 691, gen_loss: -55.7586, disc_loss: -6.4654\n",
      "Time for epoch 691 is 47.1039 sec\n",
      "Epoch 692, gen_loss: -66.8763, disc_loss: -7.3016\n",
      "Time for epoch 692 is 47.1294 sec\n",
      "Epoch 693, gen_loss: -50.4972, disc_loss: -8.4094\n",
      "Time for epoch 693 is 47.0943 sec\n",
      "Epoch 694, gen_loss: -92.2405, disc_loss: -8.1834\n",
      "Time for epoch 694 is 47.1235 sec\n",
      "Epoch 695, gen_loss: -39.0788, disc_loss: -7.5782\n",
      "Time for epoch 695 is 47.1165 sec\n",
      "Epoch 696, gen_loss: -52.7147, disc_loss: -7.8308\n",
      "Time for epoch 696 is 47.0673 sec\n",
      "Epoch 697, gen_loss: -57.1693, disc_loss: -8.4877\n",
      "Time for epoch 697 is 47.1071 sec\n",
      "Epoch 698, gen_loss: -38.4900, disc_loss: -7.8168\n",
      "Time for epoch 698 is 47.1150 sec\n",
      "Epoch 699, gen_loss: -63.2325, disc_loss: -7.8726\n",
      "Time for epoch 699 is 47.1365 sec\n",
      "Epoch 700, gen_loss: 2.8030, disc_loss: -7.9675\n",
      "Time for epoch 700 is 47.1099 sec\n",
      "Epoch 701, gen_loss: -8.7269, disc_loss: -7.3454\n",
      "Time for epoch 701 is 47.1012 sec\n",
      "Epoch 702, gen_loss: -72.0192, disc_loss: -8.5102\n",
      "Time for epoch 702 is 47.1199 sec\n",
      "Epoch 703, gen_loss: -70.0990, disc_loss: -6.4982\n",
      "Time for epoch 703 is 47.1286 sec\n",
      "Epoch 704, gen_loss: -64.1267, disc_loss: -7.8279\n",
      "Time for epoch 704 is 47.1249 sec\n",
      "Epoch 705, gen_loss: -25.9289, disc_loss: -8.8097\n",
      "Time for epoch 705 is 47.1249 sec\n",
      "Epoch 706, gen_loss: -84.5925, disc_loss: -8.3503\n",
      "Time for epoch 706 is 47.1126 sec\n",
      "Epoch 707, gen_loss: -38.7885, disc_loss: -7.1618\n",
      "Time for epoch 707 is 47.1288 sec\n",
      "Epoch 708, gen_loss: -58.4208, disc_loss: -7.9266\n",
      "Time for epoch 708 is 47.0951 sec\n",
      "Epoch 709, gen_loss: -83.9389, disc_loss: -8.1731\n",
      "Time for epoch 709 is 47.1024 sec\n",
      "Epoch 710, gen_loss: 9.0957, disc_loss: -8.0583\n",
      "Time for epoch 710 is 47.1446 sec\n",
      "Epoch 711, gen_loss: -39.4905, disc_loss: -8.2963\n",
      "Time for epoch 711 is 47.1313 sec\n",
      "Epoch 712, gen_loss: 39.1918, disc_loss: -7.9076\n",
      "Time for epoch 712 is 47.1519 sec\n",
      "Epoch 713, gen_loss: 7.0489, disc_loss: -7.9639\n",
      "Time for epoch 713 is 47.1076 sec\n",
      "Epoch 714, gen_loss: -18.7575, disc_loss: -8.7227\n",
      "Time for epoch 714 is 47.1417 sec\n",
      "Epoch 715, gen_loss: -26.4300, disc_loss: -7.3684\n",
      "Time for epoch 715 is 47.1326 sec\n",
      "Epoch 716, gen_loss: -105.1718, disc_loss: -8.4867\n",
      "Time for epoch 716 is 47.1330 sec\n",
      "Epoch 717, gen_loss: -58.0465, disc_loss: -9.6778\n",
      "Time for epoch 717 is 47.1485 sec\n",
      "Epoch 718, gen_loss: -26.7647, disc_loss: -5.5514\n",
      "Time for epoch 718 is 47.1608 sec\n",
      "Epoch 719, gen_loss: -144.8669, disc_loss: -7.9469\n",
      "Time for epoch 719 is 47.1171 sec\n",
      "Epoch 720, gen_loss: -53.7942, disc_loss: -9.0377\n",
      "Time for epoch 720 is 47.0768 sec\n",
      "Epoch 721, gen_loss: -64.8486, disc_loss: -7.3082\n",
      "Time for epoch 721 is 47.1220 sec\n",
      "Epoch 722, gen_loss: -23.0557, disc_loss: -8.5738\n",
      "Time for epoch 722 is 47.0931 sec\n",
      "Epoch 723, gen_loss: 86.3759, disc_loss: -8.2123\n",
      "Time for epoch 723 is 47.1350 sec\n",
      "Epoch 724, gen_loss: -17.5394, disc_loss: -8.6740\n",
      "Time for epoch 724 is 47.1202 sec\n",
      "Epoch 725, gen_loss: -8.4159, disc_loss: -6.7384\n",
      "Time for epoch 725 is 47.1137 sec\n",
      "Epoch 726, gen_loss: 33.2169, disc_loss: -8.5683\n",
      "Time for epoch 726 is 47.1147 sec\n",
      "Epoch 727, gen_loss: -11.3343, disc_loss: -7.4553\n",
      "Time for epoch 727 is 47.1001 sec\n",
      "Epoch 728, gen_loss: -53.4269, disc_loss: -7.9727\n",
      "Time for epoch 728 is 47.1427 sec\n",
      "Epoch 729, gen_loss: -14.3266, disc_loss: -8.2876\n",
      "Time for epoch 729 is 47.1045 sec\n",
      "Epoch 730, gen_loss: -61.8243, disc_loss: -8.1983\n",
      "Time for epoch 730 is 47.1148 sec\n",
      "Epoch 731, gen_loss: -68.6244, disc_loss: -8.1984\n",
      "Time for epoch 731 is 47.0988 sec\n",
      "Epoch 732, gen_loss: -5.2154, disc_loss: -7.9593\n",
      "Time for epoch 732 is 47.1147 sec\n",
      "Epoch 733, gen_loss: -7.3229, disc_loss: -7.0249\n",
      "Time for epoch 733 is 47.1478 sec\n",
      "Epoch 734, gen_loss: -19.5526, disc_loss: -7.6909\n",
      "Time for epoch 734 is 47.1462 sec\n",
      "Epoch 735, gen_loss: -57.4109, disc_loss: -8.6569\n",
      "Time for epoch 735 is 47.1276 sec\n",
      "Epoch 736, gen_loss: -33.2044, disc_loss: -7.0984\n",
      "Time for epoch 736 is 47.1145 sec\n",
      "Epoch 737, gen_loss: -31.6166, disc_loss: -7.5099\n",
      "Time for epoch 737 is 47.0615 sec\n",
      "Epoch 738, gen_loss: -70.3301, disc_loss: -8.9986\n",
      "Time for epoch 738 is 47.1031 sec\n",
      "Epoch 739, gen_loss: 11.6424, disc_loss: -8.9349\n",
      "Time for epoch 739 is 47.1380 sec\n",
      "Epoch 740, gen_loss: -66.5069, disc_loss: -6.8009\n",
      "Time for epoch 740 is 47.1184 sec\n",
      "Epoch 741, gen_loss: -64.3601, disc_loss: -8.0384\n",
      "Time for epoch 741 is 47.1467 sec\n",
      "Epoch 742, gen_loss: -16.9951, disc_loss: -7.0155\n",
      "Time for epoch 742 is 47.0988 sec\n",
      "Epoch 743, gen_loss: -80.9110, disc_loss: -8.8578\n",
      "Time for epoch 743 is 47.1447 sec\n",
      "Epoch 744, gen_loss: -17.0880, disc_loss: -7.8896\n",
      "Time for epoch 744 is 47.1004 sec\n",
      "Epoch 745, gen_loss: -3.7287, disc_loss: -8.0226\n",
      "Time for epoch 745 is 47.0946 sec\n",
      "Epoch 746, gen_loss: -65.1196, disc_loss: -7.5662\n",
      "Time for epoch 746 is 47.1013 sec\n",
      "Epoch 747, gen_loss: -47.2104, disc_loss: -7.0055\n",
      "Time for epoch 747 is 47.1565 sec\n",
      "Epoch 748, gen_loss: -23.6837, disc_loss: -6.8414\n",
      "Time for epoch 748 is 47.1266 sec\n",
      "Epoch 749, gen_loss: -18.2162, disc_loss: -8.0947\n",
      "Time for epoch 749 is 47.1371 sec\n",
      "Epoch 750, gen_loss: -64.6287, disc_loss: -7.6635\n",
      "Time for epoch 750 is 47.1404 sec\n",
      "Epoch 751, gen_loss: -10.6307, disc_loss: -8.3888\n",
      "Time for epoch 751 is 47.1268 sec\n",
      "Epoch 752, gen_loss: -61.1030, disc_loss: -7.9498\n",
      "Time for epoch 752 is 47.1006 sec\n",
      "Epoch 753, gen_loss: -34.2248, disc_loss: -7.9588\n",
      "Time for epoch 753 is 47.1496 sec\n",
      "Epoch 754, gen_loss: 28.3494, disc_loss: -6.9602\n",
      "Time for epoch 754 is 47.1385 sec\n",
      "Epoch 755, gen_loss: -55.4229, disc_loss: -8.8173\n",
      "Time for epoch 755 is 47.1118 sec\n",
      "Epoch 756, gen_loss: 37.2861, disc_loss: -8.2961\n",
      "Time for epoch 756 is 47.1010 sec\n",
      "Epoch 757, gen_loss: -13.8686, disc_loss: -6.8117\n",
      "Time for epoch 757 is 47.1170 sec\n",
      "Epoch 758, gen_loss: -30.1588, disc_loss: -8.1602\n",
      "Time for epoch 758 is 47.1314 sec\n",
      "Epoch 759, gen_loss: 12.9471, disc_loss: -7.9974\n",
      "Time for epoch 759 is 47.1284 sec\n",
      "Epoch 760, gen_loss: -69.2916, disc_loss: -8.0793\n",
      "Time for epoch 760 is 47.1195 sec\n",
      "Epoch 761, gen_loss: 18.1584, disc_loss: -9.0230\n",
      "Time for epoch 761 is 47.1304 sec\n",
      "Epoch 762, gen_loss: 20.2820, disc_loss: -6.3302\n",
      "Time for epoch 762 is 47.1180 sec\n",
      "Epoch 763, gen_loss: -48.0341, disc_loss: -7.7353\n",
      "Time for epoch 763 is 47.1535 sec\n",
      "Epoch 764, gen_loss: 6.5171, disc_loss: -8.7740\n",
      "Time for epoch 764 is 47.1211 sec\n",
      "Epoch 765, gen_loss: -22.4722, disc_loss: -7.3679\n",
      "Time for epoch 765 is 47.1315 sec\n",
      "Epoch 766, gen_loss: -22.4502, disc_loss: -6.7560\n",
      "Time for epoch 766 is 47.1438 sec\n",
      "Epoch 767, gen_loss: 22.9728, disc_loss: -7.5664\n",
      "Time for epoch 767 is 47.1661 sec\n",
      "Epoch 768, gen_loss: 16.4287, disc_loss: -9.0426\n",
      "Time for epoch 768 is 47.1244 sec\n",
      "Epoch 769, gen_loss: 30.1279, disc_loss: -8.2321\n",
      "Time for epoch 769 is 47.1014 sec\n",
      "Epoch 770, gen_loss: -17.9325, disc_loss: -7.5940\n",
      "Time for epoch 770 is 47.1637 sec\n",
      "Epoch 771, gen_loss: -6.8710, disc_loss: -7.1439\n",
      "Time for epoch 771 is 47.1206 sec\n",
      "Epoch 772, gen_loss: -7.2882, disc_loss: -7.2657\n",
      "Time for epoch 772 is 47.1362 sec\n",
      "Epoch 773, gen_loss: -27.3787, disc_loss: -7.6284\n",
      "Time for epoch 773 is 47.0884 sec\n",
      "Epoch 774, gen_loss: 20.3740, disc_loss: -7.7587\n",
      "Time for epoch 774 is 47.0974 sec\n",
      "Epoch 775, gen_loss: -43.3148, disc_loss: -6.7975\n",
      "Time for epoch 775 is 47.0907 sec\n",
      "Epoch 776, gen_loss: -59.3954, disc_loss: -7.3540\n",
      "Time for epoch 776 is 47.1186 sec\n",
      "Epoch 777, gen_loss: -8.5660, disc_loss: -7.6490\n",
      "Time for epoch 777 is 47.0936 sec\n",
      "Epoch 778, gen_loss: -69.4642, disc_loss: -8.5492\n",
      "Time for epoch 778 is 47.1425 sec\n",
      "Epoch 779, gen_loss: -8.4246, disc_loss: -7.2818\n",
      "Time for epoch 779 is 47.1393 sec\n",
      "Epoch 780, gen_loss: 37.5389, disc_loss: -7.2062\n",
      "Time for epoch 780 is 47.1381 sec\n",
      "Epoch 781, gen_loss: -50.2069, disc_loss: -8.4792\n",
      "Time for epoch 781 is 47.1726 sec\n",
      "Epoch 782, gen_loss: -37.3488, disc_loss: -7.5236\n",
      "Time for epoch 782 is 47.1495 sec\n",
      "Epoch 783, gen_loss: -43.5809, disc_loss: -6.1851\n",
      "Time for epoch 783 is 47.1301 sec\n",
      "Epoch 784, gen_loss: -96.9906, disc_loss: -7.3784\n",
      "Time for epoch 784 is 47.1242 sec\n",
      "Epoch 785, gen_loss: -96.1709, disc_loss: -8.3780\n",
      "Time for epoch 785 is 47.1160 sec\n",
      "Epoch 786, gen_loss: -78.3692, disc_loss: -8.2636\n",
      "Time for epoch 786 is 47.1159 sec\n",
      "Epoch 787, gen_loss: -57.5379, disc_loss: -5.3343\n",
      "Time for epoch 787 is 47.1704 sec\n",
      "Epoch 788, gen_loss: -51.2082, disc_loss: -7.6423\n",
      "Time for epoch 788 is 47.1427 sec\n",
      "Epoch 789, gen_loss: -103.0533, disc_loss: -8.8689\n",
      "Time for epoch 789 is 47.1313 sec\n",
      "Epoch 790, gen_loss: -68.4897, disc_loss: -6.9612\n",
      "Time for epoch 790 is 47.1376 sec\n",
      "Epoch 791, gen_loss: -64.4194, disc_loss: -6.3073\n",
      "Time for epoch 791 is 47.1334 sec\n",
      "Epoch 792, gen_loss: -67.0040, disc_loss: -8.0882\n",
      "Time for epoch 792 is 47.1336 sec\n",
      "Epoch 793, gen_loss: 11.4402, disc_loss: -8.5237\n",
      "Time for epoch 793 is 47.1420 sec\n",
      "Epoch 794, gen_loss: -26.1456, disc_loss: -7.5027\n",
      "Time for epoch 794 is 47.1204 sec\n",
      "Epoch 795, gen_loss: 10.5268, disc_loss: -7.3621\n",
      "Time for epoch 795 is 47.1540 sec\n",
      "Epoch 796, gen_loss: 10.0367, disc_loss: -6.5226\n",
      "Time for epoch 796 is 47.1055 sec\n",
      "Epoch 797, gen_loss: -73.9291, disc_loss: -7.0008\n",
      "Time for epoch 797 is 47.1331 sec\n",
      "Epoch 798, gen_loss: -37.1603, disc_loss: -8.6366\n",
      "Time for epoch 798 is 47.1082 sec\n",
      "Epoch 799, gen_loss: -20.8502, disc_loss: -5.8245\n",
      "Time for epoch 799 is 47.1363 sec\n",
      "Epoch 800, gen_loss: -65.9065, disc_loss: -8.0906\n",
      "Time for epoch 800 is 47.1113 sec\n",
      "Epoch 801, gen_loss: 1.0883, disc_loss: -7.5262\n",
      "Time for epoch 801 is 47.1282 sec\n",
      "Epoch 802, gen_loss: -28.6245, disc_loss: -7.5116\n",
      "Time for epoch 802 is 47.1620 sec\n",
      "Epoch 803, gen_loss: -50.0667, disc_loss: -7.2514\n",
      "Time for epoch 803 is 47.1276 sec\n",
      "Epoch 804, gen_loss: -47.6564, disc_loss: -7.6618\n",
      "Time for epoch 804 is 47.1352 sec\n",
      "Epoch 805, gen_loss: -41.6318, disc_loss: -6.7152\n",
      "Time for epoch 805 is 47.1270 sec\n",
      "Epoch 806, gen_loss: -70.0267, disc_loss: -7.5178\n",
      "Time for epoch 806 is 47.1486 sec\n",
      "Epoch 807, gen_loss: -44.0969, disc_loss: -8.2009\n",
      "Time for epoch 807 is 47.1505 sec\n",
      "Epoch 808, gen_loss: -43.0425, disc_loss: -7.1478\n",
      "Time for epoch 808 is 47.1250 sec\n",
      "Epoch 809, gen_loss: -62.9181, disc_loss: -7.0627\n",
      "Time for epoch 809 is 47.1713 sec\n",
      "Epoch 810, gen_loss: -40.1780, disc_loss: -8.2668\n",
      "Time for epoch 810 is 47.1015 sec\n",
      "Epoch 811, gen_loss: -17.0418, disc_loss: -6.9817\n",
      "Time for epoch 811 is 47.1181 sec\n",
      "Epoch 812, gen_loss: -15.8742, disc_loss: -8.1399\n",
      "Time for epoch 812 is 47.1338 sec\n",
      "Epoch 813, gen_loss: -23.1677, disc_loss: -7.6184\n",
      "Time for epoch 813 is 47.1184 sec\n",
      "Epoch 814, gen_loss: -3.7108, disc_loss: -7.9250\n",
      "Time for epoch 814 is 47.1258 sec\n",
      "Epoch 815, gen_loss: -61.1030, disc_loss: -7.4100\n",
      "Time for epoch 815 is 47.1357 sec\n",
      "Epoch 816, gen_loss: -23.9655, disc_loss: -8.2284\n",
      "Time for epoch 816 is 47.1566 sec\n",
      "Epoch 817, gen_loss: -40.4505, disc_loss: -7.8193\n",
      "Time for epoch 817 is 47.1826 sec\n",
      "Epoch 818, gen_loss: -36.2884, disc_loss: -8.7766\n",
      "Time for epoch 818 is 47.1432 sec\n",
      "Epoch 819, gen_loss: -1.2781, disc_loss: -6.6596\n",
      "Time for epoch 819 is 47.1330 sec\n",
      "Epoch 820, gen_loss: -17.8127, disc_loss: -7.2241\n",
      "Time for epoch 820 is 47.1456 sec\n",
      "Epoch 821, gen_loss: -48.7489, disc_loss: -7.2856\n",
      "Time for epoch 821 is 47.1455 sec\n",
      "Epoch 822, gen_loss: -47.6639, disc_loss: -7.0877\n",
      "Time for epoch 822 is 47.1451 sec\n",
      "Epoch 823, gen_loss: -65.4455, disc_loss: -7.3620\n",
      "Time for epoch 823 is 47.1304 sec\n",
      "Epoch 824, gen_loss: -70.3609, disc_loss: -7.8755\n",
      "Time for epoch 824 is 47.1610 sec\n",
      "Epoch 825, gen_loss: -66.0916, disc_loss: -7.2826\n",
      "Time for epoch 825 is 47.1094 sec\n",
      "Epoch 826, gen_loss: -56.5407, disc_loss: -6.9036\n",
      "Time for epoch 826 is 47.1394 sec\n",
      "Epoch 827, gen_loss: 2.5722, disc_loss: -7.4476\n",
      "Time for epoch 827 is 47.1189 sec\n",
      "Epoch 828, gen_loss: -66.0213, disc_loss: -7.4896\n",
      "Time for epoch 828 is 47.1295 sec\n",
      "Epoch 829, gen_loss: -20.3081, disc_loss: -7.6111\n",
      "Time for epoch 829 is 47.0849 sec\n",
      "Epoch 830, gen_loss: -50.7537, disc_loss: -6.3973\n",
      "Time for epoch 830 is 47.1142 sec\n",
      "Epoch 831, gen_loss: -101.7813, disc_loss: -6.4825\n",
      "Time for epoch 831 is 47.1261 sec\n",
      "Epoch 832, gen_loss: -25.2800, disc_loss: -8.0122\n",
      "Time for epoch 832 is 47.1375 sec\n",
      "Epoch 833, gen_loss: -20.3832, disc_loss: -8.2706\n",
      "Time for epoch 833 is 47.1040 sec\n",
      "Epoch 834, gen_loss: -57.6619, disc_loss: -7.1660\n",
      "Time for epoch 834 is 47.1111 sec\n",
      "Epoch 835, gen_loss: -1.4091, disc_loss: -6.0612\n",
      "Time for epoch 835 is 47.1034 sec\n",
      "Epoch 836, gen_loss: -65.5133, disc_loss: -7.8758\n",
      "Time for epoch 836 is 47.1165 sec\n",
      "Epoch 837, gen_loss: -76.9859, disc_loss: -7.2579\n",
      "Time for epoch 837 is 47.1267 sec\n",
      "Epoch 838, gen_loss: 17.1527, disc_loss: -8.0338\n",
      "Time for epoch 838 is 47.1116 sec\n",
      "Epoch 839, gen_loss: -30.5695, disc_loss: -7.7048\n",
      "Time for epoch 839 is 47.1453 sec\n",
      "Epoch 840, gen_loss: -47.1134, disc_loss: -6.3070\n",
      "Time for epoch 840 is 47.1206 sec\n",
      "Epoch 841, gen_loss: -32.6042, disc_loss: -7.0360\n",
      "Time for epoch 841 is 47.1756 sec\n",
      "Epoch 842, gen_loss: -6.2886, disc_loss: -7.1214\n",
      "Time for epoch 842 is 47.1400 sec\n",
      "Epoch 843, gen_loss: 1.1324, disc_loss: -8.7526\n",
      "Time for epoch 843 is 47.0971 sec\n",
      "Epoch 844, gen_loss: 17.2523, disc_loss: -7.4078\n",
      "Time for epoch 844 is 47.1498 sec\n",
      "Epoch 845, gen_loss: 36.8077, disc_loss: -7.2615\n",
      "Time for epoch 845 is 47.1237 sec\n",
      "Epoch 846, gen_loss: 11.1070, disc_loss: -6.7723\n",
      "Time for epoch 846 is 47.0964 sec\n",
      "Epoch 847, gen_loss: 3.1678, disc_loss: -7.1552\n",
      "Time for epoch 847 is 47.1577 sec\n",
      "Epoch 848, gen_loss: 6.7119, disc_loss: -7.6957\n",
      "Time for epoch 848 is 47.1313 sec\n",
      "Epoch 849, gen_loss: -55.4873, disc_loss: -8.1262\n",
      "Time for epoch 849 is 47.1299 sec\n",
      "Epoch 850, gen_loss: -49.0065, disc_loss: -6.4295\n",
      "Time for epoch 850 is 47.1201 sec\n",
      "Epoch 851, gen_loss: 16.9412, disc_loss: -8.1931\n",
      "Time for epoch 851 is 47.1258 sec\n",
      "Epoch 852, gen_loss: -16.1416, disc_loss: -7.5425\n",
      "Time for epoch 852 is 47.1355 sec\n",
      "Epoch 853, gen_loss: -7.6042, disc_loss: -5.7160\n",
      "Time for epoch 853 is 47.1386 sec\n",
      "Epoch 854, gen_loss: 14.9184, disc_loss: -7.9370\n",
      "Time for epoch 854 is 47.1125 sec\n",
      "Epoch 855, gen_loss: -29.4704, disc_loss: -7.7915\n",
      "Time for epoch 855 is 47.1275 sec\n",
      "Epoch 856, gen_loss: -20.4395, disc_loss: -6.7939\n",
      "Time for epoch 856 is 47.1554 sec\n",
      "Epoch 857, gen_loss: 23.9553, disc_loss: -7.4298\n",
      "Time for epoch 857 is 47.1288 sec\n",
      "Epoch 858, gen_loss: 33.9734, disc_loss: -7.3495\n",
      "Time for epoch 858 is 47.1392 sec\n",
      "Epoch 859, gen_loss: 13.2799, disc_loss: -8.3648\n",
      "Time for epoch 859 is 47.1489 sec\n",
      "Epoch 860, gen_loss: -21.2740, disc_loss: -7.2624\n",
      "Time for epoch 860 is 47.1105 sec\n",
      "Epoch 861, gen_loss: -12.2700, disc_loss: -6.1331\n",
      "Time for epoch 861 is 47.1687 sec\n",
      "Epoch 862, gen_loss: -24.5154, disc_loss: -7.4982\n",
      "Time for epoch 862 is 47.1632 sec\n",
      "Epoch 863, gen_loss: -45.9604, disc_loss: -7.3287\n",
      "Time for epoch 863 is 47.1669 sec\n",
      "Epoch 864, gen_loss: 20.4925, disc_loss: -7.5485\n",
      "Time for epoch 864 is 47.1064 sec\n",
      "Epoch 865, gen_loss: 34.8203, disc_loss: -7.5000\n",
      "Time for epoch 865 is 47.0875 sec\n",
      "Epoch 866, gen_loss: -37.1220, disc_loss: -7.8830\n",
      "Time for epoch 866 is 47.1170 sec\n",
      "Epoch 867, gen_loss: 8.8256, disc_loss: -7.0147\n",
      "Time for epoch 867 is 47.0962 sec\n",
      "Epoch 868, gen_loss: -11.1705, disc_loss: -6.7777\n",
      "Time for epoch 868 is 47.1270 sec\n",
      "Epoch 869, gen_loss: -59.0547, disc_loss: -7.4653\n",
      "Time for epoch 869 is 47.1242 sec\n",
      "Epoch 870, gen_loss: 23.0288, disc_loss: -8.1108\n",
      "Time for epoch 870 is 47.1411 sec\n",
      "Epoch 871, gen_loss: 39.5395, disc_loss: -6.3058\n",
      "Time for epoch 871 is 47.1087 sec\n",
      "Epoch 872, gen_loss: -19.9107, disc_loss: -6.4505\n",
      "Time for epoch 872 is 47.1107 sec\n",
      "Epoch 873, gen_loss: 10.7455, disc_loss: -6.9401\n",
      "Time for epoch 873 is 47.0981 sec\n",
      "Epoch 874, gen_loss: 9.4748, disc_loss: -7.8831\n",
      "Time for epoch 874 is 47.0957 sec\n",
      "Epoch 875, gen_loss: 40.1498, disc_loss: -6.6970\n",
      "Time for epoch 875 is 47.1004 sec\n",
      "Epoch 876, gen_loss: 12.0465, disc_loss: -5.7591\n",
      "Time for epoch 876 is 47.1106 sec\n",
      "Epoch 877, gen_loss: -23.4243, disc_loss: -7.6468\n",
      "Time for epoch 877 is 47.0903 sec\n",
      "Epoch 878, gen_loss: 59.0902, disc_loss: -7.1757\n",
      "Time for epoch 878 is 47.1857 sec\n",
      "Epoch 879, gen_loss: -24.3045, disc_loss: -7.3758\n",
      "Time for epoch 879 is 47.0982 sec\n",
      "Epoch 880, gen_loss: 1.7270, disc_loss: -8.0471\n",
      "Time for epoch 880 is 47.1089 sec\n",
      "Epoch 881, gen_loss: 47.8516, disc_loss: -6.7858\n",
      "Time for epoch 881 is 47.1303 sec\n",
      "Epoch 882, gen_loss: 30.1750, disc_loss: -7.7115\n",
      "Time for epoch 882 is 47.1284 sec\n",
      "Epoch 883, gen_loss: 33.3774, disc_loss: -7.7961\n",
      "Time for epoch 883 is 47.1333 sec\n",
      "Epoch 884, gen_loss: 17.4087, disc_loss: -8.0212\n",
      "Time for epoch 884 is 47.1385 sec\n",
      "Epoch 885, gen_loss: 19.3824, disc_loss: -7.2667\n",
      "Time for epoch 885 is 47.0852 sec\n",
      "Epoch 886, gen_loss: 19.9367, disc_loss: -6.8386\n",
      "Time for epoch 886 is 47.0912 sec\n",
      "Epoch 887, gen_loss: 11.9831, disc_loss: -6.5950\n",
      "Time for epoch 887 is 47.1028 sec\n",
      "Epoch 888, gen_loss: 33.4705, disc_loss: -7.0751\n",
      "Time for epoch 888 is 47.1307 sec\n",
      "Epoch 889, gen_loss: -4.5273, disc_loss: -6.3542\n",
      "Time for epoch 889 is 47.1493 sec\n",
      "Epoch 890, gen_loss: -7.2836, disc_loss: -7.2441\n",
      "Time for epoch 890 is 47.1276 sec\n",
      "Epoch 891, gen_loss: 45.5005, disc_loss: -6.1140\n",
      "Time for epoch 891 is 47.1306 sec\n",
      "Epoch 892, gen_loss: -31.9027, disc_loss: -7.5525\n",
      "Time for epoch 892 is 47.0701 sec\n",
      "Epoch 893, gen_loss: -49.4370, disc_loss: -7.1124\n",
      "Time for epoch 893 is 47.1286 sec\n",
      "Epoch 894, gen_loss: -8.5854, disc_loss: -6.9186\n",
      "Time for epoch 894 is 47.1286 sec\n",
      "Epoch 895, gen_loss: -17.5772, disc_loss: -5.6308\n",
      "Time for epoch 895 is 47.1509 sec\n",
      "Epoch 896, gen_loss: 1.5768, disc_loss: -7.6745\n",
      "Time for epoch 896 is 47.0927 sec\n",
      "Epoch 897, gen_loss: 32.3954, disc_loss: -6.8789\n",
      "Time for epoch 897 is 47.1227 sec\n",
      "Epoch 898, gen_loss: 29.7226, disc_loss: -7.5820\n",
      "Time for epoch 898 is 47.1766 sec\n",
      "Epoch 899, gen_loss: 24.1638, disc_loss: -7.5673\n",
      "Time for epoch 899 is 47.1130 sec\n",
      "Epoch 900, gen_loss: -0.5173, disc_loss: -6.1726\n",
      "Time for epoch 900 is 47.1146 sec\n",
      "Epoch 901, gen_loss: -31.1706, disc_loss: -6.5606\n",
      "Time for epoch 901 is 47.1361 sec\n",
      "Epoch 902, gen_loss: 8.8552, disc_loss: -7.3858\n",
      "Time for epoch 902 is 47.1028 sec\n",
      "Epoch 903, gen_loss: -2.6247, disc_loss: -6.7283\n",
      "Time for epoch 903 is 47.1411 sec\n",
      "Epoch 904, gen_loss: -24.5593, disc_loss: -6.5661\n",
      "Time for epoch 904 is 47.0993 sec\n",
      "Epoch 905, gen_loss: -19.6920, disc_loss: -6.5460\n",
      "Time for epoch 905 is 47.1512 sec\n",
      "Epoch 906, gen_loss: 54.2246, disc_loss: -6.3678\n",
      "Time for epoch 906 is 47.1488 sec\n",
      "Epoch 907, gen_loss: -4.4330, disc_loss: -7.0694\n",
      "Time for epoch 907 is 47.1292 sec\n",
      "Epoch 908, gen_loss: 0.0473, disc_loss: -6.3288\n",
      "Time for epoch 908 is 47.1338 sec\n",
      "Epoch 909, gen_loss: 105.3329, disc_loss: -6.8240\n",
      "Time for epoch 909 is 47.0714 sec\n",
      "Epoch 910, gen_loss: 30.1633, disc_loss: -6.7576\n",
      "Time for epoch 910 is 47.0810 sec\n",
      "Epoch 911, gen_loss: -51.5334, disc_loss: -5.9553\n",
      "Time for epoch 911 is 47.1095 sec\n",
      "Epoch 912, gen_loss: 110.2783, disc_loss: -8.7939\n",
      "Time for epoch 912 is 47.1239 sec\n",
      "Epoch 913, gen_loss: 103.7080, disc_loss: -5.0469\n",
      "Time for epoch 913 is 47.1126 sec\n",
      "Epoch 914, gen_loss: 16.6610, disc_loss: -8.7102\n",
      "Time for epoch 914 is 47.1192 sec\n",
      "Epoch 915, gen_loss: -23.5447, disc_loss: -5.7050\n",
      "Time for epoch 915 is 47.1289 sec\n",
      "Epoch 916, gen_loss: 36.5549, disc_loss: -8.3276\n",
      "Time for epoch 916 is 47.1934 sec\n",
      "Epoch 917, gen_loss: 43.4493, disc_loss: -4.3847\n",
      "Time for epoch 917 is 47.1352 sec\n",
      "Epoch 918, gen_loss: -9.9204, disc_loss: -8.2763\n",
      "Time for epoch 918 is 47.0958 sec\n",
      "Epoch 919, gen_loss: 52.3206, disc_loss: -5.5142\n",
      "Time for epoch 919 is 47.0778 sec\n",
      "Epoch 920, gen_loss: 79.7377, disc_loss: -6.1165\n",
      "Time for epoch 920 is 47.0960 sec\n",
      "Epoch 921, gen_loss: -7.6469, disc_loss: -7.0270\n",
      "Time for epoch 921 is 47.1264 sec\n",
      "Epoch 922, gen_loss: -57.7635, disc_loss: -6.9765\n",
      "Time for epoch 922 is 47.1224 sec\n",
      "Epoch 923, gen_loss: 36.1161, disc_loss: -8.1090\n",
      "Time for epoch 923 is 47.1001 sec\n",
      "Epoch 924, gen_loss: 26.6495, disc_loss: -5.6140\n",
      "Time for epoch 924 is 47.1170 sec\n",
      "Epoch 925, gen_loss: -10.9809, disc_loss: -5.5355\n",
      "Time for epoch 925 is 47.1613 sec\n",
      "Epoch 926, gen_loss: 43.2917, disc_loss: -6.3817\n",
      "Time for epoch 926 is 47.1232 sec\n",
      "Epoch 927, gen_loss: 57.5810, disc_loss: -7.2827\n",
      "Time for epoch 927 is 47.1356 sec\n",
      "Epoch 928, gen_loss: 23.8945, disc_loss: -6.6253\n",
      "Time for epoch 928 is 47.1357 sec\n",
      "Epoch 929, gen_loss: 59.3450, disc_loss: -5.9095\n",
      "Time for epoch 929 is 47.1461 sec\n",
      "Epoch 930, gen_loss: 49.2704, disc_loss: -6.5413\n",
      "Time for epoch 930 is 47.0763 sec\n",
      "Epoch 931, gen_loss: 34.8701, disc_loss: -6.9170\n",
      "Time for epoch 931 is 47.1195 sec\n",
      "Epoch 932, gen_loss: 32.4631, disc_loss: -6.1082\n",
      "Time for epoch 932 is 47.1891 sec\n",
      "Epoch 933, gen_loss: 43.3980, disc_loss: -7.0208\n",
      "Time for epoch 933 is 47.1034 sec\n",
      "Epoch 934, gen_loss: -9.9033, disc_loss: -7.1989\n",
      "Time for epoch 934 is 47.0890 sec\n",
      "Epoch 935, gen_loss: -7.8648, disc_loss: -6.2067\n",
      "Time for epoch 935 is 47.1490 sec\n",
      "Epoch 936, gen_loss: -26.1204, disc_loss: -7.7988\n",
      "Time for epoch 936 is 47.1302 sec\n",
      "Epoch 937, gen_loss: -35.6348, disc_loss: -6.3178\n",
      "Time for epoch 937 is 47.1222 sec\n",
      "Epoch 938, gen_loss: -37.9275, disc_loss: -6.6497\n",
      "Time for epoch 938 is 47.1178 sec\n",
      "Epoch 939, gen_loss: -31.0104, disc_loss: -7.8481\n",
      "Time for epoch 939 is 47.1139 sec\n",
      "Epoch 940, gen_loss: -24.3566, disc_loss: -5.5959\n",
      "Time for epoch 940 is 47.1291 sec\n",
      "Epoch 941, gen_loss: -3.7219, disc_loss: -8.1224\n",
      "Time for epoch 941 is 47.1329 sec\n",
      "Epoch 942, gen_loss: -22.2140, disc_loss: -7.3649\n",
      "Time for epoch 942 is 47.1379 sec\n",
      "Epoch 943, gen_loss: -31.6515, disc_loss: -6.8132\n",
      "Time for epoch 943 is 47.1392 sec\n",
      "Epoch 944, gen_loss: -22.8734, disc_loss: -6.8228\n",
      "Time for epoch 944 is 47.0901 sec\n",
      "Epoch 945, gen_loss: 5.7366, disc_loss: -6.8515\n",
      "Time for epoch 945 is 47.1119 sec\n",
      "Epoch 946, gen_loss: 40.2004, disc_loss: -6.2785\n",
      "Time for epoch 946 is 47.1153 sec\n",
      "Epoch 947, gen_loss: -28.9843, disc_loss: -6.7915\n",
      "Time for epoch 947 is 47.1414 sec\n",
      "Epoch 948, gen_loss: 10.5448, disc_loss: -6.6978\n",
      "Time for epoch 948 is 47.1351 sec\n",
      "Epoch 949, gen_loss: -24.1465, disc_loss: -5.8633\n",
      "Time for epoch 949 is 47.1603 sec\n",
      "Epoch 950, gen_loss: -6.4599, disc_loss: -6.3229\n",
      "Time for epoch 950 is 47.1220 sec\n",
      "Epoch 951, gen_loss: -44.0156, disc_loss: -7.1328\n",
      "Time for epoch 951 is 47.1274 sec\n",
      "Epoch 952, gen_loss: 6.2041, disc_loss: -6.9790\n",
      "Time for epoch 952 is 47.1463 sec\n",
      "Epoch 953, gen_loss: -49.1394, disc_loss: -7.5669\n",
      "Time for epoch 953 is 47.1186 sec\n",
      "Epoch 954, gen_loss: -55.5672, disc_loss: -6.4998\n",
      "Time for epoch 954 is 47.0758 sec\n",
      "Epoch 955, gen_loss: -13.4627, disc_loss: -6.8599\n",
      "Time for epoch 955 is 47.1046 sec\n",
      "Epoch 956, gen_loss: -54.4238, disc_loss: -6.4067\n",
      "Time for epoch 956 is 47.0890 sec\n",
      "Epoch 957, gen_loss: -5.1175, disc_loss: -7.0151\n",
      "Time for epoch 957 is 47.1112 sec\n",
      "Epoch 958, gen_loss: -6.9651, disc_loss: -5.6235\n",
      "Time for epoch 958 is 47.1224 sec\n",
      "Epoch 959, gen_loss: -43.9509, disc_loss: -6.8834\n",
      "Time for epoch 959 is 47.1476 sec\n",
      "Epoch 960, gen_loss: 20.5791, disc_loss: -7.6439\n",
      "Time for epoch 960 is 47.1101 sec\n",
      "Epoch 961, gen_loss: 22.5246, disc_loss: -8.0533\n",
      "Time for epoch 961 is 47.1613 sec\n",
      "Epoch 962, gen_loss: -34.3344, disc_loss: -5.0941\n",
      "Time for epoch 962 is 47.1025 sec\n",
      "Epoch 963, gen_loss: -17.9002, disc_loss: -7.5064\n",
      "Time for epoch 963 is 47.1187 sec\n",
      "Epoch 964, gen_loss: -42.8079, disc_loss: -5.7810\n",
      "Time for epoch 964 is 47.1091 sec\n",
      "Epoch 965, gen_loss: -38.8729, disc_loss: -7.2729\n",
      "Time for epoch 965 is 47.1323 sec\n",
      "Epoch 966, gen_loss: 75.3632, disc_loss: -6.6945\n",
      "Time for epoch 966 is 47.1236 sec\n",
      "Epoch 967, gen_loss: 10.9699, disc_loss: -6.4399\n",
      "Time for epoch 967 is 47.1195 sec\n",
      "Epoch 968, gen_loss: -2.8474, disc_loss: -7.2084\n",
      "Time for epoch 968 is 47.1419 sec\n",
      "Epoch 969, gen_loss: 39.1909, disc_loss: -6.3389\n",
      "Time for epoch 969 is 47.1347 sec\n",
      "Epoch 970, gen_loss: 3.9388, disc_loss: -5.6742\n",
      "Time for epoch 970 is 47.1236 sec\n",
      "Epoch 971, gen_loss: -14.1747, disc_loss: -6.6322\n",
      "Time for epoch 971 is 47.1227 sec\n",
      "Epoch 972, gen_loss: -4.1068, disc_loss: -6.2296\n",
      "Time for epoch 972 is 47.0922 sec\n",
      "Epoch 973, gen_loss: 20.6568, disc_loss: -6.8197\n",
      "Time for epoch 973 is 47.1402 sec\n",
      "Epoch 974, gen_loss: -16.5251, disc_loss: -6.8444\n",
      "Time for epoch 974 is 47.0947 sec\n",
      "Epoch 975, gen_loss: -45.0070, disc_loss: -7.3008\n",
      "Time for epoch 975 is 47.1074 sec\n",
      "Epoch 976, gen_loss: -17.2024, disc_loss: -6.0263\n",
      "Time for epoch 976 is 47.1475 sec\n",
      "Epoch 977, gen_loss: -21.0938, disc_loss: -7.4201\n",
      "Time for epoch 977 is 47.1136 sec\n",
      "Epoch 978, gen_loss: -5.2825, disc_loss: -6.1661\n",
      "Time for epoch 978 is 47.1411 sec\n",
      "Epoch 979, gen_loss: -3.4545, disc_loss: -5.5873\n",
      "Time for epoch 979 is 47.0879 sec\n",
      "Epoch 980, gen_loss: 38.1844, disc_loss: -7.1676\n",
      "Time for epoch 980 is 47.1071 sec\n",
      "Epoch 981, gen_loss: 2.0741, disc_loss: -5.7839\n",
      "Time for epoch 981 is 47.1295 sec\n",
      "Epoch 982, gen_loss: 25.7866, disc_loss: -6.6134\n",
      "Time for epoch 982 is 47.1331 sec\n",
      "Epoch 983, gen_loss: 63.1764, disc_loss: -6.5605\n",
      "Time for epoch 983 is 47.1163 sec\n",
      "Epoch 984, gen_loss: 44.9335, disc_loss: -6.4280\n",
      "Time for epoch 984 is 47.1731 sec\n",
      "Epoch 985, gen_loss: -56.8213, disc_loss: -7.2008\n",
      "Time for epoch 985 is 47.1294 sec\n",
      "Epoch 986, gen_loss: 0.1653, disc_loss: -6.2992\n",
      "Time for epoch 986 is 47.1255 sec\n",
      "Epoch 987, gen_loss: 41.1046, disc_loss: -6.2509\n",
      "Time for epoch 987 is 47.1510 sec\n",
      "Epoch 988, gen_loss: -56.8112, disc_loss: -7.5279\n",
      "Time for epoch 988 is 47.1673 sec\n",
      "Epoch 989, gen_loss: -17.4966, disc_loss: -5.7124\n",
      "Time for epoch 989 is 47.1701 sec\n",
      "Epoch 990, gen_loss: 75.1730, disc_loss: -7.7776\n",
      "Time for epoch 990 is 47.1642 sec\n",
      "Epoch 991, gen_loss: -22.9344, disc_loss: -7.1062\n",
      "Time for epoch 991 is 47.0547 sec\n",
      "Epoch 992, gen_loss: -30.2834, disc_loss: -5.8703\n",
      "Time for epoch 992 is 47.1509 sec\n",
      "Epoch 993, gen_loss: 74.2052, disc_loss: -7.3791\n",
      "Time for epoch 993 is 47.1026 sec\n",
      "Epoch 994, gen_loss: -1.3513, disc_loss: -6.2972\n",
      "Time for epoch 994 is 47.1353 sec\n",
      "Epoch 995, gen_loss: -2.7983, disc_loss: -6.1923\n",
      "Time for epoch 995 is 47.1166 sec\n",
      "Epoch 996, gen_loss: 88.2328, disc_loss: -6.3825\n",
      "Time for epoch 996 is 47.1576 sec\n",
      "Epoch 997, gen_loss: 0.2542, disc_loss: -6.3067\n",
      "Time for epoch 997 is 47.1607 sec\n",
      "Epoch 998, gen_loss: 15.3599, disc_loss: -6.7650\n",
      "Time for epoch 998 is 47.1295 sec\n",
      "Epoch 999, gen_loss: 61.0045, disc_loss: -6.5758\n",
      "Time for epoch 999 is 47.1246 sec\n",
      "Epoch 1000, gen_loss: 53.4342, disc_loss: -7.0723\n",
      "Time for epoch 1000 is 47.1058 sec\n"
     ]
    }
   ],
   "source": [
    "train(dataset, hparas['N_EPOCH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "460\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Dataset\n",
    "If you change anything during preprocessing of training dataset, you must make sure same operations have be done in testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_data_generator(caption, index):\n",
    "    caption = tf.cast(caption, tf.float32)\n",
    "    return caption, index\n",
    "\n",
    "def testing_dataset_generator(batch_size, data_generator):\n",
    "    data = pd.read_pickle('./dataset/testData.pkl')\n",
    "    captions = data['Captions'].values\n",
    "    caption = []\n",
    "    for i in range(len(captions)):\n",
    "        caption.append(captions[i])\n",
    "    caption = np.asarray(caption)\n",
    "    caption = caption.astype(np.int64)\n",
    "    index = data['ID'].values\n",
    "    index = np.asarray(index)\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((caption, index))\n",
    "    dataset = dataset.map(data_generator, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.repeat().batch(batch_size)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataset = testing_dataset_generator(hparas['BATCH_SIZE'], testing_data_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('./dataset/testData.pkl')\n",
    "captions = data['Captions'].values\n",
    "\n",
    "NUM_TEST = len(captions)\n",
    "EPOCH_TEST = int(NUM_TEST / hparas['BATCH_SIZE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(dataset):\n",
    "    hidden = text_encoder.initialize_hidden_state()\n",
    "    sample_size = hparas['BATCH_SIZE']\n",
    "    sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, hparas['Z_DIM'])).astype(np.float32)\n",
    "    \n",
    "    step = 0\n",
    "    start = time.time()\n",
    "    for captions, idx in dataset:\n",
    "        if step > EPOCH_TEST:\n",
    "            break\n",
    "        \n",
    "        fake_image = test_step(captions, sample_seed, hidden)\n",
    "        step += 1\n",
    "        for i in range(hparas['BATCH_SIZE']):\n",
    "            plt.imsave('./inference/demo/inference_{:04d}.jpg'.format(idx[i]), fake_image[i].numpy()*0.5 + 0.5)\n",
    "            \n",
    "    print('Time for inference is {:.4f} sec'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f306d4e3ca0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(checkpoint_dir + '/ckpt-20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for inference is 1.0797 sec\n"
     ]
    }
   ],
   "source": [
    "inference(testing_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output score.csv\n",
    "CAUTION: \n",
    "* Please modify GPU setting in <i>inception_score.py</i> if need.\n",
    "* Please run the below cmd in command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd testing\n",
    "!python inception_score.py ../inference/demo ../score_demo.csv 39 # BATCH_SIZE=39 is available using GTX 1080 Ti (need 9441MB memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo - TA80\n",
    "CAUTION: The code here doesn't work because the inference images aren't provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(idx):\n",
    "    fig = plt.figure(figsize=(14, 14))\n",
    "    \n",
    "    for count, i in enumerate(idx):\n",
    "        loc = np.where(i==index)[0][0]\n",
    "        text = ''\n",
    "        for word in captions[loc]:\n",
    "            if id2word_dict[word] != '<PAD>':\n",
    "                text += id2word_dict[word]\n",
    "                text += ' '\n",
    "        print(text)\n",
    "        \n",
    "        path = './inference/TA80/inference_{:04d}.jpg'.format(i)\n",
    "        fake_iamge = plt.imread(path)\n",
    "        \n",
    "        plt.subplot(7, 7, count+1)\n",
    "        plt.imshow(fake_iamge)\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('./dataset/testData.pkl')\n",
    "captions = data['Captions'].values\n",
    "index = data['ID'].values\n",
    "random_idx = [23, 216, 224, 413, 713, 859, 876, 974, 1177, 1179, 1241, 2169, 2196, 2237, \n",
    "              2356, 2611, 2621, 2786, 2951, 2962, 3145, 3255, 3327, 3639, 3654, 3927, 4262, \n",
    "              4321, 4517, 5067, 5147, 5955, 6167, 6216, 6410, 6413, 6579, 6584, 6804, 6988, \n",
    "              7049, 7160]\n",
    "\n",
    "visualize(random_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './inference/TA80/'\n",
    "img_path = Path(DATA_PATH).glob('*.jpg')\n",
    "img_path = [str(path.resolve()) for path in img_path]\n",
    "img_path = np.asarray(img_path)\n",
    "\n",
    "idx = np.random.randint(len(captions), size=42)\n",
    "idx.sort()\n",
    "\n",
    "random_idx = []\n",
    "for each in idx:\n",
    "    path = img_path[each].split(\"/\")\n",
    "    path = path[-1].replace('inference_', '')\n",
    "    random_idx.append(int(path.replace('.jpg', '')))\n",
    "    \n",
    "visualize(random_idx)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5c5b210ffa015f2312f69f2248e3163602cc860559c1494ea467ed1fecf0f25e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
