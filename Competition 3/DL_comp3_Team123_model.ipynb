{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLab Cup 3: Reverse Image Caption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"username\":\"chengchecheng\",\"key\":\"f6946452bb7b6297237d925ac6034cb7\"}Downloading datalab-cup3-reverse-image-caption-2021.zip to /home/kevin/NTHU/2021 Fall/Deep-Learning-Course-Team-Project/Competition 3\n",
      "100%|███████████████████████████████████████▉| 666M/667M [01:02<00:00, 11.6MB/s]\n",
      "100%|████████████████████████████████████████| 667M/667M [01:02<00:00, 11.2MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Follow the tutorial in this page:\n",
    "# https://www.endtoend.ai/tutorial/how-to-download-kaggle-datasets-on-ubuntu/\n",
    "!cat ~/.kaggle/kaggle.json\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "!kaggle competitions download -c datalab-cup3-reverse-image-caption-2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "# zipfile example\n",
    "def zip_list(file_path):\n",
    "    zf = zipfile.ZipFile(file_path, 'r')\n",
    "    zf.extractall('./')\n",
    "\n",
    "file_path = './datalab-cup3-reverse-image-caption-2021.zip'\n",
    "zip_list(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and Packages Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # disable warnings, info and errors \n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jan  2 00:30:00 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 465.19.01    Driver Version: 465.19.01    CUDA Version: 11.3     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\n",
      "| 72%   81C    P2   219W / 250W |   8234MiB / 11176MiB |     47%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:02:00.0 Off |                  N/A |\n",
      "| 45%   56C    P2    63W / 250W |    267MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1140      G   /usr/lib/xorg/Xorg                 16MiB |\n",
      "|    0   N/A  N/A      6275      C   ...gary/anaconda3/bin/python     8213MiB |\n",
      "|    1   N/A  N/A      6275      C   ...gary/anaconda3/bin/python      263MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the first GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[1], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        #for gpu in gpus:\n",
    "        #    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        tf.config.experimental.set_memory_growth(gpus[1], True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocseeing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 5427 vocabularies in total\n",
      "Word to id mapping, for example: flower -> 1\n",
      "Id to word mapping, for example: 1 -> flower\n",
      "Tokens: <PAD>: 5427; <RARE>: 5428\n"
     ]
    }
   ],
   "source": [
    "dictionary_path = './dictionary'\n",
    "vocab = np.load(dictionary_path + '/vocab.npy')\n",
    "print('there are {} vocabularies in total'.format(len(vocab)))\n",
    "\n",
    "word2Id_dict = dict(np.load(dictionary_path + '/word2Id.npy'))\n",
    "id2word_dict = dict(np.load(dictionary_path + '/id2Word.npy'))\n",
    "print('Word to id mapping, for example: %s -> %s' % ('flower', word2Id_dict['flower']))\n",
    "print('Id to word mapping, for example: %s -> %s' % ('1', id2word_dict['1']))\n",
    "print('Tokens: <PAD>: %s; <RARE>: %s' % (word2Id_dict['<PAD>'], word2Id_dict['<RARE>']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the flower shown has yellow anther red pistil and bright red petals.\n",
      "['9', '1', '82', '5', '11', '70', '20', '31', '3', '29', '20', '2', '5427', '5427', '5427', '5427', '5427', '5427', '5427', '5427']\n"
     ]
    }
   ],
   "source": [
    "def sent2IdList(line, MAX_SEQ_LENGTH=20):\n",
    "    MAX_SEQ_LIMIT = MAX_SEQ_LENGTH\n",
    "    padding = 0\n",
    "    \n",
    "    # data preprocessing, remove all puntuation in the texts\n",
    "    prep_line = re.sub('[%s]' % re.escape(string.punctuation), ' ', line.rstrip())\n",
    "    prep_line = prep_line.replace('-', ' ')\n",
    "    prep_line = prep_line.replace('-', ' ')\n",
    "    prep_line = prep_line.replace('  ', ' ')\n",
    "    prep_line = prep_line.replace('.', '')\n",
    "    tokens = prep_line.split(' ')\n",
    "    tokens = [\n",
    "        tokens[i] for i in range(len(tokens))\n",
    "        if tokens[i] != ' ' and tokens[i] != ''\n",
    "    ]\n",
    "    l = len(tokens)\n",
    "    padding = MAX_SEQ_LIMIT - l\n",
    "    \n",
    "    # make sure length of each text is equal to MAX_SEQ_LENGTH, and replace the less common word with <RARE> token\n",
    "    for i in range(padding):\n",
    "        tokens.append('<PAD>')\n",
    "    line = [\n",
    "        word2Id_dict[tokens[k]]\n",
    "        if tokens[k] in word2Id_dict else word2Id_dict['<RARE>']\n",
    "        for k in range(len(tokens))\n",
    "    ]\n",
    "\n",
    "    return line\n",
    "\n",
    "text = \"the flower shown has yellow anther red pistil and bright red petals.\"\n",
    "print(text)\n",
    "print(sent2IdList(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx2word(indices_list):\n",
    "    results_list = []\n",
    "    for indices in indices_list:\n",
    "        string = ''\n",
    "        length_of_string = 0\n",
    "        for idx in indices:\n",
    "            if idx == '5428':\n",
    "                string = string + ''\n",
    "            elif idx == '5427':\n",
    "                break\n",
    "            else:\n",
    "                string = string + id2word_dict[idx] + ' '\n",
    "        results_list.append(string.strip())\n",
    "    return results_list\n",
    "\n",
    "def remove_empty_string(string_list):\n",
    "    empty_flag = False\n",
    "    for string in string_list:\n",
    "        if string == '':\n",
    "            empty_flag = True\n",
    "            break\n",
    "    if empty_flag == False:\n",
    "        return string_list\n",
    "    else:\n",
    "        new_string_list = []\n",
    "        for string in string_list:\n",
    "            if string != '':\n",
    "                new_string_list.append(string)\n",
    "        return new_string_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-large-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-large-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\n",
    "    'bert-large-uncased', \n",
    "    do_lower_case=False,\n",
    "    do_basic_tokenize=False\n",
    ")\n",
    "bert_model = TFBertModel.from_pretrained('bert-large-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "def turn_to_bert_embedding(string_list):\n",
    "    try:\n",
    "        bert_inputs = bert_tokenizer(string_list, return_tensors=\"tf\", padding='max_length',max_length=30)\n",
    "        bert_outputs = bert_model(bert_inputs)\n",
    "        caption_embedding = bert_outputs.last_hidden_state[:,0]\n",
    "    except(ValueError):\n",
    "        print(string_list)\n",
    "    return caption_embedding.numpy().tolist()\n",
    "\n",
    "test_string = ['this flower is white and pink in color with petals that have small veins',\n",
    "               'the flower shown has a purple and white petal with white anther', \n",
    "               'the four heart shaped pink petals of this flower are striped with fuchsia and their centers are yellow and white']  \n",
    "print(len(turn_to_bert_embedding(test_string)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_EMBEDDING_FILE = './dataset/bert_embedding.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7370 image in training data\n"
     ]
    }
   ],
   "source": [
    "data_path = './dataset'\n",
    "df = pd.read_pickle(data_path + '/text2ImgData.pkl')\n",
    "num_training_sample = len(df)\n",
    "n_images_train = num_training_sample\n",
    "print('There are %d image in training data' % (n_images_train))\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df['texts'] = df['Captions'].apply(lambda x: idx2word(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Captions</th>\n",
       "      <th>ImagePath</th>\n",
       "      <th>texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...</td>\n",
       "      <td>./102flowers/image_06734.jpg</td>\n",
       "      <td>[the petals of the flower are pink in color an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...</td>\n",
       "      <td>./102flowers/image_06736.jpg</td>\n",
       "      <td>[this flower has white petals and yellow pisti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...</td>\n",
       "      <td>./102flowers/image_06737.jpg</td>\n",
       "      <td>[the petals on this flower are pink with white...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...</td>\n",
       "      <td>./102flowers/image_06738.jpg</td>\n",
       "      <td>[the flower has a smooth purple petal with whi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...</td>\n",
       "      <td>./102flowers/image_06739.jpg</td>\n",
       "      <td>[this white flower has bright yellow stamen wi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Captions  \\\n",
       "0  [[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...   \n",
       "1  [[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...   \n",
       "2  [[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...   \n",
       "3  [[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...   \n",
       "4  [[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...   \n",
       "\n",
       "                      ImagePath  \\\n",
       "0  ./102flowers/image_06734.jpg   \n",
       "1  ./102flowers/image_06736.jpg   \n",
       "2  ./102flowers/image_06737.jpg   \n",
       "3  ./102flowers/image_06738.jpg   \n",
       "4  ./102flowers/image_06739.jpg   \n",
       "\n",
       "                                               texts  \n",
       "0  [the petals of the flower are pink in color an...  \n",
       "1  [this flower has white petals and yellow pisti...  \n",
       "2  [the petals on this flower are pink with white...  \n",
       "3  [the flower has a smooth purple petal with whi...  \n",
       "4  [this white flower has bright yellow stamen wi...  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the petals of the flower are pink in color and have a yellow center',\n",
       " 'this flower is pink and white in color with petals that are multi colored',\n",
       " 'the purple petals have shades of white with white anther and filament',\n",
       " 'this flower has large pink petals and a white stigma in the center',\n",
       " 'this flower has petals that are pink and has a yellow stamen',\n",
       " 'a flower with short and wide petals that is light purple',\n",
       " 'this flower has small pink petals with a yellow center',\n",
       " 'this flower has large rounded pink petals with curved edges and purple veins',\n",
       " 'this flower has purple petals as well as a white stamen']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0,'texts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['texts'] = df['texts'].apply(lambda x: remove_empty_string(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Captions</th>\n",
       "      <th>ImagePath</th>\n",
       "      <th>texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...</td>\n",
       "      <td>./102flowers/image_06734.jpg</td>\n",
       "      <td>[the petals of the flower are pink in color an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...</td>\n",
       "      <td>./102flowers/image_06736.jpg</td>\n",
       "      <td>[this flower has white petals and yellow pisti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...</td>\n",
       "      <td>./102flowers/image_06737.jpg</td>\n",
       "      <td>[the petals on this flower are pink with white...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...</td>\n",
       "      <td>./102flowers/image_06738.jpg</td>\n",
       "      <td>[the flower has a smooth purple petal with whi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...</td>\n",
       "      <td>./102flowers/image_06739.jpg</td>\n",
       "      <td>[this white flower has bright yellow stamen wi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Captions  \\\n",
       "0  [[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...   \n",
       "1  [[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...   \n",
       "2  [[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...   \n",
       "3  [[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...   \n",
       "4  [[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...   \n",
       "\n",
       "                      ImagePath  \\\n",
       "0  ./102flowers/image_06734.jpg   \n",
       "1  ./102flowers/image_06736.jpg   \n",
       "2  ./102flowers/image_06737.jpg   \n",
       "3  ./102flowers/image_06738.jpg   \n",
       "4  ./102flowers/image_06739.jpg   \n",
       "\n",
       "                                               texts  \n",
       "0  [the petals of the flower are pink in color an...  \n",
       "1  [this flower has white petals and yellow pisti...  \n",
       "2  [the petals on this flower are pink with white...  \n",
       "3  [the flower has a smooth purple petal with whi...  \n",
       "4  [this white flower has bright yellow stamen wi...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_caption_num(string_list):\n",
    "    return len(string_list)\n",
    "\n",
    "df['caption_num'] = df['texts'].apply(lambda c: count_caption_num(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Captions</th>\n",
       "      <th>ImagePath</th>\n",
       "      <th>texts</th>\n",
       "      <th>caption_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...</td>\n",
       "      <td>./102flowers/image_06734.jpg</td>\n",
       "      <td>[the petals of the flower are pink in color an...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...</td>\n",
       "      <td>./102flowers/image_06736.jpg</td>\n",
       "      <td>[this flower has white petals and yellow pisti...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...</td>\n",
       "      <td>./102flowers/image_06737.jpg</td>\n",
       "      <td>[the petals on this flower are pink with white...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...</td>\n",
       "      <td>./102flowers/image_06738.jpg</td>\n",
       "      <td>[the flower has a smooth purple petal with whi...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...</td>\n",
       "      <td>./102flowers/image_06739.jpg</td>\n",
       "      <td>[this white flower has bright yellow stamen wi...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Captions  \\\n",
       "0  [[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...   \n",
       "1  [[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...   \n",
       "2  [[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...   \n",
       "3  [[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...   \n",
       "4  [[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...   \n",
       "\n",
       "                      ImagePath  \\\n",
       "0  ./102flowers/image_06734.jpg   \n",
       "1  ./102flowers/image_06736.jpg   \n",
       "2  ./102flowers/image_06737.jpg   \n",
       "3  ./102flowers/image_06738.jpg   \n",
       "4  ./102flowers/image_06739.jpg   \n",
       "\n",
       "                                               texts  caption_num  \n",
       "0  [the petals of the flower are pink in color an...            9  \n",
       "1  [this flower has white petals and yellow pisti...           10  \n",
       "2  [the petals on this flower are pink with white...            9  \n",
       "3  [the flower has a smooth purple petal with whi...           10  \n",
       "4  [this white flower has bright yellow stamen wi...            9  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{9: 1959, 10: 4837, 8: 488, 7: 75, 6: 10, 5: 1}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_dict = {}\n",
    "for num in df['caption_num'].tolist():\n",
    "    if num in num_dict:\n",
    "        num_dict[num]+=1\n",
    "    else:\n",
    "        num_dict[num]=1\n",
    "num_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['embeddings'] = df['texts'].apply(lambda x : turn_to_bert_embedding(x))\n",
    "len(df.loc[0,'embeddings'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(BERT_EMBEDDING_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70495"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No need to run this cell (run inside dataset_generator())\n",
    "df_bert = pd.read_pickle(BERT_EMBEDDING_FILE)\n",
    "\n",
    "embeddings = df_bert['embeddings'].values\n",
    "embedding = []\n",
    "\n",
    "for i in range(len(embeddings)):\n",
    "    for emb in embeddings[i]:\n",
    "        embedding.append(emb)\n",
    "embedding = np.asarray(embedding)\n",
    "embedding.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Captions</th>\n",
       "      <th>ImagePath</th>\n",
       "      <th>texts</th>\n",
       "      <th>caption_num</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...</td>\n",
       "      <td>./102flowers/image_06734.jpg</td>\n",
       "      <td>[the petals of the flower are pink in color an...</td>\n",
       "      <td>9</td>\n",
       "      <td>[[-0.6344168782234192, -0.8039702773094177, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...</td>\n",
       "      <td>./102flowers/image_06736.jpg</td>\n",
       "      <td>[this flower has white petals and yellow pisti...</td>\n",
       "      <td>10</td>\n",
       "      <td>[[-0.2546939551830292, -0.029211539775133133, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...</td>\n",
       "      <td>./102flowers/image_06737.jpg</td>\n",
       "      <td>[the petals on this flower are pink with white...</td>\n",
       "      <td>9</td>\n",
       "      <td>[[-0.15689069032669067, -0.4372050166130066, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...</td>\n",
       "      <td>./102flowers/image_06738.jpg</td>\n",
       "      <td>[the flower has a smooth purple petal with whi...</td>\n",
       "      <td>10</td>\n",
       "      <td>[[-0.07732152938842773, -0.5917125940322876, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...</td>\n",
       "      <td>./102flowers/image_06739.jpg</td>\n",
       "      <td>[this white flower has bright yellow stamen wi...</td>\n",
       "      <td>9</td>\n",
       "      <td>[[-0.3246757686138153, -0.187540203332901, -0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Captions  \\\n",
       "0  [[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...   \n",
       "1  [[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...   \n",
       "2  [[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...   \n",
       "3  [[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...   \n",
       "4  [[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...   \n",
       "\n",
       "                      ImagePath  \\\n",
       "0  ./102flowers/image_06734.jpg   \n",
       "1  ./102flowers/image_06736.jpg   \n",
       "2  ./102flowers/image_06737.jpg   \n",
       "3  ./102flowers/image_06738.jpg   \n",
       "4  ./102flowers/image_06739.jpg   \n",
       "\n",
       "                                               texts  caption_num  \\\n",
       "0  [the petals of the flower are pink in color an...            9   \n",
       "1  [this flower has white petals and yellow pisti...           10   \n",
       "2  [the petals on this flower are pink with white...            9   \n",
       "3  [the flower has a smooth purple petal with whi...           10   \n",
       "4  [this white flower has bright yellow stamen wi...            9   \n",
       "\n",
       "                                          embeddings  \n",
       "0  [[-0.6344168782234192, -0.8039702773094177, 0....  \n",
       "1  [[-0.2546939551830292, -0.029211539775133133, ...  \n",
       "2  [[-0.15689069032669067, -0.4372050166130066, -...  \n",
       "3  [[-0.07732152938842773, -0.5917125940322876, -...  \n",
       "4  [[-0.3246757686138153, -0.187540203332901, -0....  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bert.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7370 image in training data\n"
     ]
    }
   ],
   "source": [
    "data_path = './dataset'\n",
    "df = pd.read_pickle(data_path + '/text2ImgData.pkl')\n",
    "num_training_sample = len(df)\n",
    "n_images_train = num_training_sample\n",
    "print('There are %d image in training data' % (n_images_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Captions</th>\n",
       "      <th>ImagePath</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6734</th>\n",
       "      <td>[[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...</td>\n",
       "      <td>./102flowers/image_06734.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6736</th>\n",
       "      <td>[[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...</td>\n",
       "      <td>./102flowers/image_06736.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6737</th>\n",
       "      <td>[[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...</td>\n",
       "      <td>./102flowers/image_06737.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6738</th>\n",
       "      <td>[[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...</td>\n",
       "      <td>./102flowers/image_06738.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6739</th>\n",
       "      <td>[[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...</td>\n",
       "      <td>./102flowers/image_06739.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Captions  \\\n",
       "ID                                                        \n",
       "6734  [[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...   \n",
       "6736  [[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...   \n",
       "6737  [[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...   \n",
       "6738  [[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...   \n",
       "6739  [[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...   \n",
       "\n",
       "                         ImagePath  \n",
       "ID                                  \n",
       "6734  ./102flowers/image_06734.jpg  \n",
       "6736  ./102flowers/image_06736.jpg  \n",
       "6737  ./102flowers/image_06737.jpg  \n",
       "6738  ./102flowers/image_06738.jpg  \n",
       "6739  ./102flowers/image_06739.jpg  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "Reference: https://mmuratarat.github.io/2019-02-28/data-augmentation-in-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this competition, you have to generate image in size 64x64x3\n",
    "IMAGE_HEIGHT = 64\n",
    "IMAGE_WIDTH = 64\n",
    "IMAGE_CHANNEL = 3\n",
    "MAPPING_FUNC = None\n",
    "\n",
    "def vertical_flip(tf_img):\n",
    "    return tf.image.flip_left_right(tf_img)\n",
    "\n",
    "def horizontal_flip(tf_img):\n",
    "    return tf.image.flip_up_down(tf_img)\n",
    "\n",
    "def brightness(tf_img):\n",
    "    return tf.image.random_brightness(tf_img, delta=0.4, seed=2)\n",
    "\n",
    "def central_crop(tf_img):\n",
    "    return tf.image.central_crop(tf_img, 0.8)\n",
    "\n",
    "def noise_injection(tf_img):\n",
    "    noise = tf.random.normal(shape=tf.shape(tf_img), mean=0.0, stddev=1, dtype=tf.float32)\n",
    "    return tf.add(tf_img, noise)\n",
    "\n",
    "def data_generator(caption, image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.cast(img, tf.float32)\n",
    "    img = (img - 127.5) / 127.5\n",
    "    img.set_shape([None, None, 3])\n",
    "    \n",
    "    if MAPPING_FUNC is not None:\n",
    "        img = MAPPING_FUNC(img)\n",
    "\n",
    "    img = tf.image.resize(img, size=[IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "    img.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL])\n",
    "    caption = tf.cast(caption, tf.float32)    \n",
    "    return img, caption\n",
    "\n",
    "def dataset_interface(caption, image_path):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((caption, image_path))\n",
    "    dataset = dataset.map(data_generator, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.shuffle(len(caption))\n",
    "    return dataset\n",
    "\n",
    "def dataset_generator(filenames, batch_size):\n",
    "    # load the training data into two NumPy arrays\n",
    "    # df = pd.read_pickle(filenames)\n",
    "    # captions = df['Captions'].values\n",
    "    # caption = []\n",
    "    # # each image has 1 to 10 corresponding captions\n",
    "    # # we choose one of them randomly for training\n",
    "    # for i in range(len(captions)):\n",
    "    #     caption.append(random.choice(captions[i]))\n",
    "    # caption = np.asarray(caption)\n",
    "    # caption = caption.astype(np.int64)\n",
    "    # image_path = df['ImagePath'].values\n",
    "\n",
    "    #------------------------------------------\n",
    "    df_bert = pd.read_pickle(filenames)\n",
    "\n",
    "    embeddings = df_bert['embeddings'].values\n",
    "    imagePaths = df_bert['ImagePath'].values\n",
    "    embedding = []\n",
    "    image_path = []\n",
    "\n",
    "    for i in range(len(embeddings)):\n",
    "        for j in range(len(embeddings[i])):\n",
    "            embedding.append(embeddings[i][j])\n",
    "            image_path.append(imagePaths[i])\n",
    "    embedding = np.asarray(embedding)\n",
    "    embedding = embedding.astype(np.float32)\n",
    "    caption = embedding\n",
    "    image_path = np.asarray(image_path)\n",
    "    #------------------------------------------    \n",
    "    \n",
    "    # assume that each row of `features` corresponds to the same row as `labels`.\n",
    "    assert caption.shape[0] == image_path.shape[0]\n",
    "    \n",
    "    original_dataset = dataset_interface(caption, image_path)\n",
    "    dataset = original_dataset #----------------\n",
    "\n",
    "    # MAPPING_FUNC = vertical_flip\n",
    "    # vertical_dataset = dataset_interface(caption, image_path)\n",
    "\n",
    "    # MAPPING_FUNC = horizontal_flip\n",
    "    # horizontal_dataset = dataset_interface(caption, image_path)\n",
    "\n",
    "    # MAPPING_FUNC = brightness\n",
    "    # brightness_dataset = dataset_interface(caption, image_path)\n",
    "    \n",
    "    # dataset = original_dataset.concatenate(vertical_dataset)\n",
    "    # dataset = dataset.concatenate(horizontal_dataset)\n",
    "    # dataset = dataset.concatenate(brightness_dataset)\n",
    "    dataset = dataset.shuffle(len(caption)).batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "# dataset = dataset_generator(data_path + '/text2ImgData.pkl', BATCH_SIZE)\n",
    "dataset = dataset_generator(BERT_EMBEDDING_FILE, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1101"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparas = {\n",
    "    'MAX_SEQ_LENGTH': 20,                     # maximum sequence length\n",
    "    'EMBED_DIM': 256,                         # word embedding dimension\n",
    "    'VOCAB_SIZE': len(word2Id_dict),          # size of dictionary of captions\n",
    "    'RNN_HIDDEN_SIZE': 128,                   # number of RNN neurons\n",
    "    'Z_DIM': 512,                             # random noise z dimension\n",
    "    'DENSE_DIM': 128,                         # number of neurons in dense layer\n",
    "    'IMAGE_SIZE': [64, 64, 3],                # render image size\n",
    "    'BATCH_SIZE': BATCH_SIZE,\n",
    "    'LR': 1e-4,\n",
    "    'LR_DECAY': 0.5,\n",
    "    'BETA_1': 0.5,\n",
    "    'N_EPOCH': 600,\n",
    "    'N_SAMPLE': len(dataset) * BATCH_SIZE,          # size of training data\n",
    "    'CHECKPOINTS_DIR': './checkpoints/demo',  # checkpoint path\n",
    "    'PRINT_FREQ': 1                          # printing frequency of loss\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Model\n",
    "* DCGAN\n",
    "* Add some BN layers\n",
    "* Loss function: wgan-gp\n",
    "* Train by critic (5D+1G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Encode text (a caption) into hidden representation\n",
    "    input: text, which is a list of ids\n",
    "    output: embedding, or hidden representation of input text in dimension of RNN_HIDDEN_SIZE\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        self.batch_size = self.hparas['BATCH_SIZE']\n",
    "        \n",
    "        # embedding with tensorflow API\n",
    "        self.embedding = layers.Embedding(self.hparas['VOCAB_SIZE'], self.hparas['EMBED_DIM'])\n",
    "        # RNN, here we use GRU cell, another common RNN cell similar to LSTM\n",
    "        self.gru = layers.GRU(self.hparas['RNN_HIDDEN_SIZE'],\n",
    "                              return_sequences=True,\n",
    "                              return_state=True,\n",
    "                              recurrent_initializer='glorot_uniform')\n",
    "\n",
    "        # self.bidirect = layers.Bidirectional(layers.LSTM(128))\n",
    "        # self.d1 = layers.Dense(128, activation=\"relu\")\n",
    "    \n",
    "    def call(self, text, hidden):\n",
    "        text = self.embedding(text)\n",
    "        output, state = self.gru(text, initial_state = hidden)\n",
    "        # output = self.bidirect(text)\n",
    "        # output = self.d1(output)\n",
    "        return output[:, -1, :], state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.hparas['BATCH_SIZE'], self.hparas['RNN_HIDDEN_SIZE']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Generate fake image based on given text(hidden representation) and noise z\n",
    "    input: text and noise\n",
    "    output: fake image with size 64*64*3\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas):\n",
    "        super(Generator, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.d1 = tf.keras.layers.Dense(self.hparas['DENSE_DIM'])\n",
    "        self.d2 = tf.keras.layers.Dense(64*64*3)\n",
    "\n",
    "        self.d3 = layers.Dense(16*16*256, use_bias=False)\n",
    "        self.bn1= layers.BatchNormalization()\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.bn3 = layers.BatchNormalization()\n",
    "\n",
    "        self.Conv2DTrans1 = layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding=\"same\", use_bias=False)\n",
    "        self.Conv2DTrans2 = layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding=\"same\", use_bias=False)\n",
    "        self.Conv2DTrans3 = layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding=\"same\", use_bias=False)\n",
    "   \n",
    "    def call(self, text, noise_z):\n",
    "        text = self.flatten(text)\n",
    "        text = self.d1(text)\n",
    "        text = tf.nn.leaky_relu(text)\n",
    "        \n",
    "        # concatenate input text and random noise\",\n",
    "        text_concat = tf.concat([noise_z, text], axis=1)\n",
    "        text_concat = self.d3(text_concat)\n",
    "        text_concat = self.bn1(text_concat)\n",
    "        text_concat = tf.reshape(text_concat, [-1, 16, 16, 256])\n",
    "        \n",
    "        text_concat = self.Conv2DTrans1(text_concat)\n",
    "        text_concat = self.bn2(text_concat)\n",
    "        text_concat = tf.nn.leaky_relu(text_concat)\n",
    "\n",
    "        text_concat = self.Conv2DTrans2(text_concat)\n",
    "        text_concat = self.bn3(text_concat)\n",
    "        text_concat = tf.nn.leaky_relu(text_concat)\n",
    "\n",
    "        text_concat = self.Conv2DTrans3(text_concat)\n",
    "\n",
    "        logits = tf.reshape(text_concat, [-1, 64, 64, 3])\n",
    "        output = tf.nn.tanh(logits)\n",
    "\n",
    "        return logits, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Differentiate the real and fake image\n",
    "    input: image and corresponding text\n",
    "    output: labels, the real image should be 1, while the fake should be 0\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.d_text = tf.keras.layers.Dense(self.hparas['DENSE_DIM'])\n",
    "        self.d_img = tf.keras.layers.Dense(self.hparas['DENSE_DIM'])\n",
    "        self.d = tf.keras.layers.Dense(1)\n",
    "\n",
    "       \n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.bn3 = layers.BatchNormalization()\n",
    "        self.dropout = layers.Dropout(0.3)\n",
    "\n",
    "        self.Conv2D1 = layers.Conv2D(64, (5, 5), strides=(2, 2), padding=\"same\")\n",
    "        self.Conv2D2 = layers.Conv2D(128, (5, 5), strides=(2, 2), padding=\"same\")\n",
    "\n",
    "    def call(self, img, text):\n",
    "        text = self.flatten(text)\n",
    "        text = self.d_text(text)\n",
    "        text = self.bn1(text)\n",
    "        text = tf.nn.leaky_relu(text)\n",
    "\n",
    "        img = self.Conv2D1(img)\n",
    "        img = self.bn2(img)\n",
    "        img = tf.nn.leaky_relu(img)\n",
    "        img = self.dropout(img)\n",
    "\n",
    "        img = self.Conv2D2(img)\n",
    "        img = self.bn3(img)\n",
    "        img = tf.nn.leaky_relu(img)\n",
    "        img = self.dropout(img)\n",
    "        img = self.flatten(img)\n",
    "        img = self.d_img(img)\n",
    "        \n",
    "        # concatenate image with paired text\n",
    "        img_text = tf.concat([text, img], axis=1)\n",
    "        logits = self.d(img_text)\n",
    "        output = tf.nn.sigmoid(logits)\n",
    "        \n",
    "        return logits, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_encoder = TextEncoder(hparas)\n",
    "generator = Generator(hparas)\n",
    "discriminator = Discriminator(hparas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_logits, fake_logits):\n",
    "    # output value of real image should be 1\n",
    "    real_loss = cross_entropy(tf.ones_like(real_logits), real_logits)\n",
    "    # output value of fake image should be 0\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_logits), fake_logits)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    # output value of fake image should be 0\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use seperated optimizers for training generator and discriminator\n",
    "generator_optimizer = tf.keras.optimizers.Adam(hparas['LR'])\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(hparas['LR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one benefit of tf.train.Checkpoint() API is we can save everything seperately\n",
    "checkpoint_dir = hparas['CHECKPOINTS_DIR']\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                #  text_encoder=text_encoder,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "# def DC_DTrain(image, caption, hidden, noise):\n",
    "#     # z = tf.random.normal(hparas['BZ'])\n",
    "#     noise_g = tf.random.normal(shape=[hparas['BATCH_SIZE'], hparas['Z_DIM']], mean=0.0, stddev=1.0)\n",
    "\n",
    "#     with tf.GradientTape() as tp:\n",
    "#         with tf.GradientTape() as tp_gp:\n",
    "#             text_embed, hidden = text_encoder(caption, hidden)\n",
    "#             # _, fake_image = generator(text_embed, noise)\n",
    "#             # real_logits, real_output = discriminator(image, text_embed)\n",
    "#             # fake_logits, fake_output = discriminator(fake_image, text_embed)\n",
    "\n",
    "#             _, x_bar = generator(text_embed, noise_g, training = True)\n",
    "#             epsilon = tf.random.uniform([BATCH_SIZE,1,1,1])\n",
    "#             x = image\n",
    "#             x_hat = epsilon * x + (1. - epsilon) * x_bar\n",
    "\n",
    "#             x_bar = x_bar + noise * tf.random.normal(x_bar.shape)\n",
    "#             x = x + noise * tf.random.normal(x.shape)\n",
    "#             x_hat = x_hat + noise * tf.random.normal(x_hat.shape)\n",
    "\n",
    "#             z0, _ = discriminator(x_bar, text_embed, training = True)\n",
    "#             z1, _ = discriminator(x, text_embed, training = True)\n",
    "#             z2, _ = discriminator(x_hat, text_embed, training = True)\n",
    "\n",
    "#             gradient_penalty = tp_gp.gradient(z2,x_hat)\n",
    "#             gradient_penalty = tf.sqrt(tf.reduce_sum(tf.math.square(gradient_penalty),axis=[1,2,3]))\n",
    "#             loss = z0 - z1 + 10. * tf.math.square((gradient_penalty - 1.))\n",
    "#             ld = tf.reduce_mean(loss)\n",
    "#             lg = - tf.reduce_mean(z0)\n",
    "\n",
    "#     gradient_d = tp.gradient(ld, discriminator.trainable_variables)\n",
    "\n",
    "#     discriminator_optimizer.apply_gradients(zip(gradient_d, discriminator.trainable_variables))\n",
    "\n",
    "#     return lg, ld\n",
    "\n",
    "# @tf.function\n",
    "# def DC_GTrain(image, caption, hidden, noise):\n",
    "#     # z = tf.random.normal(hparas['BZ'])\n",
    "#     noise_g = tf.random.normal(shape=[hparas['BATCH_SIZE'], hparas['Z_DIM']], mean=0.0, stddev=1.0)\n",
    "\n",
    "#     with tf.GradientTape() as tp:\n",
    "#         with tf.GradientTape() as tp_gp:\n",
    "#             text_embed, hidden = text_encoder(caption, hidden)\n",
    "#             # _, fake_image = generator(text_embed, noise)\n",
    "#             # real_logits, real_output = discriminator(image, text_embed)\n",
    "#             # fake_logits, fake_output = discriminator(fake_image, text_embed)\n",
    "\n",
    "#             _, x_bar = generator(text_embed, noise_g, training = True)\n",
    "#             epsilon = tf.random.uniform([BATCH_SIZE,1,1,1])\n",
    "#             x = image\n",
    "#             x_hat = epsilon * x + (1. - epsilon) * x_bar\n",
    "\n",
    "#             x_bar = x_bar + noise * tf.random.normal(x_bar.shape)\n",
    "#             x = x + noise * tf.random.normal(x.shape)\n",
    "#             x_hat = x_hat + noise * tf.random.normal(x_hat.shape)\n",
    "\n",
    "#             z0, _ = discriminator(x_bar, text_embed, training = True)\n",
    "#             z1, _ = discriminator(x, text_embed, training = True)\n",
    "#             z2, _ = discriminator(x_hat, text_embed, training = True)\n",
    "\n",
    "#             gradient_penalty = tp_gp.gradient(z2,x_hat)\n",
    "#             gradient_penalty = tf.sqrt(tf.reduce_sum(tf.math.square(gradient_penalty),axis=[1,2,3]))\n",
    "#             loss = z0 - z1 + 10. * tf.math.square((gradient_penalty - 1.))\n",
    "#             ld = tf.reduce_mean(loss)\n",
    "#             lg = - tf.reduce_mean(z0)\n",
    "\n",
    "#     gradient_g = tp.gradient(lg, generator.trainable_variables)\n",
    "\n",
    "#     generator_optimizer.apply_gradients(zip(gradient_g, generator.trainable_variables))\n",
    "\n",
    "#     return lg, ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT version here!!!!!!!!!!!\n",
    "\n",
    "@tf.function\n",
    "def DC_DTrain(image, embedding, hidden, noise):\n",
    "    # z = tf.random.normal(hparas['BZ'])\n",
    "    noise_g = tf.random.normal(shape=[hparas['BATCH_SIZE'], hparas['Z_DIM']], mean=0.0, stddev=1.0)\n",
    "\n",
    "    with tf.GradientTape() as tp:\n",
    "        with tf.GradientTape() as tp_gp:\n",
    "            text_embed = embedding\n",
    "            # text_embed, hidden = text_encoder(caption, hidden)\n",
    "            # _, fake_image = generator(text_embed, noise)\n",
    "            # real_logits, real_output = discriminator(image, text_embed)\n",
    "            # fake_logits, fake_output = discriminator(fake_image, text_embed)\n",
    "\n",
    "            _, x_bar = generator(text_embed, noise_g, training = True)\n",
    "            epsilon = tf.random.uniform([BATCH_SIZE,1,1,1])\n",
    "            x = image\n",
    "            x_hat = epsilon * x + (1. - epsilon) * x_bar\n",
    "\n",
    "            x_bar = x_bar + noise * tf.random.normal(x_bar.shape)\n",
    "            x = x + noise * tf.random.normal(x.shape)\n",
    "            x_hat = x_hat + noise * tf.random.normal(x_hat.shape)\n",
    "\n",
    "            z0, _ = discriminator(x_bar, text_embed, training = True)\n",
    "            z1, _ = discriminator(x, text_embed, training = True)\n",
    "            z2, _ = discriminator(x_hat, text_embed, training = True)\n",
    "\n",
    "            gradient_penalty = tp_gp.gradient(z2,x_hat)\n",
    "            gradient_penalty = tf.sqrt(tf.reduce_sum(tf.math.square(gradient_penalty),axis=[1,2,3]))\n",
    "            loss = z0 - z1 + 10. * tf.math.square((gradient_penalty - 1.))\n",
    "            ld = tf.reduce_mean(loss)\n",
    "            lg = - tf.reduce_mean(z0)\n",
    "\n",
    "    gradient_d = tp.gradient(ld, discriminator.trainable_variables)\n",
    "\n",
    "    discriminator_optimizer.apply_gradients(zip(gradient_d, discriminator.trainable_variables))\n",
    "\n",
    "    return lg, ld\n",
    "\n",
    "@tf.function\n",
    "def DC_GTrain(image, embedding, hidden, noise):\n",
    "    # z = tf.random.normal(hparas['BZ'])\n",
    "    noise_g = tf.random.normal(shape=[hparas['BATCH_SIZE'], hparas['Z_DIM']], mean=0.0, stddev=1.0)\n",
    "\n",
    "    with tf.GradientTape() as tp:\n",
    "        with tf.GradientTape() as tp_gp:\n",
    "            text_embed = embedding\n",
    "            # text_embed, hidden = text_encoder(caption, hidden)\n",
    "            # _, fake_image = generator(text_embed, noise)\n",
    "            # real_logits, real_output = discriminator(image, text_embed)\n",
    "            # fake_logits, fake_output = discriminator(fake_image, text_embed)\n",
    "\n",
    "            _, x_bar = generator(text_embed, noise_g, training = True)\n",
    "            epsilon = tf.random.uniform([BATCH_SIZE,1,1,1])\n",
    "            x = image\n",
    "            x_hat = epsilon * x + (1. - epsilon) * x_bar\n",
    "\n",
    "            x_bar = x_bar + noise * tf.random.normal(x_bar.shape)\n",
    "            x = x + noise * tf.random.normal(x.shape)\n",
    "            x_hat = x_hat + noise * tf.random.normal(x_hat.shape)\n",
    "\n",
    "            z0, _ = discriminator(x_bar, text_embed, training = True)\n",
    "            z1, _ = discriminator(x, text_embed, training = True)\n",
    "            z2, _ = discriminator(x_hat, text_embed, training = True)\n",
    "\n",
    "            gradient_penalty = tp_gp.gradient(z2,x_hat)\n",
    "            gradient_penalty = tf.sqrt(tf.reduce_sum(tf.math.square(gradient_penalty),axis=[1,2,3]))\n",
    "            loss = z0 - z1 + 10. * tf.math.square((gradient_penalty - 1.))\n",
    "            ld = tf.reduce_mean(loss)\n",
    "            lg = - tf.reduce_mean(z0)\n",
    "\n",
    "    gradient_g = tp.gradient(lg, generator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradient_g, generator.trainable_variables))\n",
    "\n",
    "    return lg, ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(embedding, noise, hidden):\n",
    "    # text_embed, hidden = text_encoder(caption, hidden)\n",
    "    _, fake_image = generator(embedding, noise)\n",
    "    return fake_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "DC_Train = (\n",
    "    DC_DTrain,\n",
    "    DC_DTrain,\n",
    "    DC_DTrain,\n",
    "    DC_DTrain,\n",
    "    DC_DTrain,\n",
    "    DC_GTrain,\n",
    ")\n",
    "\n",
    "DC_Critic = len(DC_Train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visulization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1], 3))\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w, :] = image\n",
    "    return img\n",
    "\n",
    "def imsave(images, size, path):\n",
    "    # getting the pixel values between [0, 1] to save it\n",
    "    return plt.imsave(path, merge(images, size)*0.5 + 0.5)\n",
    "\n",
    "def save_images(images, size, image_path):\n",
    "    return imsave(images, size, image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_generator(caption, batch_size):\n",
    "    caption = np.asarray(caption)\n",
    "    caption = caption.astype(np.int64)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(caption)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ni = int(np.ceil(np.sqrt(hparas['BATCH_SIZE'])))\n",
    "# sample_size = hparas['BATCH_SIZE']\n",
    "# sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, hparas['Z_DIM'])).astype(np.float32)\n",
    "# sample_sentence = [\"the flower shown has yellow anther red pistil and bright red petals.\"] * int(sample_size/ni) + \\\n",
    "#                   [\"this flower has petals that are yellow, white and purple and has dark lines\"] * int(sample_size/ni) + \\\n",
    "#                   [\"the petals on this flower are white with a yellow center\"] * int(sample_size/ni) + \\\n",
    "#                   [\"this flower has a lot of small round pink petals.\"] * int(sample_size/ni) + \\\n",
    "#                   [\"this flower is orange in color, and has petals that are ruffled and rounded.\"] * int(sample_size/ni) + \\\n",
    "#                   [\"the flower has yellow petals and the center of it is brown.\"] * int(sample_size/ni) + \\\n",
    "#                   [\"this flower has petals that are blue and white.\"] * int(sample_size/ni) +\\\n",
    "#                   [\"these white flowers have petals that start off white in color and end in a white towards the tips.\"] * int(sample_size/ni)\n",
    "\n",
    "# for i, sent in enumerate(sample_sentence):\n",
    "#     sample_sentence[i] = sent2IdList(sent)\n",
    "# sample_sentence = sample_generator(sample_sentence, hparas['BATCH_SIZE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_sample_generator(embedding, batch_size):\n",
    "    embedding = np.asarray(embedding)\n",
    "    embedding = embedding.astype(np.float32)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(embedding)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ni = int(np.ceil(np.sqrt(hparas['BATCH_SIZE'])))\n",
    "sample_size = hparas['BATCH_SIZE']\n",
    "sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, hparas['Z_DIM'])).astype(np.float32)\n",
    "bert_sample_sentence = [\"the flower shown has yellow anther red pistil and bright red petals.\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower has petals that are yellow, white and purple and has dark lines\"] * int(sample_size/ni) + \\\n",
    "                  [\"the petals on this flower are white with a yellow center\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower has a lot of small round pink petals.\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower is orange in color, and has petals that are ruffled and rounded.\"] * int(sample_size/ni) + \\\n",
    "                  [\"the flower has yellow petals and the center of it is brown.\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower has petals that are blue and white.\"] * int(sample_size/ni) +\\\n",
    "                  [\"these white flowers have petals that start off white in color and end in a white towards the tips.\"] * int(sample_size/ni)\n",
    "bert_sample_sentence *= 2\n",
    "for i, sent in enumerate(bert_sample_sentence):\n",
    "    bert_sample_sentence[i] = turn_to_bert_embedding(sent)\n",
    "bert_sample_sentence = bert_sample_generator(bert_sample_sentence, hparas['BATCH_SIZE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f50258fcd00>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(checkpoint_dir + '/ckpt-11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SAVE_PATH = './samples/bert_from550'\n",
    "if not os.path.exists(SAMPLE_SAVE_PATH):\n",
    "    os.makedirs(SAMPLE_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    # hidden state of RNN\n",
    "    # hidden = text_encoder.initialize_hidden_state()\n",
    "    hidden = None\n",
    "    steps_per_epoch = int(hparas['N_SAMPLE']/hparas['BATCH_SIZE'])\n",
    "    \n",
    "    ctr = 0\n",
    "    for epoch in range(epochs):\n",
    "        g_total_loss = 0\n",
    "        d_total_loss = 0\n",
    "        start = time.time()\n",
    "        \n",
    "        if epoch < 200:\n",
    "            noise = 1.0 / float(epoch + 1)\n",
    "        else:\n",
    "            noise = 0.0\n",
    "        \n",
    "        for image, embedding in dataset:\n",
    "            lg, ld = DC_Train[ctr](image, embedding, hidden, noise)\n",
    "            ctr += 1\n",
    "            g_total_loss += lg.numpy()\n",
    "            d_total_loss += ld.numpy()\n",
    "            if ctr == DC_Critic : ctr = 0\n",
    "            \n",
    "        time_tuple = time.localtime()\n",
    "        time_string = time.strftime(\"%m/%d/%Y, %H:%M:%S\", time_tuple)\n",
    "            \n",
    "        print(\"Epoch {}, gen_loss: {:.4f}, disc_loss: {:.4f}\".format(epoch+1,\n",
    "                                                                     g_total_loss/steps_per_epoch,\n",
    "                                                                     d_total_loss/steps_per_epoch))\n",
    "        print('Time for epoch {} is {:.4f} sec'.format(epoch+1, time.time()-start))\n",
    "        \n",
    "        # save the model\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "        \n",
    "        # visualization\n",
    "        if (epoch + 1) % hparas['PRINT_FREQ'] == 0:\n",
    "            # for caption in sample_sentence:\n",
    "            #     fake_image = test_step(caption, sample_seed, hidden)\n",
    "            for embedding in bert_sample_sentence:\n",
    "                fake_image = test_step(embedding, sample_seed, hidden)\n",
    "            save_images(fake_image, [ni, ni], f'{SAMPLE_SAVE_PATH}/train_{epoch:02d}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, gen_loss: -25.3255, disc_loss: -1.3255\n",
      "Time for epoch 1 is 111.8034 sec\n",
      "Epoch 2, gen_loss: -10.3467, disc_loss: -1.1569\n",
      "Time for epoch 2 is 109.3706 sec\n",
      "Epoch 3, gen_loss: -2.2772, disc_loss: -1.0429\n",
      "Time for epoch 3 is 109.3221 sec\n",
      "Epoch 4, gen_loss: 3.5349, disc_loss: -0.9391\n",
      "Time for epoch 4 is 109.5179 sec\n",
      "Epoch 5, gen_loss: -5.5585, disc_loss: -1.1219\n",
      "Time for epoch 5 is 109.3134 sec\n",
      "Epoch 6, gen_loss: -11.3459, disc_loss: -0.8973\n",
      "Time for epoch 6 is 109.4062 sec\n",
      "Epoch 7, gen_loss: -6.1984, disc_loss: -1.2228\n",
      "Time for epoch 7 is 109.3298 sec\n",
      "Epoch 8, gen_loss: -8.1177, disc_loss: -1.1331\n",
      "Time for epoch 8 is 109.5182 sec\n",
      "Epoch 9, gen_loss: -17.2248, disc_loss: -1.0790\n",
      "Time for epoch 9 is 109.4854 sec\n",
      "Epoch 10, gen_loss: -26.1147, disc_loss: -1.2704\n",
      "Time for epoch 10 is 109.4011 sec\n",
      "Epoch 11, gen_loss: -26.8727, disc_loss: -1.0365\n",
      "Time for epoch 11 is 109.6090 sec\n",
      "Epoch 12, gen_loss: -28.4386, disc_loss: -0.9952\n",
      "Time for epoch 12 is 109.4973 sec\n",
      "Epoch 13, gen_loss: -19.2845, disc_loss: -1.1246\n",
      "Time for epoch 13 is 109.3399 sec\n",
      "Epoch 14, gen_loss: -23.4271, disc_loss: -1.3262\n",
      "Time for epoch 14 is 109.4248 sec\n",
      "Epoch 15, gen_loss: -20.3763, disc_loss: -1.3261\n",
      "Time for epoch 15 is 109.5406 sec\n",
      "Epoch 16, gen_loss: -18.8988, disc_loss: -1.1859\n",
      "Time for epoch 16 is 109.7689 sec\n",
      "Epoch 17, gen_loss: -21.2896, disc_loss: -1.2441\n",
      "Time for epoch 17 is 110.1392 sec\n",
      "Epoch 18, gen_loss: -20.2961, disc_loss: -1.3761\n",
      "Time for epoch 18 is 109.5990 sec\n",
      "Epoch 19, gen_loss: -24.1998, disc_loss: -1.3648\n",
      "Time for epoch 19 is 109.4383 sec\n",
      "Epoch 20, gen_loss: -20.2805, disc_loss: -1.3353\n",
      "Time for epoch 20 is 109.5448 sec\n",
      "Epoch 21, gen_loss: -19.5302, disc_loss: -1.4250\n",
      "Time for epoch 21 is 109.5376 sec\n",
      "Epoch 22, gen_loss: -19.5336, disc_loss: -1.3388\n",
      "Time for epoch 22 is 109.4160 sec\n",
      "Epoch 23, gen_loss: -18.1132, disc_loss: -1.4803\n",
      "Time for epoch 23 is 109.4296 sec\n",
      "Epoch 24, gen_loss: -17.8379, disc_loss: -1.3584\n",
      "Time for epoch 24 is 109.6105 sec\n",
      "Epoch 25, gen_loss: -21.4926, disc_loss: -1.5154\n",
      "Time for epoch 25 is 109.7318 sec\n",
      "Epoch 26, gen_loss: -18.4912, disc_loss: -1.5082\n",
      "Time for epoch 26 is 109.6144 sec\n",
      "Epoch 27, gen_loss: -18.3522, disc_loss: -1.5865\n",
      "Time for epoch 27 is 109.6037 sec\n",
      "Epoch 28, gen_loss: -18.5778, disc_loss: -1.6030\n",
      "Time for epoch 28 is 109.6145 sec\n",
      "Epoch 29, gen_loss: -16.0883, disc_loss: -1.6054\n",
      "Time for epoch 29 is 109.5955 sec\n",
      "Epoch 30, gen_loss: -14.8413, disc_loss: -1.6365\n",
      "Time for epoch 30 is 109.6808 sec\n",
      "Epoch 31, gen_loss: -16.6395, disc_loss: -1.6499\n",
      "Time for epoch 31 is 109.6329 sec\n",
      "Epoch 32, gen_loss: -17.8005, disc_loss: -1.6027\n",
      "Time for epoch 32 is 109.5774 sec\n",
      "Epoch 33, gen_loss: -18.6388, disc_loss: -1.6226\n",
      "Time for epoch 33 is 109.5529 sec\n",
      "Epoch 34, gen_loss: -17.3485, disc_loss: -1.5966\n",
      "Time for epoch 34 is 109.5597 sec\n",
      "Epoch 35, gen_loss: -17.1624, disc_loss: -1.6496\n",
      "Time for epoch 35 is 109.5898 sec\n",
      "Epoch 36, gen_loss: -17.3017, disc_loss: -1.6818\n",
      "Time for epoch 36 is 109.9321 sec\n",
      "Epoch 37, gen_loss: -16.2206, disc_loss: -1.6728\n",
      "Time for epoch 37 is 109.5552 sec\n",
      "Epoch 38, gen_loss: -14.3971, disc_loss: -1.6335\n",
      "Time for epoch 38 is 109.6048 sec\n",
      "Epoch 39, gen_loss: -14.3529, disc_loss: -1.6561\n",
      "Time for epoch 39 is 109.6145 sec\n",
      "Epoch 40, gen_loss: -14.4687, disc_loss: -1.6438\n",
      "Time for epoch 40 is 109.7713 sec\n",
      "Epoch 41, gen_loss: -13.6490, disc_loss: -1.6615\n",
      "Time for epoch 41 is 109.5791 sec\n",
      "Epoch 42, gen_loss: -13.1107, disc_loss: -1.7465\n",
      "Time for epoch 42 is 109.6087 sec\n",
      "Epoch 43, gen_loss: -12.7933, disc_loss: -1.7007\n",
      "Time for epoch 43 is 109.5120 sec\n",
      "Epoch 44, gen_loss: -11.8873, disc_loss: -1.7048\n",
      "Time for epoch 44 is 109.6906 sec\n",
      "Epoch 45, gen_loss: -11.7166, disc_loss: -1.7287\n",
      "Time for epoch 45 is 109.5849 sec\n",
      "Epoch 46, gen_loss: -12.1698, disc_loss: -1.6533\n",
      "Time for epoch 46 is 109.6028 sec\n",
      "Epoch 47, gen_loss: -11.2571, disc_loss: -1.6836\n",
      "Time for epoch 47 is 109.5716 sec\n",
      "Epoch 48, gen_loss: -11.8239, disc_loss: -1.5869\n",
      "Time for epoch 48 is 109.6439 sec\n",
      "Epoch 49, gen_loss: -11.6404, disc_loss: -1.7366\n",
      "Time for epoch 49 is 110.0844 sec\n",
      "Epoch 50, gen_loss: -9.9507, disc_loss: -1.6323\n",
      "Time for epoch 50 is 109.8531 sec\n"
     ]
    }
   ],
   "source": [
    "train(dataset, hparas['N_EPOCH'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Dataset\n",
    "If you change anything during preprocessing of training dataset, you must make sure same operations have be done in testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_data_generator(embedding, index):\n",
    "    embedding = tf.cast(embedding, tf.float32)\n",
    "    return embedding, index\n",
    "\n",
    "def testing_dataset_generator(data, batch_size, data_generator):\n",
    "    embeddings = data['embeddings'].values\n",
    "    embedding = []\n",
    "    for i in range(len(embeddings)):\n",
    "        embedding.append(embeddings[i])\n",
    "    embedding = np.asarray(embedding)\n",
    "    embedding = embedding.astype(np.float32)\n",
    "\n",
    "    index = data['ID'].values\n",
    "    index = np.asarray(index)\n",
    "    \n",
    "    assert embedding.shape[0] == index.shape[0]\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((embedding, index))\n",
    "    dataset = dataset.map(data_generator, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.repeat().batch(batch_size)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx2word_test(indices_list):\n",
    "    indices_list = [indices_list]\n",
    "    results_list = []\n",
    "    for indices in indices_list:\n",
    "        string = ''\n",
    "        length_of_string = 0\n",
    "        for idx in indices:\n",
    "            if idx == '5428':\n",
    "                string = string + ''\n",
    "            elif idx == '5427':\n",
    "                break\n",
    "            else:\n",
    "                string = string + id2word_dict[idx] + ' '\n",
    "        results_list.append(string.strip())\n",
    "    return results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('./dataset/testData.pkl')\n",
    "data['texts'] = data['Captions'].apply(lambda x: idx2word_test(x))\n",
    "data['texts'] = data['texts'].apply(lambda x: remove_empty_string(x))\n",
    "data['embeddings'] = data['texts'].apply(lambda x : turn_to_bert_embedding(x))\n",
    "captions = data['Captions'].values\n",
    "\n",
    "NUM_TEST = len(captions)\n",
    "EPOCH_TEST = int(NUM_TEST / hparas['BATCH_SIZE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataset = testing_dataset_generator(data, hparas['BATCH_SIZE'], testing_data_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(dataset):\n",
    "    # hidden = text_encoder.initialize_hidden_state()\n",
    "    hidden = None\n",
    "    sample_size = hparas['BATCH_SIZE']\n",
    "    sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, hparas['Z_DIM'])).astype(np.float32)\n",
    "    \n",
    "    step = 0\n",
    "    start = time.time()\n",
    "    for embedding, idx in dataset:\n",
    "        if step > EPOCH_TEST:\n",
    "            break\n",
    "        \n",
    "        fake_image = test_step(embedding, sample_seed, hidden)\n",
    "        step += 1\n",
    "        for i in range(hparas['BATCH_SIZE']):\n",
    "            plt.imsave('./inference/demo/inference_{:04d}.jpg'.format(idx[i]), fake_image[i].numpy()*0.5 + 0.5)\n",
    "            \n",
    "    print('Time for inference is {:.4f} sec'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f066cc4b430>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(checkpoint_dir + '/ckpt-12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for inference is 1.9416 sec\n"
     ]
    }
   ],
   "source": [
    "inference(testing_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output score.csv\n",
    "CAUTION: \n",
    "* Please modify GPU setting in <i>inception_score.py</i> if need.\n",
    "* Please run the below cmd in command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd testing\n",
    "!python inception_score.py ../inference/demo ../score_demo.csv 39 # BATCH_SIZE=39 is available using GTX 1080 Ti (need 9441MB memory)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5c5b210ffa015f2312f69f2248e3163602cc860559c1494ea467ed1fecf0f25e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
