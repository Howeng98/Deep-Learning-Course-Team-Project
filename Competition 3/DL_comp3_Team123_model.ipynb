{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLab Cup 3: Reverse Image Caption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"username\":\"chengchecheng\",\"key\":\"f6946452bb7b6297237d925ac6034cb7\"}Downloading datalab-cup3-reverse-image-caption-2021.zip to /home/kevin/NTHU/2021 Fall/Deep-Learning-Course-Team-Project/Competition 3\n",
      "100%|███████████████████████████████████████▉| 666M/667M [01:02<00:00, 11.6MB/s]\n",
      "100%|████████████████████████████████████████| 667M/667M [01:02<00:00, 11.2MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Follow the tutorial in this page:\n",
    "# https://www.endtoend.ai/tutorial/how-to-download-kaggle-datasets-on-ubuntu/\n",
    "!cat ~/.kaggle/kaggle.json\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "!kaggle competitions download -c datalab-cup3-reverse-image-caption-2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "# zipfile example\n",
    "def zip_list(file_path):\n",
    "    zf = zipfile.ZipFile(file_path, 'r')\n",
    "    zf.extractall('./')\n",
    "\n",
    "file_path = './datalab-cup3-reverse-image-caption-2021.zip'\n",
    "zip_list(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and Packages Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # disable warnings, info and errors \n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import re\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Dec 31 15:14:01 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 471.41       Driver Version: 471.41       CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| 28%   33C    P8    12W / 120W |    609MiB /  6144MiB |     27%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1500    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A      5364    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A      7360    C+G   ...bbwe\\Microsoft.Photos.exe    N/A      |\n",
      "|    0   N/A  N/A      8056    C+G   ...8bbwe\\Microsoft.Notes.exe    N/A      |\n",
      "|    0   N/A  N/A      8136    C+G   ...ekyb3d8bbwe\\YourPhone.exe    N/A      |\n",
      "|    0   N/A  N/A      8196    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      9824    C+G   ...cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A      9896    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     11036    C+G   ...nputApp\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     12120    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     12924    C+G   ...8wekyb3d8bbwe\\Cortana.exe    N/A      |\n",
      "|    0   N/A  N/A     13512    C+G   ...wekyb3d8bbwe\\Video.UI.exe    N/A      |\n",
      "|    0   N/A  N/A     16644    C+G   ...icrosoft VS Code\\Code.exe    N/A      |\n",
      "|    0   N/A  N/A     17916    C+G   ...LINE\\bin\\current\\LINE.exe    N/A      |\n",
      "|    0   N/A  N/A     18176    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     21008    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     21216    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     21432    C+G   Insufficient Permissions        N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the first GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        #for gpu in gpus:\n",
    "        #    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocseeing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 5427 vocabularies in total\n",
      "Word to id mapping, for example: flower -> 1\n",
      "Id to word mapping, for example: 1 -> flower\n",
      "Tokens: <PAD>: 5427; <RARE>: 5428\n"
     ]
    }
   ],
   "source": [
    "dictionary_path = './dictionary'\n",
    "vocab = np.load(dictionary_path + '/vocab.npy')\n",
    "print('there are {} vocabularies in total'.format(len(vocab)))\n",
    "\n",
    "word2Id_dict = dict(np.load(dictionary_path + '/word2Id.npy'))\n",
    "id2word_dict = dict(np.load(dictionary_path + '/id2Word.npy'))\n",
    "print('Word to id mapping, for example: %s -> %s' % ('flower', word2Id_dict['flower']))\n",
    "print('Id to word mapping, for example: %s -> %s' % ('1', id2word_dict['1']))\n",
    "print('Tokens: <PAD>: %s; <RARE>: %s' % (word2Id_dict['<PAD>'], word2Id_dict['<RARE>']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the flower shown has yellow anther red pistil and bright red petals.\n",
      "['9', '1', '82', '5', '11', '70', '20', '31', '3', '29', '20', '2', '5427', '5427', '5427', '5427', '5427', '5427', '5427', '5427']\n"
     ]
    }
   ],
   "source": [
    "def sent2IdList(line, MAX_SEQ_LENGTH=20):\n",
    "    MAX_SEQ_LIMIT = MAX_SEQ_LENGTH\n",
    "    padding = 0\n",
    "    \n",
    "    # data preprocessing, remove all puntuation in the texts\n",
    "    prep_line = re.sub('[%s]' % re.escape(string.punctuation), ' ', line.rstrip())\n",
    "    prep_line = prep_line.replace('-', ' ')\n",
    "    prep_line = prep_line.replace('-', ' ')\n",
    "    prep_line = prep_line.replace('  ', ' ')\n",
    "    prep_line = prep_line.replace('.', '')\n",
    "    tokens = prep_line.split(' ')\n",
    "    tokens = [\n",
    "        tokens[i] for i in range(len(tokens))\n",
    "        if tokens[i] != ' ' and tokens[i] != ''\n",
    "    ]\n",
    "    l = len(tokens)\n",
    "    padding = MAX_SEQ_LIMIT - l\n",
    "    \n",
    "    # make sure length of each text is equal to MAX_SEQ_LENGTH, and replace the less common word with <RARE> token\n",
    "    for i in range(padding):\n",
    "        tokens.append('<PAD>')\n",
    "    line = [\n",
    "        word2Id_dict[tokens[k]]\n",
    "        if tokens[k] in word2Id_dict else word2Id_dict['<RARE>']\n",
    "        for k in range(len(tokens))\n",
    "    ]\n",
    "\n",
    "    return line\n",
    "\n",
    "text = \"the flower shown has yellow anther red pistil and bright red petals.\"\n",
    "print(text)\n",
    "print(sent2IdList(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7370 image in training data\n"
     ]
    }
   ],
   "source": [
    "data_path = './dataset'\n",
    "df = pd.read_pickle(data_path + '/text2ImgData.pkl')\n",
    "num_training_sample = len(df)\n",
    "n_images_train = num_training_sample\n",
    "print('There are %d image in training data' % (n_images_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Captions</th>\n",
       "      <th>ImagePath</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6734</th>\n",
       "      <td>[[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...</td>\n",
       "      <td>./102flowers/image_06734.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6736</th>\n",
       "      <td>[[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...</td>\n",
       "      <td>./102flowers/image_06736.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6737</th>\n",
       "      <td>[[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...</td>\n",
       "      <td>./102flowers/image_06737.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6738</th>\n",
       "      <td>[[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...</td>\n",
       "      <td>./102flowers/image_06738.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6739</th>\n",
       "      <td>[[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...</td>\n",
       "      <td>./102flowers/image_06739.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Captions  \\\n",
       "ID                                                        \n",
       "6734  [[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...   \n",
       "6736  [[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...   \n",
       "6737  [[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...   \n",
       "6738  [[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...   \n",
       "6739  [[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...   \n",
       "\n",
       "                         ImagePath  \n",
       "ID                                  \n",
       "6734  ./102flowers/image_06734.jpg  \n",
       "6736  ./102flowers/image_06736.jpg  \n",
       "6737  ./102flowers/image_06737.jpg  \n",
       "6738  ./102flowers/image_06738.jpg  \n",
       "6739  ./102flowers/image_06739.jpg  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "Reference: https://mmuratarat.github.io/2019-02-28/data-augmentation-in-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this competition, you have to generate image in size 64x64x3\n",
    "IMAGE_HEIGHT = 64\n",
    "IMAGE_WIDTH = 64\n",
    "IMAGE_CHANNEL = 3\n",
    "MAPPING_FUNC = None\n",
    "\n",
    "def vertical_flip(tf_img):\n",
    "    return tf.image.flip_left_right(tf_img)\n",
    "\n",
    "def horizontal_flip(tf_img):\n",
    "    return tf.image.flip_up_down(tf_img)\n",
    "\n",
    "def brightness(tf_img):\n",
    "    return tf.image.random_brightness(tf_img, delta=0.4, seed=2)\n",
    "\n",
    "def central_crop(tf_img):\n",
    "    return tf.image.central_crop(tf_img, 0.8)\n",
    "\n",
    "def noise_injection(tf_img):\n",
    "    noise = tf.random.normal(shape=tf.shape(tf_img), mean=0.0, stddev=1, dtype=tf.float32)\n",
    "    return tf.add(tf_img, noise)\n",
    "\n",
    "def data_generator(caption, image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img.set_shape([None, None, 3])\n",
    "    \n",
    "    if MAPPING_FUNC is not None:\n",
    "        img = MAPPING_FUNC(img)\n",
    "\n",
    "    img = tf.image.resize(img, size=[IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "    img.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL])\n",
    "    caption = tf.cast(caption, tf.int32)    \n",
    "    return img, caption\n",
    "\n",
    "def dataset_interface(caption, image_path):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((caption, image_path))\n",
    "    dataset = dataset.map(data_generator, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.shuffle(len(caption))\n",
    "    return dataset\n",
    "\n",
    "def dataset_generator(filenames, batch_size):\n",
    "    # load the training data into two NumPy arrays\n",
    "    df = pd.read_pickle(filenames)\n",
    "    captions = df['Captions'].values\n",
    "    caption = []\n",
    "    # each image has 1 to 10 corresponding captions\n",
    "    # we choose one of them randomly for training\n",
    "    for i in range(len(captions)):\n",
    "        caption.append(random.choice(captions[i]))\n",
    "    caption = np.asarray(caption)\n",
    "    caption = caption.astype(np.int64)\n",
    "    image_path = df['ImagePath'].values\n",
    "    \n",
    "    # assume that each row of `features` corresponds to the same row as `labels`.\n",
    "    assert caption.shape[0] == image_path.shape[0]\n",
    "    \n",
    "    original_dataset = dataset_interface(caption, image_path)\n",
    "\n",
    "    MAPPING_FUNC = vertical_flip\n",
    "    vertical_dataset = dataset_interface(caption, image_path)\n",
    "\n",
    "    MAPPING_FUNC = horizontal_flip\n",
    "    horizontal_dataset = dataset_interface(caption, image_path)\n",
    "\n",
    "    MAPPING_FUNC = brightness\n",
    "    brightness_dataset = dataset_interface(caption, image_path)\n",
    "    \n",
    "    dataset = original_dataset.concatenate(vertical_dataset)\n",
    "    dataset = dataset.concatenate(horizontal_dataset)\n",
    "    dataset = dataset.concatenate(brightness_dataset)\n",
    "    dataset = dataset.shuffle(len(caption)).batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "dataset = dataset_generator(data_path + '/text2ImgData.pkl', BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparas = {\n",
    "    'MAX_SEQ_LENGTH': 20,                     # maximum sequence length\n",
    "    'EMBED_DIM': 256,                         # word embedding dimension\n",
    "    'VOCAB_SIZE': len(word2Id_dict),          # size of dictionary of captions\n",
    "    'RNN_HIDDEN_SIZE': 128,                   # number of RNN neurons\n",
    "    'Z_DIM': 512,                             # random noise z dimension\n",
    "    'DENSE_DIM': 128,                         # number of neurons in dense layer\n",
    "    'IMAGE_SIZE': [64, 64, 3],                # render image size\n",
    "    'BATCH_SIZE': 64,\n",
    "    'LR': 1e-4,\n",
    "    'LR_DECAY': 0.5,\n",
    "    'BETA_1': 0.5,\n",
    "    'N_EPOCH': 600,\n",
    "    'N_SAMPLE': num_training_sample,          # size of training data\n",
    "    'CHECKPOINTS_DIR': './checkpoints/demo',  # checkpoint path\n",
    "    'PRINT_FREQ': 2                          # printing frequency of loss\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditioinal GAN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Encode text (a caption) into hidden representation\n",
    "    input: text, which is a list of ids\n",
    "    output: embedding, or hidden representation of input text in dimension of RNN_HIDDEN_SIZE\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        self.batch_size = self.hparas['BATCH_SIZE']\n",
    "        \n",
    "        # embedding with tensorflow API\n",
    "        self.embedding = layers.Embedding(self.hparas['VOCAB_SIZE'], self.hparas['EMBED_DIM'])\n",
    "        # RNN, here we use GRU cell, another common RNN cell similar to LSTM\n",
    "        self.gru = layers.GRU(self.hparas['RNN_HIDDEN_SIZE'],\n",
    "                              return_sequences=True,\n",
    "                              return_state=True,\n",
    "                              recurrent_initializer='glorot_uniform')\n",
    "\n",
    "        # self.bidirect = layers.Bidirectional(layers.LSTM(128))\n",
    "        # self.d1 = layers.Dense(128, activation=\"relu\")\n",
    "    \n",
    "    def call(self, text, hidden):\n",
    "        text = self.embedding(text)\n",
    "        output, state = self.gru(text, initial_state = hidden)\n",
    "        # output = self.bidirect(text)\n",
    "        # output = self.d1(output)\n",
    "        return output[:, -1, :], state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.hparas['BATCH_SIZE'], self.hparas['RNN_HIDDEN_SIZE']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Generate fake image based on given text(hidden representation) and noise z\n",
    "    input: text and noise\n",
    "    output: fake image with size 64*64*3\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas):\n",
    "        super(Generator, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.d1 = tf.keras.layers.Dense(self.hparas['DENSE_DIM'])\n",
    "        self.d2 = tf.keras.layers.Dense(64*64*3)\n",
    "\n",
    "        self.d3 = layers.Dense(16*16*256, use_bias=False)\n",
    "        self.bn1= layers.BatchNormalization()\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.bn3 = layers.BatchNormalization()\n",
    "\n",
    "        self.Conv2DTrans1 = layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding=\"same\", use_bias=False)\n",
    "        self.Conv2DTrans2 = layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding=\"same\", use_bias=False)\n",
    "        self.Conv2DTrans3 = layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding=\"same\", use_bias=False)\n",
    "   \n",
    "    def call(self, text, noise_z):\n",
    "        text = self.flatten(text)\n",
    "        text = self.d1(text)\n",
    "        text = tf.nn.leaky_relu(text)\n",
    "        \n",
    "        # concatenate input text and random noise\",\n",
    "        text_concat = tf.concat([noise_z, text], axis=1)\n",
    "        text_concat = self.d3(text_concat)\n",
    "        text_concat = self.bn1(text_concat)\n",
    "        text_concat = tf.reshape(text_concat, [-1, 16, 16, 256])\n",
    "        \n",
    "        text_concat = self.Conv2DTrans1(text_concat)\n",
    "        text_concat = self.bn2(text_concat)\n",
    "        text_concat = tf.nn.leaky_relu(text_concat)\n",
    "\n",
    "        text_concat = self.Conv2DTrans2(text_concat)\n",
    "        text_concat = self.bn3(text_concat)\n",
    "        text_concat = tf.nn.leaky_relu(text_concat)\n",
    "\n",
    "        text_concat = self.Conv2DTrans3(text_concat)\n",
    "\n",
    "        logits = tf.reshape(text_concat, [-1, 64, 64, 3])\n",
    "        output = tf.nn.tanh(logits)\n",
    "\n",
    "        return logits, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Differentiate the real and fake image\n",
    "    input: image and corresponding text\n",
    "    output: labels, the real image should be 1, while the fake should be 0\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.d_text = tf.keras.layers.Dense(self.hparas['DENSE_DIM'])\n",
    "        self.d_img = tf.keras.layers.Dense(self.hparas['DENSE_DIM'])\n",
    "        self.d = tf.keras.layers.Dense(1)\n",
    "\n",
    "       \n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.bn3 = layers.BatchNormalization()\n",
    "        self.dropout = layers.Dropout(0.3)\n",
    "\n",
    "        self.Conv2D1 = layers.Conv2D(64, (5, 5), strides=(2, 2), padding=\"same\")\n",
    "        self.Conv2D2 = layers.Conv2D(128, (5, 5), strides=(2, 2), padding=\"same\")\n",
    "\n",
    "    def call(self, img, text):\n",
    "        text = self.flatten(text)\n",
    "        text = self.d_text(text)\n",
    "        text = self.bn1(text)\n",
    "        text = tf.nn.leaky_relu(text)\n",
    "\n",
    "        img = self.Conv2D1(img)\n",
    "        img = self.bn2(img)\n",
    "        img = tf.nn.leaky_relu(img)\n",
    "        img = self.dropout(img)\n",
    "\n",
    "        img = self.Conv2D2(img)\n",
    "        img = self.bn3(img)\n",
    "        img = tf.nn.leaky_relu(img)\n",
    "        img = self.dropout(img)\n",
    "        img = self.flatten(img)\n",
    "        img = self.d_img(img)\n",
    "        \n",
    "        # concatenate image with paired text\n",
    "        img_text = tf.concat([text, img], axis=1)\n",
    "        logits = self.d(img_text)\n",
    "        output = tf.nn.sigmoid(logits)\n",
    "        \n",
    "        return logits, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = TextEncoder(hparas)\n",
    "generator = Generator(hparas)\n",
    "discriminator = Discriminator(hparas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_logits, fake_logits):\n",
    "    # output value of real image should be 1\n",
    "    real_loss = cross_entropy(tf.ones_like(real_logits), real_logits)\n",
    "    # output value of fake image should be 0\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_logits), fake_logits)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    # output value of fake image should be 0\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use seperated optimizers for training generator and discriminator\n",
    "generator_optimizer = tf.keras.optimizers.Adam(hparas['LR'])\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(hparas['LR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one benefit of tf.train.Checkpoint() API is we can save everything seperately\n",
    "checkpoint_dir = hparas['CHECKPOINTS_DIR']\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 text_encoder=text_encoder,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def DC_DTrain(image, caption, hidden, noise):\n",
    "    # z = tf.random.normal(hparas['BZ'])\n",
    "    noise_g = tf.random.normal(shape=[hparas['BATCH_SIZE'], hparas['Z_DIM']], mean=0.0, stddev=1.0)\n",
    "\n",
    "    with tf.GradientTape() as tp:\n",
    "        with tf.GradientTape() as tp_gp:\n",
    "            text_embed, hidden = text_encoder(caption, hidden)\n",
    "            # _, fake_image = generator(text_embed, noise)\n",
    "            # real_logits, real_output = discriminator(image, text_embed)\n",
    "            # fake_logits, fake_output = discriminator(fake_image, text_embed)\n",
    "\n",
    "            _, x_bar = generator(text_embed, noise_g, training = True)\n",
    "            epsilon = tf.random.uniform([BATCH_SIZE,1,1,1])\n",
    "            x = image\n",
    "            x_hat = epsilon * x + (1. - epsilon) * x_bar\n",
    "\n",
    "            x_bar = x_bar + noise * tf.random.normal(x_bar.shape)\n",
    "            x = x + noise * tf.random.normal(x.shape)\n",
    "            x_hat = x_hat + noise * tf.random.normal(x_hat.shape)\n",
    "\n",
    "            z0, _ = discriminator(x_bar, text_embed, training = True)\n",
    "            z1, _ = discriminator(x, text_embed, training = True)\n",
    "            z2, _ = discriminator(x_hat, text_embed, training = True)\n",
    "\n",
    "            gradient_penalty = tp_gp.gradient(z2,x_hat)\n",
    "            gradient_penalty = tf.sqrt(tf.reduce_sum(tf.math.square(gradient_penalty),axis=[1,2,3]))\n",
    "            loss = z0 - z1 + 10. * tf.math.square((gradient_penalty - 1.))\n",
    "            ld = tf.reduce_mean(loss)\n",
    "            lg = - tf.reduce_mean(z0)\n",
    "\n",
    "    gradient_d = tp.gradient(ld, discriminator.trainable_variables)\n",
    "\n",
    "    discriminator_optimizer.apply_gradients(zip(gradient_d, discriminator.trainable_variables))\n",
    "\n",
    "    return lg, ld\n",
    "\n",
    "@tf.function\n",
    "def DC_GTrain(image, caption, hidden, noise):\n",
    "    # z = tf.random.normal(hparas['BZ'])\n",
    "    noise_g = tf.random.normal(shape=[hparas['BATCH_SIZE'], hparas['Z_DIM']], mean=0.0, stddev=1.0)\n",
    "\n",
    "    with tf.GradientTape() as tp:\n",
    "        with tf.GradientTape() as tp_gp:\n",
    "            text_embed, hidden = text_encoder(caption, hidden)\n",
    "            # _, fake_image = generator(text_embed, noise)\n",
    "            # real_logits, real_output = discriminator(image, text_embed)\n",
    "            # fake_logits, fake_output = discriminator(fake_image, text_embed)\n",
    "\n",
    "            _, x_bar = generator(text_embed, noise_g, training = True)\n",
    "            epsilon = tf.random.uniform([BATCH_SIZE,1,1,1])\n",
    "            x = image\n",
    "            x_hat = epsilon * x + (1. - epsilon) * x_bar\n",
    "\n",
    "            x_bar = x_bar + noise * tf.random.normal(x_bar.shape)\n",
    "            x = x + noise * tf.random.normal(x.shape)\n",
    "            x_hat = x_hat + noise * tf.random.normal(x_hat.shape)\n",
    "\n",
    "            z0, _ = discriminator(x_bar, text_embed, training = True)\n",
    "            z1, _ = discriminator(x, text_embed, training = True)\n",
    "            z2, _ = discriminator(x_hat, text_embed, training = True)\n",
    "\n",
    "            gradient_penalty = tp_gp.gradient(z2,x_hat)\n",
    "            gradient_penalty = tf.sqrt(tf.reduce_sum(tf.math.square(gradient_penalty),axis=[1,2,3]))\n",
    "            loss = z0 - z1 + 10. * tf.math.square((gradient_penalty - 1.))\n",
    "            ld = tf.reduce_mean(loss)\n",
    "            lg = - tf.reduce_mean(z0)\n",
    "\n",
    "    gradient_g = tp.gradient(lg, generator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradient_g, generator.trainable_variables))\n",
    "\n",
    "    return lg, ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(caption, noise, hidden):\n",
    "    text_embed, hidden = text_encoder(caption, hidden)\n",
    "    _, fake_image = generator(text_embed, noise)\n",
    "    return fake_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "DC_Train = (\n",
    "    DC_DTrain,\n",
    "    DC_DTrain,\n",
    "    DC_DTrain,\n",
    "    DC_DTrain,\n",
    "    DC_DTrain,\n",
    "    DC_GTrain,\n",
    ")\n",
    "\n",
    "DC_Critic = len(DC_Train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visulization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1], 3))\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w, :] = image\n",
    "    return img\n",
    "\n",
    "def imsave(images, size, path):\n",
    "    # getting the pixel values between [0, 1] to save it\n",
    "    return plt.imsave(path, merge(images, size)*0.5 + 0.5)\n",
    "\n",
    "def save_images(images, size, image_path):\n",
    "    return imsave(images, size, image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_generator(caption, batch_size):\n",
    "    caption = np.asarray(caption)\n",
    "    caption = caption.astype(np.int64)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(caption)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ni = int(np.ceil(np.sqrt(hparas['BATCH_SIZE'])))\n",
    "sample_size = hparas['BATCH_SIZE']\n",
    "sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, hparas['Z_DIM'])).astype(np.float32)\n",
    "sample_sentence = [\"the flower shown has yellow anther red pistil and bright red petals.\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower has petals that are yellow, white and purple and has dark lines\"] * int(sample_size/ni) + \\\n",
    "                  [\"the petals on this flower are white with a yellow center\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower has a lot of small round pink petals.\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower is orange in color, and has petals that are ruffled and rounded.\"] * int(sample_size/ni) + \\\n",
    "                  [\"the flower has yellow petals and the center of it is brown.\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower has petals that are blue and white.\"] * int(sample_size/ni) +\\\n",
    "                  [\"these white flowers have petals that start off white in color and end in a white towards the tips.\"] * int(sample_size/ni)\n",
    "\n",
    "for i, sent in enumerate(sample_sentence):\n",
    "    sample_sentence[i] = sent2IdList(sent)\n",
    "sample_sentence = sample_generator(sample_sentence, hparas['BATCH_SIZE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SAVE_PATH = './samples/data_aug'\n",
    "if not os.path.exists(SAMPLE_SAVE_PATH):\n",
    "    os.makedirs(SAMPLE_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    # hidden state of RNN\n",
    "    hidden = text_encoder.initialize_hidden_state()\n",
    "    steps_per_epoch = int(hparas['N_SAMPLE']/hparas['BATCH_SIZE'])\n",
    "    \n",
    "    ctr = 0\n",
    "    for epoch in range(hparas['N_EPOCH']):\n",
    "        g_total_loss = 0\n",
    "        d_total_loss = 0\n",
    "        start = time.time()\n",
    "        \n",
    "        if epoch < 200:\n",
    "            noise = 1.0 / float(epoch + 1)\n",
    "        else:\n",
    "            noise = 0.0\n",
    "        \n",
    "        for image, caption in dataset:\n",
    "            # g_loss, d_loss = train_step(image, caption, hidden)\n",
    "            # g_total_loss += g_loss\n",
    "            # d_total_loss += d_loss\n",
    "            lg, ld = DC_Train[ctr](image, caption, hidden, noise) # (real_img, embed, noise_decay)\n",
    "            ctr += 1\n",
    "            g_total_loss += lg.numpy()\n",
    "            d_total_loss += ld.numpy()\n",
    "            if ctr == DC_Critic : ctr = 0\n",
    "            \n",
    "        time_tuple = time.localtime()\n",
    "        time_string = time.strftime(\"%m/%d/%Y, %H:%M:%S\", time_tuple)\n",
    "            \n",
    "        print(\"Epoch {}, gen_loss: {:.4f}, disc_loss: {:.4f}\".format(epoch+1,\n",
    "                                                                     g_total_loss/steps_per_epoch,\n",
    "                                                                     d_total_loss/steps_per_epoch))\n",
    "        print('Time for epoch {} is {:.4f} sec'.format(epoch+1, time.time()-start))\n",
    "        \n",
    "        # save the model\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "        \n",
    "        # visualization\n",
    "        if (epoch + 1) % hparas['PRINT_FREQ'] == 0:\n",
    "            for caption in sample_sentence:\n",
    "                fake_image = test_step(caption, sample_seed, hidden)\n",
    "            save_images(fake_image, [ni, ni], f'{SAMPLE_SAVE_PATH}/train_{epoch:02d}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, gen_loss: 73.9757, disc_loss: -90.0106\n",
      "Time for epoch 1 is 142.1015 sec\n",
      "Epoch 2, gen_loss: 60.8442, disc_loss: -46.4319\n",
      "Time for epoch 2 is 108.0243 sec\n",
      "Epoch 3, gen_loss: 54.6786, disc_loss: -32.7383\n",
      "Time for epoch 3 is 108.5080 sec\n",
      "Epoch 4, gen_loss: 13.0229, disc_loss: -28.0899\n",
      "Time for epoch 4 is 105.8900 sec\n",
      "Epoch 5, gen_loss: 7.1462, disc_loss: -27.6263\n",
      "Time for epoch 5 is 98.4359 sec\n",
      "Epoch 6, gen_loss: 6.7402, disc_loss: -23.3633\n",
      "Time for epoch 6 is 94.4476 sec\n",
      "Epoch 7, gen_loss: 4.6539, disc_loss: -21.0453\n",
      "Time for epoch 7 is 98.9751 sec\n",
      "Epoch 8, gen_loss: 9.4946, disc_loss: -20.4935\n",
      "Time for epoch 8 is 103.5243 sec\n",
      "Epoch 9, gen_loss: 10.7162, disc_loss: -20.2983\n",
      "Time for epoch 9 is 102.7106 sec\n",
      "Epoch 10, gen_loss: 17.4246, disc_loss: -19.8870\n",
      "Time for epoch 10 is 97.3631 sec\n",
      "Epoch 11, gen_loss: 17.6436, disc_loss: -19.4131\n",
      "Time for epoch 11 is 107.5117 sec\n",
      "Epoch 12, gen_loss: 21.0869, disc_loss: -20.2515\n",
      "Time for epoch 12 is 115.2191 sec\n",
      "Epoch 13, gen_loss: 24.2555, disc_loss: -19.1293\n",
      "Time for epoch 13 is 136.3396 sec\n",
      "Epoch 14, gen_loss: 28.5410, disc_loss: -20.5511\n",
      "Time for epoch 14 is 128.9185 sec\n",
      "Epoch 15, gen_loss: 31.5408, disc_loss: -21.3768\n",
      "Time for epoch 15 is 138.8858 sec\n",
      "Epoch 16, gen_loss: 39.6776, disc_loss: -19.3919\n",
      "Time for epoch 16 is 142.4383 sec\n",
      "Epoch 17, gen_loss: 43.6229, disc_loss: -20.5484\n",
      "Time for epoch 17 is 144.2754 sec\n",
      "Epoch 18, gen_loss: 44.9885, disc_loss: -20.4094\n",
      "Time for epoch 18 is 143.9244 sec\n",
      "Epoch 19, gen_loss: 46.3334, disc_loss: -20.3040\n",
      "Time for epoch 19 is 143.8994 sec\n",
      "Epoch 20, gen_loss: 42.9252, disc_loss: -19.8513\n",
      "Time for epoch 20 is 144.8708 sec\n",
      "Epoch 21, gen_loss: 48.6087, disc_loss: -22.5298\n",
      "Time for epoch 21 is 144.7482 sec\n",
      "Epoch 22, gen_loss: 62.8912, disc_loss: -24.0798\n",
      "Time for epoch 22 is 146.6800 sec\n",
      "Epoch 23, gen_loss: 60.9471, disc_loss: -24.2825\n",
      "Time for epoch 23 is 145.0563 sec\n",
      "Epoch 24, gen_loss: 64.7679, disc_loss: -25.0539\n",
      "Time for epoch 24 is 146.5378 sec\n",
      "Epoch 25, gen_loss: 70.0330, disc_loss: -27.9650\n",
      "Time for epoch 25 is 196.3865 sec\n",
      "Epoch 26, gen_loss: 64.7611, disc_loss: -24.5496\n",
      "Time for epoch 26 is 195.6950 sec\n",
      "Epoch 27, gen_loss: 71.7391, disc_loss: -26.7066\n",
      "Time for epoch 27 is 192.1345 sec\n",
      "Epoch 28, gen_loss: 78.0235, disc_loss: -26.9360\n",
      "Time for epoch 28 is 190.1761 sec\n",
      "Epoch 29, gen_loss: 80.8211, disc_loss: -27.3976\n",
      "Time for epoch 29 is 191.6319 sec\n",
      "Epoch 30, gen_loss: 83.6703, disc_loss: -26.6443\n",
      "Time for epoch 30 is 193.0451 sec\n",
      "Epoch 31, gen_loss: 78.1904, disc_loss: -28.2030\n",
      "Time for epoch 31 is 191.8882 sec\n",
      "Epoch 32, gen_loss: 76.1812, disc_loss: -26.5320\n",
      "Time for epoch 32 is 193.1947 sec\n",
      "Epoch 33, gen_loss: 70.2452, disc_loss: -26.8995\n",
      "Time for epoch 33 is 199.2994 sec\n",
      "Epoch 34, gen_loss: 73.8344, disc_loss: -28.6641\n",
      "Time for epoch 34 is 199.3044 sec\n",
      "Epoch 35, gen_loss: 74.1287, disc_loss: -28.5998\n",
      "Time for epoch 35 is 198.2582 sec\n",
      "Epoch 36, gen_loss: 63.3806, disc_loss: -26.6686\n",
      "Time for epoch 36 is 184.6455 sec\n",
      "Epoch 37, gen_loss: 55.0815, disc_loss: -28.5836\n",
      "Time for epoch 37 is 188.7825 sec\n",
      "Epoch 38, gen_loss: 48.0374, disc_loss: -27.2234\n",
      "Time for epoch 38 is 193.5433 sec\n",
      "Epoch 39, gen_loss: 50.8444, disc_loss: -23.1192\n",
      "Time for epoch 39 is 195.1525 sec\n",
      "Epoch 40, gen_loss: 45.6118, disc_loss: -24.7009\n",
      "Time for epoch 40 is 188.9244 sec\n",
      "Epoch 41, gen_loss: 46.4787, disc_loss: -26.9823\n",
      "Time for epoch 41 is 190.2700 sec\n",
      "Epoch 42, gen_loss: 43.9514, disc_loss: -27.5417\n",
      "Time for epoch 42 is 141.9906 sec\n",
      "Epoch 43, gen_loss: 45.7880, disc_loss: -27.4028\n",
      "Time for epoch 43 is 103.6897 sec\n"
     ]
    }
   ],
   "source": [
    "train(dataset, hparas['N_EPOCH'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Dataset\n",
    "If you change anything during preprocessing of training dataset, you must make sure same operations have be done in testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_data_generator(caption, index):\n",
    "    caption = tf.cast(caption, tf.float32)\n",
    "    return caption, index\n",
    "\n",
    "def testing_dataset_generator(batch_size, data_generator):\n",
    "    data = pd.read_pickle('./dataset/testData.pkl')\n",
    "    captions = data['Captions'].values\n",
    "    caption = []\n",
    "    for i in range(len(captions)):\n",
    "        caption.append(captions[i])\n",
    "    caption = np.asarray(caption)\n",
    "    caption = caption.astype(np.int64)\n",
    "    index = data['ID'].values\n",
    "    index = np.asarray(index)\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((caption, index))\n",
    "    dataset = dataset.map(data_generator, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.repeat().batch(batch_size)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataset = testing_dataset_generator(hparas['BATCH_SIZE'], testing_data_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('./dataset/testData.pkl')\n",
    "captions = data['Captions'].values\n",
    "\n",
    "NUM_TEST = len(captions)\n",
    "EPOCH_TEST = int(NUM_TEST / hparas['BATCH_SIZE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(dataset):\n",
    "    hidden = text_encoder.initialize_hidden_state()\n",
    "    sample_size = hparas['BATCH_SIZE']\n",
    "    sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, hparas['Z_DIM'])).astype(np.float32)\n",
    "    \n",
    "    step = 0\n",
    "    start = time.time()\n",
    "    for captions, idx in dataset:\n",
    "        if step > EPOCH_TEST:\n",
    "            break\n",
    "        \n",
    "        fake_image = test_step(captions, sample_seed, hidden)\n",
    "        step += 1\n",
    "        for i in range(hparas['BATCH_SIZE']):\n",
    "            plt.imsave('./inference/demo/inference_{:04d}.jpg'.format(idx[i]), fake_image[i].numpy()*0.5 + 0.5)\n",
    "            \n",
    "    print('Time for inference is {:.4f} sec'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fd6ae6ed940>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(checkpoint_dir + '/ckpt-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for inference is 1.0649 sec\n"
     ]
    }
   ],
   "source": [
    "inference(testing_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output score.csv\n",
    "CAUTION: \n",
    "* Please modify GPU setting in <i>inception_score.py</i> if need.\n",
    "* Please run the below cmd in command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: can't open file 'inception_score.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!cd testing\n",
    "!python inception_score.py ../inference/demo ../score.csv 39 # BATCH_SIZE=39 is available using GTX 1080 Ti (need 9441MB memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo - TA80\n",
    "CAUTION: The code here doesn't work because the inference images aren't provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(idx):\n",
    "    fig = plt.figure(figsize=(14, 14))\n",
    "    \n",
    "    for count, i in enumerate(idx):\n",
    "        loc = np.where(i==index)[0][0]\n",
    "        text = ''\n",
    "        for word in captions[loc]:\n",
    "            if id2word_dict[word] != '<PAD>':\n",
    "                text += id2word_dict[word]\n",
    "                text += ' '\n",
    "        print(text)\n",
    "        \n",
    "        path = './inference/TA80/inference_{:04d}.jpg'.format(i)\n",
    "        fake_iamge = plt.imread(path)\n",
    "        \n",
    "        plt.subplot(7, 7, count+1)\n",
    "        plt.imshow(fake_iamge)\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flower with white long white petals and very long purple stamen \n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './inference/TA80/inference_0023.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-234b46ae47f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m               7049, 7160]\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mvisualize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-34-c0a4dc562183>\u001b[0m in \u001b[0;36mvisualize\u001b[0;34m(idx)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./inference/TA80/inference_{:04d}.jpg'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mfake_iamge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   2244\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2245\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2246\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   1494\u001b[0m                     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1495\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1496\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mimg_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1497\u001b[0m         return (_pil_png_to_float_array(image)\n\u001b[1;32m   1498\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPngImagePlugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPngImageFile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   2910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2911\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2912\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2913\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './inference/TA80/inference_0023.jpg'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1008x1008 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_pickle('./dataset/testData.pkl')\n",
    "captions = data['Captions'].values\n",
    "index = data['ID'].values\n",
    "random_idx = [23, 216, 224, 413, 713, 859, 876, 974, 1177, 1179, 1241, 2169, 2196, 2237, \n",
    "              2356, 2611, 2621, 2786, 2951, 2962, 3145, 3255, 3327, 3639, 3654, 3927, 4262, \n",
    "              4321, 4517, 5067, 5147, 5955, 6167, 6216, 6410, 6413, 6579, 6584, 6804, 6988, \n",
    "              7049, 7160]\n",
    "\n",
    "visualize(random_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './inference/TA80/'\n",
    "img_path = Path(DATA_PATH).glob('*.jpg')\n",
    "img_path = [str(path.resolve()) for path in img_path]\n",
    "img_path = np.asarray(img_path)\n",
    "\n",
    "idx = np.random.randint(len(captions), size=42)\n",
    "idx.sort()\n",
    "\n",
    "random_idx = []\n",
    "for each in idx:\n",
    "    path = img_path[each].split(\"/\")\n",
    "    path = path[-1].replace('inference_', '')\n",
    "    random_idx.append(int(path.replace('.jpg', '')))\n",
    "    \n",
    "visualize(random_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5c5b210ffa015f2312f69f2248e3163602cc860559c1494ea467ed1fecf0f25e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
