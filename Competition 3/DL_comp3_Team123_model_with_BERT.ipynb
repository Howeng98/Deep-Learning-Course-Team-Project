{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLab Cup 3: Reverse Image Caption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow the tutorial in this page:\n",
    "# https://www.endtoend.ai/tutorial/how-to-download-kaggle-datasets-on-ubuntu/\n",
    "!cat ~/.kaggle/kaggle.json\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "!kaggle competitions download -c datalab-cup3-reverse-image-caption-2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "# zipfile example\n",
    "def zip_list(file_path):\n",
    "    zf = zipfile.ZipFile(file_path, 'r')\n",
    "    zf.extractall('./')\n",
    "\n",
    "file_path = './datalab-cup3-reverse-image-caption-2021.zip'\n",
    "zip_list(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and Packages Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # disable warnings, info and errors \n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan  6 14:19:05 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 495.29.05    Driver Version: 495.29.05    CUDA Version: 11.5     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0 Off |                  N/A |\n",
      "|  0%   43C    P8    10W / 250W |     16MiB /  8119MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  On   | 00000000:02:00.0 Off |                  N/A |\n",
      "|  0%   42C    P8    11W / 250W |      8MiB /  8119MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1201      G   /usr/lib/xorg/Xorg                  9MiB |\n",
      "|    0   N/A  N/A      1439      G   /usr/bin/gnome-shell                2MiB |\n",
      "|    1   N/A  N/A      1201      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the first GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        #for gpu in gpus:\n",
    "        #    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 5427 vocabularies in total\n",
      "Word to id mapping, for example: flower -> 1\n",
      "Id to word mapping, for example: 1 -> flower\n",
      "Tokens: <PAD>: 5427; <RARE>: 5428\n"
     ]
    }
   ],
   "source": [
    "dictionary_path = './dictionary'\n",
    "vocab = np.load(dictionary_path + '/vocab.npy')\n",
    "print('there are {} vocabularies in total'.format(len(vocab)))\n",
    "\n",
    "word2Id_dict = dict(np.load(dictionary_path + '/word2Id.npy'))\n",
    "id2word_dict = dict(np.load(dictionary_path + '/id2Word.npy'))\n",
    "print('Word to id mapping, for example: %s -> %s' % ('flower', word2Id_dict['flower']))\n",
    "print('Id to word mapping, for example: %s -> %s' % ('1', id2word_dict['1']))\n",
    "print('Tokens: <PAD>: %s; <RARE>: %s' % (word2Id_dict['<PAD>'], word2Id_dict['<RARE>']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the flower shown has yellow anther red pistil and bright red petals.\n",
      "['9', '1', '82', '5', '11', '70', '20', '31', '3', '29', '20', '2', '5427', '5427', '5427', '5427', '5427', '5427', '5427', '5427']\n"
     ]
    }
   ],
   "source": [
    "def sent2IdList(line, MAX_SEQ_LENGTH=20):\n",
    "    MAX_SEQ_LIMIT = MAX_SEQ_LENGTH\n",
    "    padding = 0\n",
    "    \n",
    "    # data preprocessing, remove all puntuation in the texts\n",
    "    prep_line = re.sub('[%s]' % re.escape(string.punctuation), ' ', line.rstrip())\n",
    "    prep_line = prep_line.replace('-', ' ')\n",
    "    prep_line = prep_line.replace('-', ' ')\n",
    "    prep_line = prep_line.replace('  ', ' ')\n",
    "    prep_line = prep_line.replace('.', '')\n",
    "    tokens = prep_line.split(' ')\n",
    "    tokens = [\n",
    "        tokens[i] for i in range(len(tokens))\n",
    "        if tokens[i] != ' ' and tokens[i] != ''\n",
    "    ]\n",
    "    l = len(tokens)\n",
    "    padding = MAX_SEQ_LIMIT - l\n",
    "    \n",
    "    # make sure length of each text is equal to MAX_SEQ_LENGTH, and replace the less common word with <RARE> token\n",
    "    for i in range(padding):\n",
    "        tokens.append('<PAD>')\n",
    "    line = [\n",
    "        word2Id_dict[tokens[k]]\n",
    "        if tokens[k] in word2Id_dict else word2Id_dict['<RARE>']\n",
    "        for k in range(len(tokens))\n",
    "    ]\n",
    "\n",
    "    return line\n",
    "\n",
    "text = \"the flower shown has yellow anther red pistil and bright red petals.\"\n",
    "print(text)\n",
    "print(sent2IdList(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx2word(indices_list):\n",
    "    results_list = []\n",
    "    for indices in indices_list:\n",
    "        string = ''\n",
    "        length_of_string = 0\n",
    "        for idx in indices:\n",
    "            if idx == '5428':\n",
    "                string = string + ''\n",
    "            elif idx == '5427':\n",
    "                break\n",
    "            else:\n",
    "                string = string + id2word_dict[idx] + ' '\n",
    "        results_list.append(string.strip())\n",
    "    return results_list\n",
    "\n",
    "def remove_empty_string(string_list):\n",
    "    empty_flag = False\n",
    "    for string in string_list:\n",
    "        if string == '':\n",
    "            empty_flag = True\n",
    "            break\n",
    "    if empty_flag == False:\n",
    "        return string_list\n",
    "    else:\n",
    "        new_string_list = []\n",
    "        for string in string_list:\n",
    "            if string != '':\n",
    "                new_string_list.append(string)\n",
    "        return new_string_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\n",
    "    'bert-large-uncased', \n",
    "    do_lower_case=False,\n",
    "    do_basic_tokenize=False\n",
    ")\n",
    "bert_model = TFBertModel.from_pretrained('bert-large-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_to_bert_embedding(string_list):\n",
    "    try:\n",
    "        bert_inputs = bert_tokenizer(string_list, return_tensors=\"tf\", padding='max_length',max_length=30)\n",
    "        bert_outputs = bert_model(bert_inputs)\n",
    "        caption_embedding = bert_outputs.last_hidden_state[:,0]\n",
    "    except(ValueError):\n",
    "        print(string_list)\n",
    "    return caption_embedding.numpy().tolist()\n",
    "\n",
    "test_string = ['this flower is white and pink in color with petals that have small veins',\n",
    "               'the flower shown has a purple and white petal with white anther', \n",
    "               'the four heart shaped pink petals of this flower are striped with fuchsia and their centers are yellow and white']  \n",
    "print(len(turn_to_bert_embedding(test_string)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_EMBEDDING_FILE = './dataset/bert_embedding.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset - Run Once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7370 image in training data\n"
     ]
    }
   ],
   "source": [
    "data_path = './dataset'\n",
    "df = pd.read_pickle(data_path + '/text2ImgData.pkl')\n",
    "num_training_sample = len(df)\n",
    "n_images_train = num_training_sample\n",
    "print('There are %d image in training data' % (n_images_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Captions</th>\n",
       "      <th>ImagePath</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6734</th>\n",
       "      <td>[[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...</td>\n",
       "      <td>./102flowers/image_06734.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6736</th>\n",
       "      <td>[[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...</td>\n",
       "      <td>./102flowers/image_06736.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6737</th>\n",
       "      <td>[[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...</td>\n",
       "      <td>./102flowers/image_06737.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6738</th>\n",
       "      <td>[[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...</td>\n",
       "      <td>./102flowers/image_06738.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6739</th>\n",
       "      <td>[[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...</td>\n",
       "      <td>./102flowers/image_06739.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Captions  \\\n",
       "ID                                                        \n",
       "6734  [[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...   \n",
       "6736  [[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...   \n",
       "6737  [[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...   \n",
       "6738  [[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...   \n",
       "6739  [[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...   \n",
       "\n",
       "                         ImagePath  \n",
       "ID                                  \n",
       "6734  ./102flowers/image_06734.jpg  \n",
       "6736  ./102flowers/image_06736.jpg  \n",
       "6737  ./102flowers/image_06737.jpg  \n",
       "6738  ./102flowers/image_06738.jpg  \n",
       "6739  ./102flowers/image_06739.jpg  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0,'texts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['texts'] = df['texts'].apply(lambda x: remove_empty_string(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_caption_num(string_list):\n",
    "    return len(string_list)\n",
    "\n",
    "df['caption_num'] = df['texts'].apply(lambda c: count_caption_num(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dict = {}\n",
    "for num in df['caption_num'].tolist():\n",
    "    if num in num_dict:\n",
    "        num_dict[num]+=1\n",
    "    else:\n",
    "        num_dict[num]=1\n",
    "num_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['embeddings'] = df['texts'].apply(lambda x : turn_to_bert_embedding(x))\n",
    "len(df.loc[0,'embeddings'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(BERT_EMBEDDING_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to run this cell (run inside dataset_generator())\n",
    "df_bert = pd.read_pickle(BERT_EMBEDDING_FILE)\n",
    "\n",
    "embeddings = df_bert['embeddings'].values\n",
    "embedding = []\n",
    "\n",
    "for i in range(len(embeddings)):\n",
    "    for emb in embeddings[i]:\n",
    "        embedding.append(emb)\n",
    "embedding = np.asarray(embedding)\n",
    "embedding.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './dataset'\n",
    "df = pd.read_pickle(data_path + '/text2ImgData.pkl')\n",
    "num_training_sample = len(df)\n",
    "n_images_train = num_training_sample\n",
    "print('There are %d image in training data' % (n_images_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation\n",
    "\n",
    "Reference: https://mmuratarat.github.io/2019-02-28/data-augmentation-in-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this competition, you have to generate image in size 64x64x3\n",
    "IMAGE_HEIGHT = 64\n",
    "IMAGE_WIDTH = 64\n",
    "IMAGE_CHANNEL = 3\n",
    "MAPPING_FUNC = None\n",
    "\n",
    "def vertical_flip(tf_img):\n",
    "    return tf.image.flip_left_right(tf_img)\n",
    "\n",
    "def horizontal_flip(tf_img):\n",
    "    return tf.image.flip_up_down(tf_img)\n",
    "\n",
    "def brightness(tf_img):\n",
    "    return tf.image.random_brightness(tf_img, 0.2, 2)\n",
    "\n",
    "def data_generator(caption, image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.cast(img, tf.float32)\n",
    "    img = (img - 127.5) / 127.5\n",
    "    # img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # img = tf.image.per_image_standardization(img)\n",
    "    img.set_shape([None, None, 3])\n",
    "    \n",
    "    if MAPPING_FUNC is not None:\n",
    "        img = MAPPING_FUNC(img)\n",
    "\n",
    "    img = tf.image.resize(img, size=[IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "    img.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL])\n",
    "    caption = tf.cast(caption, tf.int32)    \n",
    "    return img, caption\n",
    "\n",
    "def transform_dataset(caption, image_path):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((caption, image_path))\n",
    "    dataset = dataset.map(data_generator, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.shuffle(len(caption))\n",
    "    return dataset\n",
    "\n",
    "def dataset_generator(filenames, batch_size):\n",
    "    # load the training data into two NumPy arrays\n",
    "    df = pd.read_pickle(filenames)\n",
    "    captions = df['Captions'].values\n",
    "    caption = []\n",
    "    # each image has 1 to 10 corresponding captions\n",
    "    # we choose one of them randomly for training\n",
    "    for i in range(len(captions)):\n",
    "        caption.append(random.choice(captions[i]))\n",
    "    caption = np.asarray(caption)\n",
    "    caption = caption.astype(np.int64)\n",
    "    image_path = df['ImagePath'].values\n",
    "    \n",
    "    # assume that each row of features corresponds to the same row as labels.\n",
    "    assert caption.shape[0] == image_path.shape[0]\n",
    "    \n",
    "    original_dataset = transform_dataset(caption, image_path)\n",
    "\n",
    "    MAPPING_FUNC = vertical_flip\n",
    "    vertical_dataset = transform_dataset(caption, image_path)\n",
    "\n",
    "    MAPPING_FUNC = horizontal_flip\n",
    "    horizontal_dataset = transform_dataset(caption, image_path)\n",
    "\n",
    "    MAPPING_FUNC = brightness\n",
    "    brightness_dataset = transform_dataset(caption, image_path)\n",
    "    \n",
    "    dataset = original_dataset.concatenate(vertical_dataset)\n",
    "    dataset = dataset.concatenate(horizontal_dataset)\n",
    "    dataset = dataset.concatenate(brightness_dataset)\n",
    "    dataset = dataset.shuffle(len(caption)).batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "460"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# also can go 256\n",
    "BATCH_SIZE = 64 \n",
    "dataset = dataset_generator(BERT_EMBEDDING_FILE, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparas = {\n",
    "    'MAX_SEQ_LENGTH': 20,                     # maximum sequence length\n",
    "    'EMBED_DIM': 256,                         # word embedding dimension\n",
    "    'VOCAB_SIZE': len(word2Id_dict),          # size of dictionary of captions\n",
    "    'RNN_HIDDEN_SIZE': 128,                   # number of RNN neurons\n",
    "    'Z_DIM': 512,                             # random noise z dimension\n",
    "    'DENSE_DIM': 128,                         # number of neurons in dense layer\n",
    "    'IMAGE_SIZE': [64, 64, 3],                # render image size\n",
    "    'BATCH_SIZE': BATCH_SIZE,\n",
    "    'LR': 1e-4,\n",
    "    'LR_DECAY': 0.5,\n",
    "    'BETA_1': 0.5,\n",
    "    'N_EPOCH': 1000,\n",
    "    'N_SAMPLE': len(dataset) * BATCH_SIZE,          # size of training data\n",
    "    'CHECKPOINTS_DIR': './checkpoints/demo',  # checkpoint path\n",
    "    'PRINT_FREQ': 1                          # printing frequency of loss\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Model\n",
    "* DCGAN\n",
    "* Add some BN layers\n",
    "* Loss function: wgan-gp\n",
    "* Train by critic (5D+1G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Encode text (a caption) into hidden representation\n",
    "    input: text, which is a list of ids\n",
    "    output: embedding, or hidden representation of input text in dimension of RNN_HIDDEN_SIZE\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        self.batch_size = self.hparas['BATCH_SIZE']\n",
    "        \n",
    "        # embedding with tensorflow API\n",
    "        self.embedding = layers.Embedding(self.hparas['VOCAB_SIZE'], self.hparas['EMBED_DIM'])\n",
    "        # RNN, here we use GRU cell, another common RNN cell similar to LSTM\n",
    "        self.gru = layers.GRU(self.hparas['RNN_HIDDEN_SIZE'],\n",
    "                              return_sequences=True,\n",
    "                              return_state=True,\n",
    "                              recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, text, hidden):\n",
    "        text = self.embedding(text)\n",
    "        output, state = self.gru(text, initial_state = hidden)\n",
    "        return output[:, -1, :], state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.hparas['BATCH_SIZE'], self.hparas['RNN_HIDDEN_SIZE']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Generate fake image based on given text(hidden representation) and noise z\n",
    "    input: text and noise\n",
    "    output: fake image with size 64*64*3\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas):\n",
    "        super(Generator, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.d1 = tf.keras.layers.Dense(self.hparas['DENSE_DIM'])\n",
    "        self.d2 = tf.keras.layers.Dense(64*64*3)\n",
    "\n",
    "        self.d3 = layers.Dense(16*16*256, use_bias=False)\n",
    "        self.bn1= layers.BatchNormalization()\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.bn3 = layers.BatchNormalization()\n",
    "\n",
    "        self.Conv2DTrans1 = layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding=\"same\", use_bias=False)\n",
    "        self.Conv2DTrans2 = layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding=\"same\", use_bias=False)\n",
    "        self.Conv2DTrans3 = layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding=\"same\", use_bias=False)\n",
    "   \n",
    "    def call(self, text, noise_z):\n",
    "        text = self.flatten(text)\n",
    "        text = self.d1(text)\n",
    "        text = tf.nn.leaky_relu(text)\n",
    "        \n",
    "        # concatenate input text and random noise\",\n",
    "        text_concat = tf.concat([noise_z, text], axis=1)\n",
    "        text_concat = self.d3(text_concat)\n",
    "        text_concat = self.bn1(text_concat)\n",
    "        text_concat = tf.reshape(text_concat, [-1, 16, 16, 256])\n",
    "        \n",
    "        text_concat = self.Conv2DTrans1(text_concat)\n",
    "        text_concat = self.bn2(text_concat)\n",
    "        text_concat = tf.nn.leaky_relu(text_concat)\n",
    "\n",
    "        text_concat = self.Conv2DTrans2(text_concat)\n",
    "        text_concat = self.bn3(text_concat)\n",
    "        text_concat = tf.nn.leaky_relu(text_concat)\n",
    "\n",
    "        text_concat = self.Conv2DTrans3(text_concat)\n",
    "\n",
    "        logits = tf.reshape(text_concat, [-1, 64, 64, 3])\n",
    "        output = tf.nn.tanh(logits)\n",
    "\n",
    "        return logits, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Differentiate the real and fake image\n",
    "    input: image and corresponding text\n",
    "    output: labels, the real image should be 1, while the fake should be 0\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.d_text = tf.keras.layers.Dense(self.hparas['DENSE_DIM'])\n",
    "        self.d_img = tf.keras.layers.Dense(self.hparas['DENSE_DIM'])\n",
    "        self.d = tf.keras.layers.Dense(1)\n",
    "\n",
    "       \n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.bn3 = layers.BatchNormalization()\n",
    "        self.dropout = layers.Dropout(0.3)\n",
    "\n",
    "        self.Conv2D1 = layers.Conv2D(64, (5, 5), strides=(2, 2), padding=\"same\")\n",
    "        self.Conv2D2 = layers.Conv2D(128, (5, 5), strides=(2, 2), padding=\"same\")\n",
    "\n",
    "    def call(self, img, text):\n",
    "        text = self.flatten(text)\n",
    "        text = self.d_text(text)\n",
    "        text = self.bn1(text)\n",
    "        text = tf.nn.leaky_relu(text)\n",
    "\n",
    "        img = self.Conv2D1(img)\n",
    "        img = self.bn2(img)\n",
    "        img = tf.nn.leaky_relu(img)\n",
    "        img = self.dropout(img)\n",
    "\n",
    "        img = self.Conv2D2(img)\n",
    "        img = self.bn3(img)\n",
    "        img = tf.nn.leaky_relu(img)\n",
    "        img = self.dropout(img)\n",
    "        img = self.flatten(img)\n",
    "        img = self.d_img(img)\n",
    "        \n",
    "        # concatenate image with paired text\n",
    "        img_text = tf.concat([text, img], axis=1)\n",
    "        logits = self.d(img_text)\n",
    "        output = tf.nn.sigmoid(logits)\n",
    "        \n",
    "        return logits, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using BERT\n",
    "generator = Generator(hparas)\n",
    "discriminator = Discriminator(hparas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_logits, fake_logits):\n",
    "    # output value of real image should be 1\n",
    "    real_loss = cross_entropy(tf.ones_like(real_logits), real_logits)\n",
    "    # output value of fake image should be 0\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_logits), fake_logits)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    # output value of fake image should be 0\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use seperated optimizers for training generator and discriminator\n",
    "generator_optimizer = tf.keras.optimizers.Adam(hparas['LR'])\n",
    "discriminator_optimizer = tf.keras.optimizers.SGD(hparas['LR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume training from epoch 378\n"
     ]
    }
   ],
   "source": [
    "# one benefit of tf.train.Checkpoint() API is we can save everything seperately\n",
    "checkpoint_dir = hparas['CHECKPOINTS_DIR']\n",
    "checkpoint_name = 'ckpt'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)\n",
    "\n",
    "last_ckp = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "# print(last_ckp)\n",
    "if last_ckp:\n",
    "    init_epoch = int(last_ckp.split(\"-\")[-1])+1\n",
    "    print(f'Resume training from epoch {init_epoch-1}')\n",
    "    checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator,\n",
    "                                 epoch=tf.Variable(init_epoch-1))\n",
    "    checkpoint.restore(last_ckp)\n",
    "else:\n",
    "    init_epoch=1\n",
    "    checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator,\n",
    "                                 epoch=tf.Variable(init_epoch-1))\n",
    "\n",
    "manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=10,\n",
    "                                     checkpoint_name=checkpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def DC_DTrain(image, embedding, hidden, noise):\n",
    "    # z = tf.random.normal(hparas['BZ'])\n",
    "    noise_g = tf.random.normal(shape=[hparas['BATCH_SIZE'], hparas['Z_DIM']], mean=0.0, stddev=1.0)\n",
    "\n",
    "    with tf.GradientTape() as tp:\n",
    "        with tf.GradientTape() as tp_gp:\n",
    "            text_embed = embedding\n",
    "            # text_embed, hidden = text_encoder(caption, hidden)\n",
    "            # _, fake_image = generator(text_embed, noise)\n",
    "            # real_logits, real_output = discriminator(image, text_embed)\n",
    "            # fake_logits, fake_output = discriminator(fake_image, text_embed)\n",
    "\n",
    "            _, x_bar = generator(text_embed, noise_g, training = True)\n",
    "            epsilon = tf.random.uniform([BATCH_SIZE,1,1,1])\n",
    "            x = image\n",
    "            x_hat = epsilon * x + (1. - epsilon) * x_bar\n",
    "\n",
    "            x_bar = x_bar + noise * tf.random.normal(x_bar.shape)\n",
    "            x = x + noise * tf.random.normal(x.shape)\n",
    "            x_hat = x_hat + noise * tf.random.normal(x_hat.shape)\n",
    "\n",
    "            z0, _ = discriminator(x_bar, text_embed, training = True)\n",
    "            z1, _ = discriminator(x, text_embed, training = True)\n",
    "            z2, _ = discriminator(x_hat, text_embed, training = True)\n",
    "\n",
    "            gradient_penalty = tp_gp.gradient(z2,x_hat)\n",
    "            gradient_penalty = tf.sqrt(tf.reduce_sum(tf.math.square(gradient_penalty),axis=[1,2,3]))\n",
    "            loss = z0 - z1 + 10. * tf.math.square((gradient_penalty - 1.))\n",
    "            ld = tf.reduce_mean(loss)\n",
    "            lg = - tf.reduce_mean(z0)\n",
    "\n",
    "    gradient_d = tp.gradient(ld, discriminator.trainable_variables)\n",
    "\n",
    "    discriminator_optimizer.apply_gradients(zip(gradient_d, discriminator.trainable_variables))\n",
    "\n",
    "    return lg, ld\n",
    "\n",
    "@tf.function\n",
    "def DC_GTrain(image, embedding, hidden, noise):\n",
    "    # z = tf.random.normal(hparas['BZ'])\n",
    "    noise_g = tf.random.normal(shape=[hparas['BATCH_SIZE'], hparas['Z_DIM']], mean=0.0, stddev=1.0)\n",
    "\n",
    "    with tf.GradientTape() as tp:\n",
    "        with tf.GradientTape() as tp_gp:\n",
    "            text_embed = embedding\n",
    "            # text_embed, hidden = text_encoder(caption, hidden)\n",
    "            # _, fake_image = generator(text_embed, noise)\n",
    "            # real_logits, real_output = discriminator(image, text_embed)\n",
    "            # fake_logits, fake_output = discriminator(fake_image, text_embed)\n",
    "\n",
    "            _, x_bar = generator(text_embed, noise_g, training = True)\n",
    "            epsilon = tf.random.uniform([BATCH_SIZE,1,1,1])\n",
    "            x = image\n",
    "            x_hat = epsilon * x + (1. - epsilon) * x_bar\n",
    "\n",
    "            x_bar = x_bar + noise * tf.random.normal(x_bar.shape)\n",
    "            x = x + noise * tf.random.normal(x.shape)\n",
    "            x_hat = x_hat + noise * tf.random.normal(x_hat.shape)\n",
    "\n",
    "            z0, _ = discriminator(x_bar, text_embed, training = True)\n",
    "            z1, _ = discriminator(x, text_embed, training = True)\n",
    "            z2, _ = discriminator(x_hat, text_embed, training = True)\n",
    "\n",
    "            gradient_penalty = tp_gp.gradient(z2,x_hat)\n",
    "            gradient_penalty = tf.sqrt(tf.reduce_sum(tf.math.square(gradient_penalty),axis=[1,2,3]))\n",
    "            loss = z0 - z1 + 10. * tf.math.square((gradient_penalty - 1.))\n",
    "            ld = tf.reduce_mean(loss)\n",
    "            lg = - tf.reduce_mean(z0)\n",
    "\n",
    "    gradient_g = tp.gradient(lg, generator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradient_g, generator.trainable_variables))\n",
    "\n",
    "    return lg, ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT version\n",
    "@tf.function\n",
    "def test_step(embedding, noise, hidden):\n",
    "    # text_embed, hidden = text_encoder(caption, hidden)\n",
    "    _, fake_image = generator(embedding, noise)\n",
    "    return fake_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "DC_Train = (\n",
    "    DC_DTrain,\n",
    "    DC_DTrain,\n",
    "    DC_DTrain,\n",
    "    DC_DTrain,\n",
    "    DC_DTrain,\n",
    "    DC_GTrain,\n",
    ")\n",
    "\n",
    "DC_Critic = len(DC_Train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visulization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1], 3))\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w, :] = image\n",
    "    return img\n",
    "\n",
    "def imsave(images, size, path):\n",
    "    # getting the pixel values between [0, 1] to save it\n",
    "    return plt.imsave(path, merge(images, size)*0.5 + 0.5)\n",
    "\n",
    "def save_images(images, size, image_path):\n",
    "    return imsave(images, size, image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_generator(caption, batch_size):\n",
    "    caption = np.asarray(caption)\n",
    "    caption = caption.astype(np.int64)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(caption)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_sample_generator(embedding, batch_size):\n",
    "    embedding = np.asarray(embedding)\n",
    "    embedding = embedding.astype(np.float32)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(embedding)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ni = int(np.ceil(np.sqrt(hparas['BATCH_SIZE'])))\n",
    "sample_size = hparas['BATCH_SIZE']\n",
    "sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, hparas['Z_DIM'])).astype(np.float32)\n",
    "sample_sentence = [\"the flower shown has yellow anther red pistil and bright red petals.\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower has petals that are yellow, white and purple and has dark lines\"] * int(sample_size/ni) + \\\n",
    "                  [\"the petals on this flower are white with a yellow center\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower has a lot of small round pink petals.\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower is orange in color, and has petals that are ruffled and rounded.\"] * int(sample_size/ni) + \\\n",
    "                  [\"the flower has yellow petals and the center of it is brown.\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower has petals that are blue and white.\"] * int(sample_size/ni) +\\\n",
    "                  [\"these white flowers have petals that start off white in color and end in a white towards the tips.\"] * int(sample_size/ni)\n",
    "\n",
    "bert_sample_sentence *= 2\n",
    "for i, sent in enumerate(bert_sample_sentence):\n",
    "    bert_sample_sentence[i] = turn_to_bert_embedding(sent)\n",
    "bert_sample_sentence = bert_sample_generator(bert_sample_sentence, hparas['BATCH_SIZE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SAVE_PATH = './samples/bert_fullcaption_fullaug'\n",
    "if not os.path.exists(SAMPLE_SAVE_PATH):\n",
    "    os.makedirs(SAMPLE_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    # hidden state of RNN\n",
    "    # hidden = text_encoder.initialize_hidden_state()\n",
    "    hidden = None\n",
    "    steps_per_epoch = int(hparas['N_SAMPLE']/hparas['BATCH_SIZE'])\n",
    "    \n",
    "    ctr = 0\n",
    "    for epoch in range(init_epoch, epochs):\n",
    "        g_total_loss = 0\n",
    "        d_total_loss = 0\n",
    "        start = time.time()\n",
    "        \n",
    "        if epoch < 200:\n",
    "            noise = 1.0 / float(epoch + 1)\n",
    "        else:\n",
    "            noise = 0.0\n",
    "        \n",
    "        for image, embedding in dataset:\n",
    "            lg, ld = DC_Train[ctr](image, embedding, hidden, noise)\n",
    "            ctr += 1\n",
    "            g_total_loss += lg.numpy()\n",
    "            d_total_loss += ld.numpy()\n",
    "            if ctr == DC_Critic : ctr = 0\n",
    "            \n",
    "        time_tuple = time.localtime()\n",
    "        time_string = time.strftime(\"%m/%d/%Y, %H:%M:%S\", time_tuple)\n",
    "            \n",
    "        print(\"Epoch {}, gen_loss: {:.4f}, disc_loss: {:.4f}\".format(epoch+1,\n",
    "                                                                     g_total_loss/steps_per_epoch,\n",
    "                                                                     d_total_loss/steps_per_epoch))\n",
    "        print('Time for epoch {} is {:.4f} sec'.format(epoch+1, time.time()-start))\n",
    "        \n",
    "        # save the model\n",
    "        save_path = manager.save()\n",
    "        print(\"Saved checkpoint for epoch {}: {}\".format(epoch, save_path))\n",
    "        \n",
    "        # visualization\n",
    "        if (epoch + 1) % hparas['PRINT_FREQ'] == 0:\n",
    "            for embedding in bert_sample_sentence:\n",
    "                fake_image = test_step(embedding, sample_seed, hidden)\n",
    "            save_images(fake_image, [ni, ni], f'{SAMPLE_SAVE_PATH}/train_{epoch:02d}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(dataset, hparas['N_EPOCH'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Dataset\n",
    "If you change anything during preprocessing of training dataset, you must make sure same operations have be done in testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_data_generator(embedding, index):\n",
    "    embedding = tf.cast(embedding, tf.float32)\n",
    "    return embedding, index\n",
    "\n",
    "def testing_dataset_generator(data, batch_size, data_generator):\n",
    "    embeddings = data['embeddings'].values\n",
    "    embedding = []\n",
    "    for i in range(len(embeddings)):\n",
    "        embedding.append(embeddings[i])\n",
    "    embedding = np.asarray(embedding)\n",
    "    embedding = embedding.astype(np.float32)\n",
    "\n",
    "    index = data['ID'].values\n",
    "    index = np.asarray(index)\n",
    "    \n",
    "    assert embedding.shape[0] == index.shape[0]\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((embedding, index))\n",
    "    dataset = dataset.map(data_generator, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.repeat().batch(batch_size)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx2word_test(indices_list):\n",
    "    indices_list = [indices_list]\n",
    "    results_list = []\n",
    "    for indices in indices_list:\n",
    "        string = ''\n",
    "        length_of_string = 0\n",
    "        for idx in indices:\n",
    "            if idx == '5428':\n",
    "                string = string + ''\n",
    "            elif idx == '5427':\n",
    "                break\n",
    "            else:\n",
    "                string = string + id2word_dict[idx] + ' '\n",
    "        results_list.append(string.strip())\n",
    "    return results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('./dataset/testData.pkl')\n",
    "data['texts'] = data['Captions'].apply(lambda x: idx2word_test(x))\n",
    "data['texts'] = data['texts'].apply(lambda x: remove_empty_string(x))\n",
    "data['embeddings'] = data['texts'].apply(lambda x : turn_to_bert_embedding(x))\n",
    "captions = data['Captions'].values\n",
    "\n",
    "NUM_TEST = len(captions)\n",
    "EPOCH_TEST = int(NUM_TEST / hparas['BATCH_SIZE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataset = testing_dataset_generator(hparas['BATCH_SIZE'], testing_data_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(dataset):\n",
    "    # hidden = text_encoder.initialize_hidden_state()\n",
    "    hidden = None\n",
    "    sample_size = hparas['BATCH_SIZE']\n",
    "    sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, hparas['Z_DIM'])).astype(np.float32)\n",
    "    \n",
    "    step = 0\n",
    "    start = time.time()\n",
    "    for embedding, idx in dataset:\n",
    "        if step > EPOCH_TEST:\n",
    "            break\n",
    "        sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, hparas['Z_DIM'])).astype(np.float32)\n",
    "        fake_image  = test_step(embedding, sample_seed, hidden)\n",
    "        sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, hparas['Z_DIM'])).astype(np.float32)\n",
    "        fake_image1 = test_step(embedding, sample_seed, hidden)\n",
    "        sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, hparas['Z_DIM'])).astype(np.float32)\n",
    "        fake_image2 = test_step(embedding, sample_seed, hidden)\n",
    "        sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, hparas['Z_DIM'])).astype(np.float32)\n",
    "        fake_image3 = test_step(embedding, sample_seed, hidden)\n",
    "        sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, hparas['Z_DIM'])).astype(np.float32)\n",
    "        fake_image4 = test_step(embedding, sample_seed, hidden)\n",
    "        step += 1\n",
    "        for i in range(hparas['BATCH_SIZE']):\n",
    "            plt.imsave('./inference/demo/inference_{:04d}.jpg'.format(idx[i]),   fake_image[i].numpy()*0.5 + 0.5)\n",
    "            plt.imsave('./inference/our_inference/inference_{:04d}_1.jpg'.format(idx[i]), fake_image[i].numpy()*0.5 + 0.5)\n",
    "            plt.imsave('./inference/our_inference/inference_{:04d}_2.jpg'.format(idx[i]), fake_image1[i].numpy()*0.5 + 0.5)\n",
    "            plt.imsave('./inference/our_inference/inference_{:04d}_3.jpg'.format(idx[i]), fake_image2[i].numpy()*0.5 + 0.5)\n",
    "            plt.imsave('./inference/our_inference/inference_{:04d}_4.jpg'.format(idx[i]), fake_image3[i].numpy()*0.5 + 0.5)\n",
    "            plt.imsave('./inference/our_inference/inference_{:04d}_5.jpg'.format(idx[i]), fake_image4[i].numpy()*0.5 + 0.5)\n",
    "            \n",
    "    print('Time for inference is {:.4f} sec'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for inference is 31.3629 sec\n"
     ]
    }
   ],
   "source": [
    "inference(testing_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output score.csv\n",
    "CAUTION: \n",
    "* Please modify GPU setting in <i>inception_score.py</i> if need.\n",
    "* Please run the below cmd in command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd testing\n",
    "!python inception_score.py ../inference/demo ../score_demo.csv 39 # BATCH_SIZE=39 is available using GTX 1080 Ti (need 9441MB memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(idx):\n",
    "    fig = plt.figure(figsize=(14,14))\n",
    "    \n",
    "    for count, i in enumerate(idx):\n",
    "        loc = np.where(i==index)[0][0]\n",
    "        text = ''\n",
    "        for word in captions[loc]:\n",
    "            if id2word_dict[word] != '<PAD>':\n",
    "                text += id2word_dict[word]\n",
    "                text += ' '\n",
    "        print(text)\n",
    "        for j in range(len(idx)):\n",
    "            path  = './inference/our_inference/inference_{:04d}_{:d}.jpg'.format(i,j+1)\n",
    "            fake_image = plt.imread(path)\n",
    "\n",
    "            plt.subplot(5, 5, count*5+j+1)\n",
    "            plt.imshow(fake_image)\n",
    "            \n",
    "            plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 5427 vocabularies in total\n",
      "Word to id mapping, for example: flower -> 1\n",
      "Id to word mapping, for example: 1 -> flower\n",
      "Tokens: <PAD>: 5427; <RARE>: 5428\n"
     ]
    }
   ],
   "source": [
    "dictionary_path = './dictionary'\n",
    "vocab = np.load(dictionary_path + '/vocab.npy')\n",
    "print('there are {} vocabularies in total'.format(len(vocab)))\n",
    "\n",
    "word2Id_dict = dict(np.load(dictionary_path + '/word2Id.npy'))\n",
    "id2word_dict = dict(np.load(dictionary_path + '/id2Word.npy'))\n",
    "print('Word to id mapping, for example: %s -> %s' % ('flower', word2Id_dict['flower']))\n",
    "print('Id to word mapping, for example: %s -> %s' % ('1', id2word_dict['1']))\n",
    "print('Tokens: <PAD>: %s; <RARE>: %s' % (word2Id_dict['<PAD>'], word2Id_dict['<RARE>']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_pickle('./dataset/testData.pkl')\n",
    "captions = data['Captions'].values\n",
    "index = data['ID'].values\n",
    "random_idx = [253, 267, 413, 841, 3462]\n",
    "\n",
    "visualize(random_idx)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5c5b210ffa015f2312f69f2248e3163602cc860559c1494ea467ed1fecf0f25e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
