{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLab Cup 3: Reverse Image Caption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow the tutorial in this page:\n",
    "# https://www.endtoend.ai/tutorial/how-to-download-kaggle-datasets-on-ubuntu/\n",
    "!cat ~/.kaggle/kaggle.json\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "!kaggle competitions download -c datalab-cup3-reverse-image-caption-2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "# zipfile example\n",
    "def zip_list(file_path):\n",
    "    zf = zipfile.ZipFile(file_path, 'r')\n",
    "    zf.extractall('./')\n",
    "\n",
    "file_path = './datalab-cup3-reverse-image-caption-2021.zip'\n",
    "zip_list(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and Packages Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # disable warnings, info and errors \n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the first GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        #for gpu in gpus:\n",
    "        #    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Dec 31 19:27:44 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 495.29.05    Driver Version: 495.29.05    CUDA Version: 11.5     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0 Off |                  N/A |\n",
      "| 27%   52C    P2    46W / 250W |    247MiB /  8119MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  On   | 00000000:02:00.0 Off |                  N/A |\n",
      "| 44%   54C    P2    59W / 250W |   4643MiB /  8119MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1224      G   /usr/lib/xorg/Xorg                  9MiB |\n",
      "|    0   N/A  N/A      1423      G   /usr/bin/gnome-shell                2MiB |\n",
      "|    0   N/A  N/A   1286825      C   /usr/bin/python3                  231MiB |\n",
      "|    1   N/A  N/A      1224      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    1   N/A  N/A   1283788      C   /bin/python3                     4635MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocseeing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 5427 vocabularies in total\n",
      "Word to id mapping, for example: flower -> 1\n",
      "Id to word mapping, for example: 1 -> flower\n",
      "Tokens: <PAD>: 5427; <RARE>: 5428\n"
     ]
    }
   ],
   "source": [
    "dictionary_path = './dictionary'\n",
    "vocab = np.load(dictionary_path + '/vocab.npy')\n",
    "print('there are {} vocabularies in total'.format(len(vocab)))\n",
    "\n",
    "word2Id_dict = dict(np.load(dictionary_path + '/word2Id.npy'))\n",
    "id2word_dict = dict(np.load(dictionary_path + '/id2Word.npy'))\n",
    "print('Word to id mapping, for example: %s -> %s' % ('flower', word2Id_dict['flower']))\n",
    "print('Id to word mapping, for example: %s -> %s' % ('1', id2word_dict['1']))\n",
    "print('Tokens: <PAD>: %s; <RARE>: %s' % (word2Id_dict['<PAD>'], word2Id_dict['<RARE>']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the flower shown has yellow anther red pistil and bright red petals.\n",
      "['9', '1', '82', '5', '11', '70', '20', '31', '3', '29', '20', '2', '5427', '5427', '5427', '5427', '5427', '5427', '5427', '5427']\n"
     ]
    }
   ],
   "source": [
    "def sent2IdList(line, MAX_SEQ_LENGTH=20):\n",
    "    MAX_SEQ_LIMIT = MAX_SEQ_LENGTH\n",
    "    padding = 0\n",
    "    \n",
    "    # data preprocessing, remove all puntuation in the texts\n",
    "    prep_line = re.sub('[%s]' % re.escape(string.punctuation), ' ', line.rstrip())\n",
    "    prep_line = prep_line.replace('-', ' ')\n",
    "    prep_line = prep_line.replace('-', ' ')\n",
    "    prep_line = prep_line.replace('  ', ' ')\n",
    "    prep_line = prep_line.replace('.', '')\n",
    "    tokens = prep_line.split(' ')\n",
    "    tokens = [\n",
    "        tokens[i] for i in range(len(tokens))\n",
    "        if tokens[i] != ' ' and tokens[i] != ''\n",
    "    ]\n",
    "    l = len(tokens)\n",
    "    padding = MAX_SEQ_LIMIT - l\n",
    "    \n",
    "    # make sure length of each text is equal to MAX_SEQ_LENGTH, and replace the less common word with <RARE> token\n",
    "    for i in range(padding):\n",
    "        tokens.append('<PAD>')\n",
    "    line = [\n",
    "        word2Id_dict[tokens[k]]\n",
    "        if tokens[k] in word2Id_dict else word2Id_dict['<RARE>']\n",
    "        for k in range(len(tokens))\n",
    "    ]\n",
    "\n",
    "    return line\n",
    "\n",
    "text = \"the flower shown has yellow anther red pistil and bright red petals.\"\n",
    "print(text)\n",
    "print(sent2IdList(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx2word(indices_list):\n",
    "    results_list = []\n",
    "    for indices in indices_list:\n",
    "        string = ''\n",
    "        length_of_string = 0\n",
    "        for idx in indices:\n",
    "            if idx == '5428':\n",
    "                string = string + ''\n",
    "            elif idx == '5427':\n",
    "                break\n",
    "            else:\n",
    "                string = string + id2word_dict[idx] + ' '\n",
    "        results_list.append(string.strip())\n",
    "    return results_list\n",
    "\n",
    "def remove_empty_string(string_list):\n",
    "    empty_flag = False\n",
    "    for string in string_list:\n",
    "        if string == '':\n",
    "            empty_flag = True\n",
    "            break\n",
    "    if empty_flag == False:\n",
    "        return string_list\n",
    "    else:\n",
    "        new_string_list = []\n",
    "        for string in string_list:\n",
    "            if string != '':\n",
    "                new_string_list.append(string)\n",
    "        return new_string_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-large-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-large-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\n",
    "    'bert-large-uncased', \n",
    "    do_lower_case=False,\n",
    "    do_basic_tokenize=False\n",
    ")\n",
    "bert_model = TFBertModel.from_pretrained('bert-large-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "def turn_to_bert_embedding(string_list):\n",
    "    try:\n",
    "        bert_inputs = bert_tokenizer(string_list, return_tensors=\"tf\", padding='max_length',max_length=30)\n",
    "        bert_outputs = bert_model(bert_inputs)\n",
    "        caption_embedding = bert_outputs.last_hidden_state[:,0]\n",
    "    except(ValueError):\n",
    "        print(string_list)\n",
    "    return caption_embedding.numpy().tolist()\n",
    "\n",
    "test_string = ['this flower is white and pink in color with petals that have small veins',\n",
    "               'the flower shown has a purple and white petal with white anther', \n",
    "               'the four heart shaped pink petals of this flower are striped with fuchsia and their centers are yellow and white']  \n",
    "print(len(turn_to_bert_embedding(test_string)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_EMBEDDING_FILE = './dataset/bert_embedding.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7370 image in training data\n"
     ]
    }
   ],
   "source": [
    "data_path = './dataset'\n",
    "df = pd.read_pickle(data_path + '/text2ImgData.pkl')\n",
    "num_training_sample = len(df)\n",
    "n_images_train = num_training_sample\n",
    "print('There are %d image in training data' % (n_images_train))\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df['texts'] = df['Captions'].apply(lambda x: idx2word(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Captions</th>\n",
       "      <th>ImagePath</th>\n",
       "      <th>texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...</td>\n",
       "      <td>./102flowers/image_06734.jpg</td>\n",
       "      <td>[the petals of the flower are pink in color an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...</td>\n",
       "      <td>./102flowers/image_06736.jpg</td>\n",
       "      <td>[this flower has white petals and yellow pisti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...</td>\n",
       "      <td>./102flowers/image_06737.jpg</td>\n",
       "      <td>[the petals on this flower are pink with white...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...</td>\n",
       "      <td>./102flowers/image_06738.jpg</td>\n",
       "      <td>[the flower has a smooth purple petal with whi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...</td>\n",
       "      <td>./102flowers/image_06739.jpg</td>\n",
       "      <td>[this white flower has bright yellow stamen wi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Captions  \\\n",
       "0  [[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...   \n",
       "1  [[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...   \n",
       "2  [[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...   \n",
       "3  [[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...   \n",
       "4  [[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...   \n",
       "\n",
       "                      ImagePath  \\\n",
       "0  ./102flowers/image_06734.jpg   \n",
       "1  ./102flowers/image_06736.jpg   \n",
       "2  ./102flowers/image_06737.jpg   \n",
       "3  ./102flowers/image_06738.jpg   \n",
       "4  ./102flowers/image_06739.jpg   \n",
       "\n",
       "                                               texts  \n",
       "0  [the petals of the flower are pink in color an...  \n",
       "1  [this flower has white petals and yellow pisti...  \n",
       "2  [the petals on this flower are pink with white...  \n",
       "3  [the flower has a smooth purple petal with whi...  \n",
       "4  [this white flower has bright yellow stamen wi...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the petals of the flower are pink in color and have a yellow center',\n",
       " 'this flower is pink and white in color with petals that are multi colored',\n",
       " 'the purple petals have shades of white with white anther and filament',\n",
       " 'this flower has large pink petals and a white stigma in the center',\n",
       " 'this flower has petals that are pink and has a yellow stamen',\n",
       " 'a flower with short and wide petals that is light purple',\n",
       " 'this flower has small pink petals with a yellow center',\n",
       " 'this flower has large rounded pink petals with curved edges and purple veins',\n",
       " 'this flower has purple petals as well as a white stamen']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0,'texts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['texts'] = df['texts'].apply(lambda x: remove_empty_string(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Captions</th>\n",
       "      <th>ImagePath</th>\n",
       "      <th>texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...</td>\n",
       "      <td>./102flowers/image_06734.jpg</td>\n",
       "      <td>[the petals of the flower are pink in color an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...</td>\n",
       "      <td>./102flowers/image_06736.jpg</td>\n",
       "      <td>[this flower has white petals and yellow pisti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...</td>\n",
       "      <td>./102flowers/image_06737.jpg</td>\n",
       "      <td>[the petals on this flower are pink with white...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...</td>\n",
       "      <td>./102flowers/image_06738.jpg</td>\n",
       "      <td>[the flower has a smooth purple petal with whi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...</td>\n",
       "      <td>./102flowers/image_06739.jpg</td>\n",
       "      <td>[this white flower has bright yellow stamen wi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Captions  \\\n",
       "0  [[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...   \n",
       "1  [[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...   \n",
       "2  [[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...   \n",
       "3  [[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...   \n",
       "4  [[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...   \n",
       "\n",
       "                      ImagePath  \\\n",
       "0  ./102flowers/image_06734.jpg   \n",
       "1  ./102flowers/image_06736.jpg   \n",
       "2  ./102flowers/image_06737.jpg   \n",
       "3  ./102flowers/image_06738.jpg   \n",
       "4  ./102flowers/image_06739.jpg   \n",
       "\n",
       "                                               texts  \n",
       "0  [the petals of the flower are pink in color an...  \n",
       "1  [this flower has white petals and yellow pisti...  \n",
       "2  [the petals on this flower are pink with white...  \n",
       "3  [the flower has a smooth purple petal with whi...  \n",
       "4  [this white flower has bright yellow stamen wi...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_caption_num(string_list):\n",
    "    return len(string_list)\n",
    "\n",
    "df['caption_num'] = df['texts'].apply(lambda c: count_caption_num(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Captions</th>\n",
       "      <th>ImagePath</th>\n",
       "      <th>texts</th>\n",
       "      <th>caption_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...</td>\n",
       "      <td>./102flowers/image_06734.jpg</td>\n",
       "      <td>[the petals of the flower are pink in color an...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...</td>\n",
       "      <td>./102flowers/image_06736.jpg</td>\n",
       "      <td>[this flower has white petals and yellow pisti...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...</td>\n",
       "      <td>./102flowers/image_06737.jpg</td>\n",
       "      <td>[the petals on this flower are pink with white...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...</td>\n",
       "      <td>./102flowers/image_06738.jpg</td>\n",
       "      <td>[the flower has a smooth purple petal with whi...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...</td>\n",
       "      <td>./102flowers/image_06739.jpg</td>\n",
       "      <td>[this white flower has bright yellow stamen wi...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Captions  \\\n",
       "0  [[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...   \n",
       "1  [[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...   \n",
       "2  [[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...   \n",
       "3  [[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...   \n",
       "4  [[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...   \n",
       "\n",
       "                      ImagePath  \\\n",
       "0  ./102flowers/image_06734.jpg   \n",
       "1  ./102flowers/image_06736.jpg   \n",
       "2  ./102flowers/image_06737.jpg   \n",
       "3  ./102flowers/image_06738.jpg   \n",
       "4  ./102flowers/image_06739.jpg   \n",
       "\n",
       "                                               texts  caption_num  \n",
       "0  [the petals of the flower are pink in color an...            9  \n",
       "1  [this flower has white petals and yellow pisti...           10  \n",
       "2  [the petals on this flower are pink with white...            9  \n",
       "3  [the flower has a smooth purple petal with whi...           10  \n",
       "4  [this white flower has bright yellow stamen wi...            9  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{9: 1959, 10: 4837, 8: 488, 7: 75, 6: 10, 5: 1}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_dict = {}\n",
    "for num in df['caption_num'].tolist():\n",
    "    if num in num_dict:\n",
    "        num_dict[num]+=1\n",
    "    else:\n",
    "        num_dict[num]=1\n",
    "num_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['embeddings'] = df['texts'].apply(lambda x : turn_to_bert_embedding(x))\n",
    "len(df.loc[0,'embeddings'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(BERT_EMBEDDING_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70495"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No need to run this cell (run inside dataset_generator())\n",
    "df_bert = pd.read_pickle(BERT_EMBEDDING_FILE)\n",
    "\n",
    "embeddings = df_bert['embeddings'].values\n",
    "embedding = []\n",
    "\n",
    "for i in range(len(embeddings)):\n",
    "    for emb in embeddings[i]:\n",
    "        embedding.append(emb)\n",
    "embedding = np.asarray(embedding)\n",
    "embedding.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Captions</th>\n",
       "      <th>ImagePath</th>\n",
       "      <th>texts</th>\n",
       "      <th>caption_num</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...</td>\n",
       "      <td>./102flowers/image_06734.jpg</td>\n",
       "      <td>[the petals of the flower are pink in color an...</td>\n",
       "      <td>9</td>\n",
       "      <td>[[-0.6344168782234192, -0.8039702773094177, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...</td>\n",
       "      <td>./102flowers/image_06736.jpg</td>\n",
       "      <td>[this flower has white petals and yellow pisti...</td>\n",
       "      <td>10</td>\n",
       "      <td>[[-0.2546939551830292, -0.029211539775133133, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...</td>\n",
       "      <td>./102flowers/image_06737.jpg</td>\n",
       "      <td>[the petals on this flower are pink with white...</td>\n",
       "      <td>9</td>\n",
       "      <td>[[-0.15689069032669067, -0.4372050166130066, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...</td>\n",
       "      <td>./102flowers/image_06738.jpg</td>\n",
       "      <td>[the flower has a smooth purple petal with whi...</td>\n",
       "      <td>10</td>\n",
       "      <td>[[-0.07732152938842773, -0.5917125940322876, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...</td>\n",
       "      <td>./102flowers/image_06739.jpg</td>\n",
       "      <td>[this white flower has bright yellow stamen wi...</td>\n",
       "      <td>9</td>\n",
       "      <td>[[-0.3246757686138153, -0.187540203332901, -0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Captions  \\\n",
       "0  [[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...   \n",
       "1  [[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...   \n",
       "2  [[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...   \n",
       "3  [[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...   \n",
       "4  [[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...   \n",
       "\n",
       "                      ImagePath  \\\n",
       "0  ./102flowers/image_06734.jpg   \n",
       "1  ./102flowers/image_06736.jpg   \n",
       "2  ./102flowers/image_06737.jpg   \n",
       "3  ./102flowers/image_06738.jpg   \n",
       "4  ./102flowers/image_06739.jpg   \n",
       "\n",
       "                                               texts  caption_num  \\\n",
       "0  [the petals of the flower are pink in color an...            9   \n",
       "1  [this flower has white petals and yellow pisti...           10   \n",
       "2  [the petals on this flower are pink with white...            9   \n",
       "3  [the flower has a smooth purple petal with whi...           10   \n",
       "4  [this white flower has bright yellow stamen wi...            9   \n",
       "\n",
       "                                          embeddings  \n",
       "0  [[-0.6344168782234192, -0.8039702773094177, 0....  \n",
       "1  [[-0.2546939551830292, -0.029211539775133133, ...  \n",
       "2  [[-0.15689069032669067, -0.4372050166130066, -...  \n",
       "3  [[-0.07732152938842773, -0.5917125940322876, -...  \n",
       "4  [[-0.3246757686138153, -0.187540203332901, -0....  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bert.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7370 image in training data\n"
     ]
    }
   ],
   "source": [
    "data_path = './dataset'\n",
    "df = pd.read_pickle(data_path + '/text2ImgData.pkl')\n",
    "num_training_sample = len(df)\n",
    "n_images_train = num_training_sample\n",
    "print('There are %d image in training data' % (n_images_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Captions</th>\n",
       "      <th>ImagePath</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6734</th>\n",
       "      <td>[[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...</td>\n",
       "      <td>./102flowers/image_06734.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6736</th>\n",
       "      <td>[[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...</td>\n",
       "      <td>./102flowers/image_06736.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6737</th>\n",
       "      <td>[[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...</td>\n",
       "      <td>./102flowers/image_06737.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6738</th>\n",
       "      <td>[[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...</td>\n",
       "      <td>./102flowers/image_06738.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6739</th>\n",
       "      <td>[[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...</td>\n",
       "      <td>./102flowers/image_06739.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Captions  \\\n",
       "ID                                                        \n",
       "6734  [[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...   \n",
       "6736  [[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...   \n",
       "6737  [[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...   \n",
       "6738  [[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...   \n",
       "6739  [[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...   \n",
       "\n",
       "                         ImagePath  \n",
       "ID                                  \n",
       "6734  ./102flowers/image_06734.jpg  \n",
       "6736  ./102flowers/image_06736.jpg  \n",
       "6737  ./102flowers/image_06737.jpg  \n",
       "6738  ./102flowers/image_06738.jpg  \n",
       "6739  ./102flowers/image_06739.jpg  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "Reference: https://mmuratarat.github.io/2019-02-28/data-augmentation-in-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this competition, you have to generate image in size 64x64x3\n",
    "IMAGE_HEIGHT = 64\n",
    "IMAGE_WIDTH = 64\n",
    "IMAGE_CHANNEL = 3\n",
    "MAPPING_FUNC = None\n",
    "\n",
    "def vertical_flip(tf_img):\n",
    "    return tf.image.flip_left_right(tf_img)\n",
    "\n",
    "def horizontal_flip(tf_img):\n",
    "    return tf.image.flip_up_down(tf_img)\n",
    "\n",
    "def brightness(tf_img):\n",
    "    return tf.image.random_brightness(tf_img, delta=0.4, seed=2)\n",
    "\n",
    "def central_crop(tf_img):\n",
    "    return tf.image.central_crop(tf_img, 0.8)\n",
    "\n",
    "def noise_injection(tf_img):\n",
    "    noise = tf.random.normal(shape=tf.shape(tf_img), mean=0.0, stddev=1, dtype=tf.float32)\n",
    "    return tf.add(tf_img, noise)\n",
    "\n",
    "def data_generator(caption, image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.cast(img, tf.float32)\n",
    "    img = (img - 127.5) / 127.5\n",
    "    img.set_shape([None, None, 3])\n",
    "    \n",
    "    if MAPPING_FUNC is not None:\n",
    "        img = MAPPING_FUNC(img)\n",
    "\n",
    "    img = tf.image.resize(img, size=[IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "    img.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL])\n",
    "    caption = tf.cast(caption, tf.float32)    \n",
    "    return img, caption\n",
    "\n",
    "def dataset_interface(caption, image_path):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((caption, image_path))\n",
    "    dataset = dataset.map(data_generator, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.shuffle(len(caption))\n",
    "    return dataset\n",
    "\n",
    "def dataset_generator(filenames, batch_size):\n",
    "    \n",
    "    df_bert = pd.read_pickle(filenames)\n",
    "\n",
    "    embeddings = df_bert['embeddings'].values\n",
    "    imagePaths = df_bert['ImagePath'].values\n",
    "    embedding = []\n",
    "    image_path = []\n",
    "\n",
    "    for i in range(len(embeddings)):\n",
    "        embedding.append(random.choice(embeddings[i]))\n",
    "        image_path.append(imagePaths[i])\n",
    "        # for j in range(len(embeddings[i])):\n",
    "        #     embedding.append(embeddings[i][j])\n",
    "        #     image_path.append(imagePaths[i])\n",
    "    embedding = np.asarray(embedding)\n",
    "    embedding = embedding.astype(np.float32)\n",
    "    caption = embedding\n",
    "    image_path = np.asarray(image_path)\n",
    "    #------------------------------------------    \n",
    "    \n",
    "    # assume that each row of `features` corresponds to the same row as `labels`.\n",
    "    assert caption.shape[0] == image_path.shape[0]\n",
    "    \n",
    "    original_dataset = dataset_interface(caption, image_path)\n",
    "    # dataset = original_dataset #----------------\n",
    "\n",
    "    MAPPING_FUNC = vertical_flip\n",
    "    vertical_dataset = dataset_interface(caption, image_path)\n",
    "\n",
    "    MAPPING_FUNC = horizontal_flip\n",
    "    horizontal_dataset = dataset_interface(caption, image_path)\n",
    "\n",
    "    MAPPING_FUNC = brightness\n",
    "    brightness_dataset = dataset_interface(caption, image_path)\n",
    "    \n",
    "    dataset = original_dataset.concatenate(vertical_dataset)\n",
    "    dataset = dataset.concatenate(horizontal_dataset)\n",
    "    dataset = dataset.concatenate(brightness_dataset)\n",
    "    dataset = dataset.shuffle(len(caption)).batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "# dataset = dataset_generator(data_path + '/text2ImgData.pkl', BATCH_SIZE)\n",
    "dataset = dataset_generator(BERT_EMBEDDING_FILE, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "460"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparas = {\n",
    "    'MAX_SEQ_LENGTH': 20,                     # maximum sequence length\n",
    "    'EMBED_DIM': 256,                         # word embedding dimension\n",
    "    'VOCAB_SIZE': len(word2Id_dict),          # size of dictionary of captions\n",
    "    'RNN_HIDDEN_SIZE': 128,                   # number of RNN neurons\n",
    "    'Z_DIM': 512,                             # random noise z dimension\n",
    "    'DENSE_DIM': 128,                         # number of neurons in dense layer\n",
    "    'IMAGE_SIZE': [64, 64, 3],                # render image size\n",
    "    'BATCH_SIZE': BATCH_SIZE,\n",
    "    'LR': 1e-4,\n",
    "    'LR_DECAY': 0.5,\n",
    "    'BETA_1': 0.5,\n",
    "    'N_EPOCH': 1000,\n",
    "    'N_SAMPLE': len(dataset) * BATCH_SIZE,          # size of training data\n",
    "    'CHECKPOINTS_DIR': './checkpoints/demo',  # checkpoint path\n",
    "    'PRINT_FREQ': 1                          # printing frequency of loss\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Model\n",
    "* DCGAN\n",
    "* Add some BN layers\n",
    "* Loss function: wgan-gp\n",
    "* Train by critic (5D+1G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Encode text (a caption) into hidden representation\n",
    "    input: text, which is a list of ids\n",
    "    output: embedding, or hidden representation of input text in dimension of RNN_HIDDEN_SIZE\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        self.batch_size = self.hparas['BATCH_SIZE']\n",
    "        \n",
    "        # embedding with tensorflow API\n",
    "        self.embedding = layers.Embedding(self.hparas['VOCAB_SIZE'], self.hparas['EMBED_DIM'])\n",
    "        # RNN, here we use GRU cell, another common RNN cell similar to LSTM\n",
    "        self.gru = layers.GRU(self.hparas['RNN_HIDDEN_SIZE'],\n",
    "                              return_sequences=True,\n",
    "                              return_state=True,\n",
    "                              recurrent_initializer='glorot_uniform')\n",
    "\n",
    "        # self.bidirect = layers.Bidirectional(layers.LSTM(128))\n",
    "        # self.d1 = layers.Dense(128, activation=\"relu\")\n",
    "    \n",
    "    def call(self, text, hidden):\n",
    "        text = self.embedding(text)\n",
    "        output, state = self.gru(text, initial_state = hidden)\n",
    "        # output = self.bidirect(text)\n",
    "        # output = self.d1(output)\n",
    "        return output[:, -1, :], state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.hparas['BATCH_SIZE'], self.hparas['RNN_HIDDEN_SIZE']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Generate fake image based on given text(hidden representation) and noise z\n",
    "    input: text and noise\n",
    "    output: fake image with size 64*64*3\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas):\n",
    "        super(Generator, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.d1 = tf.keras.layers.Dense(self.hparas['DENSE_DIM'])\n",
    "        self.d2 = tf.keras.layers.Dense(64*64*3)\n",
    "\n",
    "        self.d3 = layers.Dense(8*8*512, use_bias=False)\n",
    "        self.bn1= layers.BatchNormalization()\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.bn3 = layers.BatchNormalization()\n",
    "        self.bn4 = layers.BatchNormalization()\n",
    "        self.dropout = layers.Dropout(0.5)\n",
    "\n",
    "        self.Conv2DTrans1 = layers.Conv2DTranspose(256, (5, 5), strides=(1, 1), padding=\"same\", use_bias=False)\n",
    "        self.Conv2DTrans2 = layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding=\"same\", use_bias=False)\n",
    "        self.Conv2DTrans3 = layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding=\"same\", use_bias=False)\n",
    "        self.Conv2DTrans4 = layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding=\"same\", use_bias=False)\n",
    "        \n",
    "    def call(self, text, noise_z):\n",
    "        text = self.flatten(text)\n",
    "        text = self.d1(text)\n",
    "        text = tf.nn.leaky_relu(text)\n",
    "        \n",
    "        # concatenate input text and random noise\n",
    "        text_concat = tf.concat([noise_z, text], axis=1)\n",
    "        text_concat = self.d3(text_concat)\n",
    "        text_concat = self.bn1(text_concat)\n",
    "        text_concat = tf.reshape(text_concat, [-1, 8, 8, 512])\n",
    "        \n",
    "        text_concat = self.Conv2DTrans1(text_concat)\n",
    "        text_concat = self.bn2(text_concat)\n",
    "        text_concat = tf.nn.leaky_relu(text_concat)\n",
    "        text_concat = self.dropout(text_concat)\n",
    "\n",
    "        text_concat = self.Conv2DTrans2(text_concat)\n",
    "        text_concat = self.bn3(text_concat)\n",
    "        text_concat = tf.nn.leaky_relu(text_concat)\n",
    "        text_concat = self.dropout(text_concat)\n",
    "\n",
    "        text_concat = self.Conv2DTrans3(text_concat)\n",
    "        text_concat = self.bn4(text_concat)\n",
    "        text_concat = tf.nn.leaky_relu(text_concat)\n",
    "        text_concat = self.dropout(text_concat)\n",
    "\n",
    "        text_concat = self.Conv2DTrans4(text_concat)\n",
    "\n",
    "        logits = tf.reshape(text_concat, [-1, 64, 64, 3])\n",
    "        output = tf.nn.tanh(logits)\n",
    "\n",
    "        return logits, output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Differentiate the real and fake image\n",
    "    input: image and corresponding text\n",
    "    output: labels, the real image should be 1, while the fake should be 0\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.d_text = tf.keras.layers.Dense(self.hparas['DENSE_DIM'])\n",
    "        self.d_img = tf.keras.layers.Dense(self.hparas['DENSE_DIM'])\n",
    "        self.d = tf.keras.layers.Dense(1)\n",
    "\n",
    "        self.Conv2D1 = layers.Conv2D(64, (5, 5), strides=(2, 2), padding=\"same\")\n",
    "        self.bn = layers.BatchNormalization()\n",
    "        self.dropout = layers.Dropout(0.3)\n",
    "\n",
    "        self.Conv2D2 = layers.Conv2D(128, (5, 5), strides=(2, 2), padding=\"same\")\n",
    "        self.Conv2D3 = layers.Conv2D(256, (5, 5), strides=(2, 2), padding=\"same\")\n",
    "\n",
    "    def call(self, img, text):\n",
    "        text = self.flatten(text)\n",
    "        text = self.d_text(text)\n",
    "        text = tf.nn.leaky_relu(text)\n",
    "        \n",
    "        img = self.Conv2D1(img)\n",
    "        img = tf.nn.leaky_relu(img)\n",
    "        img = self.dropout(img)\n",
    "\n",
    "        img = self.Conv2D2(img)\n",
    "        img = tf.nn.leaky_relu(img)\n",
    "        img = self.dropout(img)\n",
    "\n",
    "        img = self.Conv2D3(img)\n",
    "        img = tf.nn.leaky_relu(img)\n",
    "        img = self.dropout(img)\n",
    "\n",
    "        img = self.flatten(img)\n",
    "        img = self.d_img(img)\n",
    "        \n",
    "        # concatenate image with paired text\n",
    "        img_text = tf.concat([text, img], axis=1)\n",
    "        \n",
    "        logits = self.d(img_text)\n",
    "        output = tf.nn.sigmoid(logits)\n",
    "        \n",
    "        return logits, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_encoder = TextEncoder(hparas)\n",
    "generator = Generator(hparas)\n",
    "discriminator = Discriminator(hparas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_logits, fake_logits):\n",
    "    # output value of real image should be 1\n",
    "    real_loss = cross_entropy(tf.ones_like(real_logits), real_logits)\n",
    "    # output value of fake image should be 0\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_logits), fake_logits)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    # output value of fake image should be 0\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use seperated optimizers for training generator and discriminator\n",
    "generator_optimizer = tf.keras.optimizers.Adam(hparas['LR'])\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(hparas['LR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume training from epoch 599\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).discriminator_optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).discriminator_optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).discriminator_optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).discriminator_optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).discriminator_optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator_optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator_optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator_optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator_optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator_optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).discriminator.d_text.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).discriminator.d_text.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).discriminator.d_img.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).discriminator.d_img.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).discriminator.d.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).discriminator.d.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).discriminator.Conv2D1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).discriminator.Conv2D1.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).discriminator.Conv2D2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).discriminator.Conv2D2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).discriminator.Conv2D3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).discriminator.Conv2D3.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator.d1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator.d1.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator.d3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator.bn1.axis\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator.bn1.gamma\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator.bn1.beta\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator.bn1.moving_mean\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator.bn1.moving_variance\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator.bn2.axis\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator.bn2.gamma\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator.bn2.beta\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator.bn2.moving_mean\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator.bn2.moving_variance\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator.bn3.axis\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator.bn3.gamma\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator.bn3.beta\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator.bn3.moving_mean\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator.bn3.moving_variance\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator.bn4.axis\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator.bn4.gamma\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator.bn4.beta\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator.bn4.moving_mean\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator.bn4.moving_variance\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator.Conv2DTrans1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator.Conv2DTrans2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator.Conv2DTrans3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator.Conv2DTrans4.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).discriminator_optimizer's state 'm' for (root).discriminator.d_text.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).discriminator_optimizer's state 'm' for (root).discriminator.d_text.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).discriminator_optimizer's state 'm' for (root).discriminator.d_img.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).discriminator_optimizer's state 'm' for (root).discriminator.d_img.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).discriminator_optimizer's state 'm' for (root).discriminator.d.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).discriminator_optimizer's state 'm' for (root).discriminator.d.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).discriminator_optimizer's state 'm' for (root).discriminator.Conv2D1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).discriminator_optimizer's state 'm' for (root).discriminator.Conv2D1.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).discriminator_optimizer's state 'm' for (root).discriminator.Conv2D2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).discriminator_optimizer's state 'm' for (root).discriminator.Conv2D2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).discriminator_optimizer's state 'm' for (root).discriminator.Conv2D3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).discriminator_optimizer's state 'm' for (root).discriminator.Conv2D3.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).discriminator_optimizer's state 'v' for (root).discriminator.d_text.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).discriminator_optimizer's state 'v' for (root).discriminator.d_text.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).discriminator_optimizer's state 'v' for (root).discriminator.d_img.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).discriminator_optimizer's state 'v' for (root).discriminator.d_img.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).discriminator_optimizer's state 'v' for (root).discriminator.d.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).discriminator_optimizer's state 'v' for (root).discriminator.d.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).discriminator_optimizer's state 'v' for (root).discriminator.Conv2D1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).discriminator_optimizer's state 'v' for (root).discriminator.Conv2D1.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).discriminator_optimizer's state 'v' for (root).discriminator.Conv2D2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).discriminator_optimizer's state 'v' for (root).discriminator.Conv2D2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).discriminator_optimizer's state 'v' for (root).discriminator.Conv2D3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).discriminator_optimizer's state 'v' for (root).discriminator.Conv2D3.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator_optimizer's state 'm' for (root).generator.d1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator_optimizer's state 'm' for (root).generator.d1.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator_optimizer's state 'm' for (root).generator.d3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator_optimizer's state 'm' for (root).generator.bn1.gamma\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator_optimizer's state 'm' for (root).generator.bn1.beta\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator_optimizer's state 'm' for (root).generator.bn2.gamma\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator_optimizer's state 'm' for (root).generator.bn2.beta\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator_optimizer's state 'm' for (root).generator.bn3.gamma\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator_optimizer's state 'm' for (root).generator.bn3.beta\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator_optimizer's state 'm' for (root).generator.bn4.gamma\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator_optimizer's state 'm' for (root).generator.bn4.beta\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator_optimizer's state 'm' for (root).generator.Conv2DTrans1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator_optimizer's state 'm' for (root).generator.Conv2DTrans2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator_optimizer's state 'm' for (root).generator.Conv2DTrans3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator_optimizer's state 'm' for (root).generator.Conv2DTrans4.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator_optimizer's state 'v' for (root).generator.d1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator_optimizer's state 'v' for (root).generator.d1.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator_optimizer's state 'v' for (root).generator.d3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator_optimizer's state 'v' for (root).generator.bn1.gamma\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator_optimizer's state 'v' for (root).generator.bn1.beta\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator_optimizer's state 'v' for (root).generator.bn2.gamma\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator_optimizer's state 'v' for (root).generator.bn2.beta\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator_optimizer's state 'v' for (root).generator.bn3.gamma\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator_optimizer's state 'v' for (root).generator.bn3.beta\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator_optimizer's state 'v' for (root).generator.bn4.gamma\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator_optimizer's state 'v' for (root).generator.bn4.beta\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator_optimizer's state 'v' for (root).generator.Conv2DTrans1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator_optimizer's state 'v' for (root).generator.Conv2DTrans2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator_optimizer's state 'v' for (root).generator.Conv2DTrans3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).generator_optimizer's state 'v' for (root).generator.Conv2DTrans4.kernel\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    }
   ],
   "source": [
    "# one benefit of tf.train.Checkpoint() API is we can save everything seperately\n",
    "checkpoint_dir = hparas['CHECKPOINTS_DIR']\n",
    "checkpoint_name = 'comp3'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)\n",
    "\n",
    "last_ckp = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "if last_ckp:\n",
    "    init_epoch = int(last_ckp.split(\"-\")[-1])+1\n",
    "    print(f'Resume training from epoch {init_epoch-1}')\n",
    "    checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator,\n",
    "                                 epoch=tf.Variable(init_epoch-1))\n",
    "    checkpoint.restore(last_ckp)\n",
    "    \n",
    "else:\n",
    "    init_epoch=1\n",
    "    checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator,\n",
    "                                 epoch=tf.Variable(init_epoch-1))\n",
    "\n",
    "manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=10,\n",
    "                                     checkpoint_name=checkpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT version here!!!!!!!!!!!\n",
    "\n",
    "@tf.function\n",
    "def DC_DTrain(image, embedding, hidden, noise):\n",
    "    # z = tf.random.normal(hparas['BZ'])\n",
    "    noise_g = tf.random.normal(shape=[hparas['BATCH_SIZE'], hparas['Z_DIM']], mean=0.0, stddev=1.0)\n",
    "\n",
    "    with tf.GradientTape() as tp:\n",
    "        with tf.GradientTape() as tp_gp:\n",
    "            text_embed = embedding\n",
    "            # text_embed, hidden = text_encoder(caption, hidden)\n",
    "            # _, fake_image = generator(text_embed, noise)\n",
    "            # real_logits, real_output = discriminator(image, text_embed)\n",
    "            # fake_logits, fake_output = discriminator(fake_image, text_embed)\n",
    "\n",
    "            _, x_bar = generator(text_embed, noise_g, training = True)\n",
    "            epsilon = tf.random.uniform([BATCH_SIZE,1,1,1])\n",
    "            x = image\n",
    "            x_hat = epsilon * x + (1. - epsilon) * x_bar\n",
    "\n",
    "            x_bar = x_bar + noise * tf.random.normal(x_bar.shape)\n",
    "            x = x + noise * tf.random.normal(x.shape)\n",
    "            x_hat = x_hat + noise * tf.random.normal(x_hat.shape)\n",
    "\n",
    "            z0, _ = discriminator(x_bar, text_embed, training = True)\n",
    "            z1, _ = discriminator(x, text_embed, training = True)\n",
    "            z2, _ = discriminator(x_hat, text_embed, training = True)\n",
    "\n",
    "            gradient_penalty = tp_gp.gradient(z2,x_hat)\n",
    "            gradient_penalty = tf.sqrt(tf.reduce_sum(tf.math.square(gradient_penalty),axis=[1,2,3]))\n",
    "            loss = z0 - z1 + 10. * tf.math.square((gradient_penalty - 1.))\n",
    "            ld = tf.reduce_mean(loss)\n",
    "            lg = - tf.reduce_mean(z0)\n",
    "\n",
    "    gradient_d = tp.gradient(ld, discriminator.trainable_variables)\n",
    "\n",
    "    discriminator_optimizer.apply_gradients(zip(gradient_d, discriminator.trainable_variables))\n",
    "\n",
    "    return lg, ld\n",
    "\n",
    "@tf.function\n",
    "def DC_GTrain(image, embedding, hidden, noise):\n",
    "    # z = tf.random.normal(hparas['BZ'])\n",
    "    noise_g = tf.random.normal(shape=[hparas['BATCH_SIZE'], hparas['Z_DIM']], mean=0.0, stddev=1.0)\n",
    "\n",
    "    with tf.GradientTape() as tp:\n",
    "        with tf.GradientTape() as tp_gp:\n",
    "            text_embed = embedding\n",
    "            # text_embed, hidden = text_encoder(caption, hidden)\n",
    "            # _, fake_image = generator(text_embed, noise)\n",
    "            # real_logits, real_output = discriminator(image, text_embed)\n",
    "            # fake_logits, fake_output = discriminator(fake_image, text_embed)\n",
    "\n",
    "            _, x_bar = generator(text_embed, noise_g, training = True)\n",
    "            epsilon = tf.random.uniform([BATCH_SIZE,1,1,1])\n",
    "            x = image\n",
    "            x_hat = epsilon * x + (1. - epsilon) * x_bar\n",
    "\n",
    "            x_bar = x_bar + noise * tf.random.normal(x_bar.shape)\n",
    "            x = x + noise * tf.random.normal(x.shape)\n",
    "            x_hat = x_hat + noise * tf.random.normal(x_hat.shape)\n",
    "\n",
    "            z0, _ = discriminator(x_bar, text_embed, training = True)\n",
    "            z1, _ = discriminator(x, text_embed, training = True)\n",
    "            z2, _ = discriminator(x_hat, text_embed, training = True)\n",
    "\n",
    "            gradient_penalty = tp_gp.gradient(z2,x_hat)\n",
    "            gradient_penalty = tf.sqrt(tf.reduce_sum(tf.math.square(gradient_penalty),axis=[1,2,3]))\n",
    "            loss = z0 - z1 + 10. * tf.math.square((gradient_penalty - 1.))\n",
    "            ld = tf.reduce_mean(loss)\n",
    "            lg = - tf.reduce_mean(z0)\n",
    "\n",
    "    gradient_g = tp.gradient(lg, generator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradient_g, generator.trainable_variables))\n",
    "\n",
    "    return lg, ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(embedding, noise, hidden):\n",
    "    # text_embed, hidden = text_encoder(caption, hidden)\n",
    "    _, fake_image = generator(embedding, noise)\n",
    "    return fake_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "DC_Train = (\n",
    "    DC_DTrain,\n",
    "    DC_DTrain,\n",
    "    DC_DTrain,\n",
    "    DC_DTrain,\n",
    "    DC_DTrain,\n",
    "    DC_GTrain,\n",
    ")\n",
    "\n",
    "DC_Critic = len(DC_Train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visulization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1], 3))\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w, :] = image\n",
    "    return img\n",
    "\n",
    "def imsave(images, size, path):\n",
    "    # getting the pixel values between [0, 1] to save it\n",
    "    return plt.imsave(path, merge(images, size)*0.5 + 0.5)\n",
    "\n",
    "def save_images(images, size, image_path):\n",
    "    return imsave(images, size, image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_generator(caption, batch_size):\n",
    "    caption = np.asarray(caption)\n",
    "    caption = caption.astype(np.int64)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(caption)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_sample_generator(embedding, batch_size):\n",
    "    embedding = np.asarray(embedding)\n",
    "    embedding = embedding.astype(np.float32)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(embedding)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "ni = int(np.ceil(np.sqrt(hparas['BATCH_SIZE'])))\n",
    "sample_size = hparas['BATCH_SIZE']\n",
    "sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, hparas['Z_DIM'])).astype(np.float32)\n",
    "bert_sample_sentence = [\"the flower shown has yellow anther red pistil and bright red petals.\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower has petals that are yellow, white and purple and has dark lines\"] * int(sample_size/ni) + \\\n",
    "                  [\"the petals on this flower are white with a yellow center\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower has a lot of small round pink petals.\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower is orange in color, and has petals that are ruffled and rounded.\"] * int(sample_size/ni) + \\\n",
    "                  [\"the flower has yellow petals and the center of it is brown.\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower has petals that are blue and white.\"] * int(sample_size/ni) +\\\n",
    "                  [\"these white flowers have petals that start off white in color and end in a white towards the tips.\"] * int(sample_size/ni)\n",
    "bert_sample_sentence *= 2\n",
    "for i, sent in enumerate(bert_sample_sentence):\n",
    "    bert_sample_sentence[i] = turn_to_bert_embedding(sent)\n",
    "bert_sample_sentence = bert_sample_generator(bert_sample_sentence, hparas['BATCH_SIZE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint.restore(checkpoint_dir + '/ckpt-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SAVE_PATH = './samples/bert_1caption_fullAug'\n",
    "if not os.path.exists(SAMPLE_SAVE_PATH):\n",
    "    os.makedirs(SAMPLE_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    # hidden state of RNN\n",
    "    # hidden = text_encoder.initialize_hidden_state()\n",
    "    hidden = None\n",
    "    steps_per_epoch = int(hparas['N_SAMPLE']/hparas['BATCH_SIZE'])\n",
    "    \n",
    "    ctr = 0\n",
    "    for epoch in range(init_epoch, epochs):\n",
    "        g_total_loss = 0\n",
    "        d_total_loss = 0\n",
    "        start = time.time()\n",
    "        \n",
    "        if epoch < 200:\n",
    "            noise = 1.0 / float(epoch + 1)\n",
    "        else:\n",
    "            noise = 0.0\n",
    "        \n",
    "        for image, embedding in dataset:\n",
    "            lg, ld = DC_Train[ctr](image, embedding, hidden, noise)\n",
    "            ctr += 1\n",
    "            g_total_loss += lg.numpy()\n",
    "            d_total_loss += ld.numpy()\n",
    "            if ctr == DC_Critic : ctr = 0\n",
    "            \n",
    "        time_tuple = time.localtime()\n",
    "        time_string = time.strftime(\"%m/%d/%Y, %H:%M:%S\", time_tuple)\n",
    "            \n",
    "        print(\"Epoch {}, gen_loss: {:.4f}, disc_loss: {:.4f}\".format(epoch+1,\n",
    "                                                                     g_total_loss/steps_per_epoch,\n",
    "                                                                     d_total_loss/steps_per_epoch))\n",
    "        print('Time for epoch {} is {:.4f} sec'.format(epoch+1, time.time()-start))\n",
    "        \n",
    "        # save the model\n",
    "        # if (epoch + 1) % 50 == 0:\n",
    "        # checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "        save_path = manager.save()\n",
    "        print(\"Saved checkpoint for epoch {}: {}\".format(epoch, save_path))\n",
    "        \n",
    "        # visualization\n",
    "        if (epoch + 1) % hparas['PRINT_FREQ'] == 0:\n",
    "            # for caption in sample_sentence:\n",
    "            #     fake_image = test_step(caption, sample_seed, hidden)\n",
    "            for embedding in bert_sample_sentence:\n",
    "                fake_image = test_step(embedding, sample_seed, hidden)\n",
    "            save_images(fake_image, [ni, ni], f'{SAMPLE_SAVE_PATH}/train_{epoch:02d}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 601, gen_loss: 18.9409, disc_loss: -2.9178\n",
      "Time for epoch 601 is 62.4446 sec\n",
      "Saved checkpoint for epoch 600: ./checkpoints/demo/comp3-600\n",
      "Epoch 602, gen_loss: 7.1714, disc_loss: -2.8414\n",
      "Time for epoch 602 is 65.5344 sec\n",
      "Saved checkpoint for epoch 601: ./checkpoints/demo/comp3-601\n",
      "Epoch 603, gen_loss: 13.4235, disc_loss: -3.0148\n",
      "Time for epoch 603 is 62.3431 sec\n",
      "Saved checkpoint for epoch 602: ./checkpoints/demo/comp3-602\n",
      "Epoch 604, gen_loss: 12.3242, disc_loss: -3.1756\n",
      "Time for epoch 604 is 62.0300 sec\n",
      "Saved checkpoint for epoch 603: ./checkpoints/demo/comp3-603\n",
      "Epoch 605, gen_loss: 19.3226, disc_loss: -2.9327\n",
      "Time for epoch 605 is 68.5274 sec\n",
      "Saved checkpoint for epoch 604: ./checkpoints/demo/comp3-604\n",
      "Epoch 606, gen_loss: 6.6535, disc_loss: -3.3199\n",
      "Time for epoch 606 is 75.0668 sec\n",
      "Saved checkpoint for epoch 605: ./checkpoints/demo/comp3-605\n",
      "Epoch 607, gen_loss: 8.6937, disc_loss: -2.7810\n",
      "Time for epoch 607 is 61.2524 sec\n",
      "Saved checkpoint for epoch 606: ./checkpoints/demo/comp3-606\n",
      "Epoch 608, gen_loss: 10.6591, disc_loss: -2.7664\n",
      "Time for epoch 608 is 66.4962 sec\n",
      "Saved checkpoint for epoch 607: ./checkpoints/demo/comp3-607\n",
      "Epoch 609, gen_loss: 18.8677, disc_loss: -3.0856\n",
      "Time for epoch 609 is 63.8228 sec\n",
      "Saved checkpoint for epoch 608: ./checkpoints/demo/comp3-608\n",
      "Epoch 610, gen_loss: 9.7179, disc_loss: -2.9125\n",
      "Time for epoch 610 is 76.3988 sec\n",
      "Saved checkpoint for epoch 609: ./checkpoints/demo/comp3-609\n",
      "Epoch 611, gen_loss: 5.1006, disc_loss: -2.9738\n",
      "Time for epoch 611 is 65.1851 sec\n",
      "Saved checkpoint for epoch 610: ./checkpoints/demo/comp3-610\n",
      "Epoch 612, gen_loss: 19.9349, disc_loss: -2.9674\n",
      "Time for epoch 612 is 65.3466 sec\n",
      "Saved checkpoint for epoch 611: ./checkpoints/demo/comp3-611\n",
      "Epoch 613, gen_loss: 9.9866, disc_loss: -2.8705\n",
      "Time for epoch 613 is 67.9681 sec\n",
      "Saved checkpoint for epoch 612: ./checkpoints/demo/comp3-612\n",
      "Epoch 614, gen_loss: 5.8911, disc_loss: -3.0196\n",
      "Time for epoch 614 is 62.5892 sec\n",
      "Saved checkpoint for epoch 613: ./checkpoints/demo/comp3-613\n",
      "Epoch 615, gen_loss: 17.8021, disc_loss: -3.0771\n",
      "Time for epoch 615 is 60.6973 sec\n",
      "Saved checkpoint for epoch 614: ./checkpoints/demo/comp3-614\n",
      "Epoch 616, gen_loss: 9.3988, disc_loss: -2.7497\n",
      "Time for epoch 616 is 66.2333 sec\n",
      "Saved checkpoint for epoch 615: ./checkpoints/demo/comp3-615\n",
      "Epoch 617, gen_loss: 11.4734, disc_loss: -2.9044\n",
      "Time for epoch 617 is 75.0884 sec\n",
      "Saved checkpoint for epoch 616: ./checkpoints/demo/comp3-616\n",
      "Epoch 618, gen_loss: 11.8548, disc_loss: -2.9023\n",
      "Time for epoch 618 is 65.3501 sec\n",
      "Saved checkpoint for epoch 617: ./checkpoints/demo/comp3-617\n",
      "Epoch 619, gen_loss: 22.6778, disc_loss: -2.9223\n",
      "Time for epoch 619 is 64.6173 sec\n",
      "Saved checkpoint for epoch 618: ./checkpoints/demo/comp3-618\n",
      "Epoch 620, gen_loss: 23.8859, disc_loss: -3.0176\n",
      "Time for epoch 620 is 64.9740 sec\n",
      "Saved checkpoint for epoch 619: ./checkpoints/demo/comp3-619\n",
      "Epoch 621, gen_loss: 16.5729, disc_loss: -3.0664\n",
      "Time for epoch 621 is 64.5266 sec\n",
      "Saved checkpoint for epoch 620: ./checkpoints/demo/comp3-620\n",
      "Epoch 622, gen_loss: 13.2080, disc_loss: -3.1026\n",
      "Time for epoch 622 is 66.5155 sec\n",
      "Saved checkpoint for epoch 621: ./checkpoints/demo/comp3-621\n",
      "Epoch 623, gen_loss: 4.1898, disc_loss: -3.0312\n",
      "Time for epoch 623 is 66.3464 sec\n",
      "Saved checkpoint for epoch 622: ./checkpoints/demo/comp3-622\n",
      "Epoch 624, gen_loss: 21.9195, disc_loss: -3.1438\n",
      "Time for epoch 624 is 65.2155 sec\n",
      "Saved checkpoint for epoch 623: ./checkpoints/demo/comp3-623\n",
      "Epoch 625, gen_loss: -7.0216, disc_loss: -2.8899\n",
      "Time for epoch 625 is 68.1808 sec\n",
      "Saved checkpoint for epoch 624: ./checkpoints/demo/comp3-624\n",
      "Epoch 626, gen_loss: -15.6638, disc_loss: -2.8246\n",
      "Time for epoch 626 is 64.1499 sec\n",
      "Saved checkpoint for epoch 625: ./checkpoints/demo/comp3-625\n",
      "Epoch 627, gen_loss: 24.9593, disc_loss: -2.8200\n",
      "Time for epoch 627 is 59.6114 sec\n",
      "Saved checkpoint for epoch 626: ./checkpoints/demo/comp3-626\n",
      "Epoch 628, gen_loss: 14.6768, disc_loss: -3.0347\n",
      "Time for epoch 628 is 65.6648 sec\n",
      "Saved checkpoint for epoch 627: ./checkpoints/demo/comp3-627\n",
      "Epoch 629, gen_loss: 6.3095, disc_loss: -3.0312\n",
      "Time for epoch 629 is 73.6683 sec\n",
      "Saved checkpoint for epoch 628: ./checkpoints/demo/comp3-628\n",
      "Epoch 630, gen_loss: 13.6710, disc_loss: -2.9446\n",
      "Time for epoch 630 is 66.8137 sec\n",
      "Saved checkpoint for epoch 629: ./checkpoints/demo/comp3-629\n",
      "Epoch 631, gen_loss: 4.7085, disc_loss: -2.8470\n",
      "Time for epoch 631 is 63.1626 sec\n",
      "Saved checkpoint for epoch 630: ./checkpoints/demo/comp3-630\n",
      "Epoch 632, gen_loss: 2.1944, disc_loss: -2.9137\n",
      "Time for epoch 632 is 62.8907 sec\n",
      "Saved checkpoint for epoch 631: ./checkpoints/demo/comp3-631\n",
      "Epoch 633, gen_loss: 6.3873, disc_loss: -3.0323\n",
      "Time for epoch 633 is 60.7647 sec\n",
      "Saved checkpoint for epoch 632: ./checkpoints/demo/comp3-632\n",
      "Epoch 634, gen_loss: 5.3973, disc_loss: -2.9265\n",
      "Time for epoch 634 is 71.7747 sec\n",
      "Saved checkpoint for epoch 633: ./checkpoints/demo/comp3-633\n",
      "Epoch 635, gen_loss: 13.1832, disc_loss: -2.9170\n",
      "Time for epoch 635 is 70.6063 sec\n",
      "Saved checkpoint for epoch 634: ./checkpoints/demo/comp3-634\n",
      "Epoch 636, gen_loss: -0.6634, disc_loss: -2.9930\n",
      "Time for epoch 636 is 62.2772 sec\n",
      "Saved checkpoint for epoch 635: ./checkpoints/demo/comp3-635\n",
      "Epoch 637, gen_loss: 7.4476, disc_loss: -3.0084\n",
      "Time for epoch 637 is 65.3001 sec\n",
      "Saved checkpoint for epoch 636: ./checkpoints/demo/comp3-636\n",
      "Epoch 638, gen_loss: 9.9832, disc_loss: -3.1340\n",
      "Time for epoch 638 is 64.7841 sec\n",
      "Saved checkpoint for epoch 637: ./checkpoints/demo/comp3-637\n",
      "Epoch 639, gen_loss: -1.6087, disc_loss: -2.6993\n",
      "Time for epoch 639 is 66.9786 sec\n",
      "Saved checkpoint for epoch 638: ./checkpoints/demo/comp3-638\n",
      "Epoch 640, gen_loss: 12.3844, disc_loss: -2.5402\n",
      "Time for epoch 640 is 67.9786 sec\n",
      "Saved checkpoint for epoch 639: ./checkpoints/demo/comp3-639\n",
      "Epoch 641, gen_loss: 18.6931, disc_loss: -2.8726\n",
      "Time for epoch 641 is 65.2680 sec\n",
      "Saved checkpoint for epoch 640: ./checkpoints/demo/comp3-640\n",
      "Epoch 642, gen_loss: 5.3271, disc_loss: -2.8186\n",
      "Time for epoch 642 is 67.4311 sec\n",
      "Saved checkpoint for epoch 641: ./checkpoints/demo/comp3-641\n",
      "Epoch 643, gen_loss: 9.1094, disc_loss: -2.6440\n",
      "Time for epoch 643 is 65.4654 sec\n",
      "Saved checkpoint for epoch 642: ./checkpoints/demo/comp3-642\n",
      "Epoch 644, gen_loss: 0.6292, disc_loss: -2.8651\n",
      "Time for epoch 644 is 60.4102 sec\n",
      "Saved checkpoint for epoch 643: ./checkpoints/demo/comp3-643\n",
      "Epoch 645, gen_loss: 5.8009, disc_loss: -3.0994\n",
      "Time for epoch 645 is 65.5653 sec\n",
      "Saved checkpoint for epoch 644: ./checkpoints/demo/comp3-644\n",
      "Epoch 646, gen_loss: -2.8561, disc_loss: -2.9882\n",
      "Time for epoch 646 is 81.9624 sec\n",
      "Saved checkpoint for epoch 645: ./checkpoints/demo/comp3-645\n",
      "Epoch 647, gen_loss: -7.0121, disc_loss: -3.2125\n",
      "Time for epoch 647 is 66.3938 sec\n",
      "Saved checkpoint for epoch 646: ./checkpoints/demo/comp3-646\n",
      "Epoch 648, gen_loss: 5.9008, disc_loss: -3.1850\n",
      "Time for epoch 648 is 64.5454 sec\n",
      "Saved checkpoint for epoch 647: ./checkpoints/demo/comp3-647\n",
      "Epoch 649, gen_loss: 8.9074, disc_loss: -2.9711\n",
      "Time for epoch 649 is 66.1960 sec\n",
      "Saved checkpoint for epoch 648: ./checkpoints/demo/comp3-648\n",
      "Epoch 650, gen_loss: 7.4576, disc_loss: -3.0010\n",
      "Time for epoch 650 is 68.2805 sec\n",
      "Saved checkpoint for epoch 649: ./checkpoints/demo/comp3-649\n",
      "Epoch 651, gen_loss: 6.1076, disc_loss: -2.9483\n",
      "Time for epoch 651 is 65.3576 sec\n",
      "Saved checkpoint for epoch 650: ./checkpoints/demo/comp3-650\n",
      "Epoch 652, gen_loss: 3.3920, disc_loss: -2.8835\n",
      "Time for epoch 652 is 65.6069 sec\n",
      "Saved checkpoint for epoch 651: ./checkpoints/demo/comp3-651\n",
      "Epoch 653, gen_loss: 16.2846, disc_loss: -2.7525\n",
      "Time for epoch 653 is 68.3648 sec\n",
      "Saved checkpoint for epoch 652: ./checkpoints/demo/comp3-652\n",
      "Epoch 654, gen_loss: 0.2890, disc_loss: -3.0827\n",
      "Time for epoch 654 is 61.9016 sec\n",
      "Saved checkpoint for epoch 653: ./checkpoints/demo/comp3-653\n",
      "Epoch 655, gen_loss: -0.5350, disc_loss: -3.0818\n",
      "Time for epoch 655 is 62.4978 sec\n",
      "Saved checkpoint for epoch 654: ./checkpoints/demo/comp3-654\n",
      "Epoch 656, gen_loss: 3.3191, disc_loss: -2.6481\n",
      "Time for epoch 656 is 72.7446 sec\n",
      "Saved checkpoint for epoch 655: ./checkpoints/demo/comp3-655\n",
      "Epoch 657, gen_loss: 12.3912, disc_loss: -2.9905\n",
      "Time for epoch 657 is 68.6570 sec\n",
      "Saved checkpoint for epoch 656: ./checkpoints/demo/comp3-656\n",
      "Epoch 658, gen_loss: 2.4773, disc_loss: -3.1534\n",
      "Time for epoch 658 is 82.0706 sec\n",
      "Saved checkpoint for epoch 657: ./checkpoints/demo/comp3-657\n",
      "Epoch 659, gen_loss: 11.4606, disc_loss: -2.7961\n",
      "Time for epoch 659 is 67.9747 sec\n",
      "Saved checkpoint for epoch 658: ./checkpoints/demo/comp3-658\n",
      "Epoch 660, gen_loss: 9.6365, disc_loss: -2.9457\n",
      "Time for epoch 660 is 65.5983 sec\n",
      "Saved checkpoint for epoch 659: ./checkpoints/demo/comp3-659\n",
      "Epoch 661, gen_loss: 11.9668, disc_loss: -3.0104\n",
      "Time for epoch 661 is 61.4012 sec\n",
      "Saved checkpoint for epoch 660: ./checkpoints/demo/comp3-660\n",
      "Epoch 662, gen_loss: 14.2059, disc_loss: -2.9066\n",
      "Time for epoch 662 is 62.8534 sec\n",
      "Saved checkpoint for epoch 661: ./checkpoints/demo/comp3-661\n",
      "Epoch 663, gen_loss: 16.6709, disc_loss: -3.1926\n",
      "Time for epoch 663 is 73.2805 sec\n",
      "Saved checkpoint for epoch 662: ./checkpoints/demo/comp3-662\n",
      "Epoch 664, gen_loss: -1.6694, disc_loss: -2.7527\n",
      "Time for epoch 664 is 68.9883 sec\n",
      "Saved checkpoint for epoch 663: ./checkpoints/demo/comp3-663\n",
      "Epoch 665, gen_loss: 19.9455, disc_loss: -2.8447\n",
      "Time for epoch 665 is 82.0426 sec\n",
      "Saved checkpoint for epoch 664: ./checkpoints/demo/comp3-664\n",
      "Epoch 666, gen_loss: 22.8277, disc_loss: -2.7741\n",
      "Time for epoch 666 is 81.9826 sec\n",
      "Saved checkpoint for epoch 665: ./checkpoints/demo/comp3-665\n",
      "Epoch 667, gen_loss: 18.1807, disc_loss: -2.9264\n",
      "Time for epoch 667 is 66.0746 sec\n",
      "Saved checkpoint for epoch 666: ./checkpoints/demo/comp3-666\n",
      "Epoch 668, gen_loss: 7.7474, disc_loss: -2.8831\n",
      "Time for epoch 668 is 63.8019 sec\n",
      "Saved checkpoint for epoch 667: ./checkpoints/demo/comp3-667\n",
      "Epoch 669, gen_loss: 21.2476, disc_loss: -2.6517\n",
      "Time for epoch 669 is 64.7991 sec\n",
      "Saved checkpoint for epoch 668: ./checkpoints/demo/comp3-668\n",
      "Epoch 670, gen_loss: 10.2340, disc_loss: -2.7989\n",
      "Time for epoch 670 is 65.2836 sec\n",
      "Saved checkpoint for epoch 669: ./checkpoints/demo/comp3-669\n",
      "Epoch 671, gen_loss: 10.8595, disc_loss: -3.1478\n",
      "Time for epoch 671 is 66.8775 sec\n",
      "Saved checkpoint for epoch 670: ./checkpoints/demo/comp3-670\n",
      "Epoch 672, gen_loss: 1.3375, disc_loss: -2.8256\n",
      "Time for epoch 672 is 66.1754 sec\n",
      "Saved checkpoint for epoch 671: ./checkpoints/demo/comp3-671\n",
      "Epoch 673, gen_loss: -0.8861, disc_loss: -2.7688\n",
      "Time for epoch 673 is 65.3560 sec\n",
      "Saved checkpoint for epoch 672: ./checkpoints/demo/comp3-672\n",
      "Epoch 674, gen_loss: 8.8113, disc_loss: -2.9024\n",
      "Time for epoch 674 is 67.7814 sec\n",
      "Saved checkpoint for epoch 673: ./checkpoints/demo/comp3-673\n",
      "Epoch 675, gen_loss: -6.4201, disc_loss: -2.7283\n",
      "Time for epoch 675 is 64.1257 sec\n",
      "Saved checkpoint for epoch 674: ./checkpoints/demo/comp3-674\n",
      "Epoch 676, gen_loss: 10.7058, disc_loss: -2.9520\n",
      "Time for epoch 676 is 59.6393 sec\n",
      "Saved checkpoint for epoch 675: ./checkpoints/demo/comp3-675\n",
      "Epoch 677, gen_loss: 15.1014, disc_loss: -2.9179\n",
      "Time for epoch 677 is 67.1659 sec\n",
      "Saved checkpoint for epoch 676: ./checkpoints/demo/comp3-676\n",
      "Epoch 678, gen_loss: -4.8899, disc_loss: -2.9875\n",
      "Time for epoch 678 is 81.9781 sec\n",
      "Saved checkpoint for epoch 677: ./checkpoints/demo/comp3-677\n",
      "Epoch 679, gen_loss: 20.5789, disc_loss: -2.8254\n",
      "Time for epoch 679 is 65.7260 sec\n",
      "Saved checkpoint for epoch 678: ./checkpoints/demo/comp3-678\n",
      "Epoch 680, gen_loss: 7.7274, disc_loss: -2.8054\n",
      "Time for epoch 680 is 64.6872 sec\n",
      "Saved checkpoint for epoch 679: ./checkpoints/demo/comp3-679\n",
      "Epoch 681, gen_loss: 24.6007, disc_loss: -2.9215\n",
      "Time for epoch 681 is 66.4079 sec\n",
      "Saved checkpoint for epoch 680: ./checkpoints/demo/comp3-680\n",
      "Epoch 682, gen_loss: 15.9838, disc_loss: -3.0101\n",
      "Time for epoch 682 is 67.3407 sec\n",
      "Saved checkpoint for epoch 681: ./checkpoints/demo/comp3-681\n",
      "Epoch 683, gen_loss: 23.3342, disc_loss: -2.8036\n",
      "Time for epoch 683 is 66.0119 sec\n",
      "Saved checkpoint for epoch 682: ./checkpoints/demo/comp3-682\n",
      "Epoch 684, gen_loss: 1.3341, disc_loss: -2.6650\n",
      "Time for epoch 684 is 65.3466 sec\n",
      "Saved checkpoint for epoch 683: ./checkpoints/demo/comp3-683\n",
      "Epoch 685, gen_loss: -3.6440, disc_loss: -2.9704\n",
      "Time for epoch 685 is 68.1288 sec\n",
      "Saved checkpoint for epoch 684: ./checkpoints/demo/comp3-684\n",
      "Epoch 686, gen_loss: 9.8892, disc_loss: -2.8291\n",
      "Time for epoch 686 is 66.6493 sec\n",
      "Saved checkpoint for epoch 685: ./checkpoints/demo/comp3-685\n",
      "Epoch 687, gen_loss: 13.5664, disc_loss: -2.8409\n",
      "Time for epoch 687 is 62.3254 sec\n",
      "Saved checkpoint for epoch 686: ./checkpoints/demo/comp3-686\n",
      "Epoch 688, gen_loss: -1.0938, disc_loss: -3.0002\n",
      "Time for epoch 688 is 61.7875 sec\n",
      "Saved checkpoint for epoch 687: ./checkpoints/demo/comp3-687\n",
      "Epoch 689, gen_loss: 8.8039, disc_loss: -2.9055\n",
      "Time for epoch 689 is 72.0699 sec\n",
      "Saved checkpoint for epoch 688: ./checkpoints/demo/comp3-688\n",
      "Epoch 690, gen_loss: 14.6548, disc_loss: -2.9772\n",
      "Time for epoch 690 is 69.4691 sec\n",
      "Saved checkpoint for epoch 689: ./checkpoints/demo/comp3-689\n",
      "Epoch 691, gen_loss: 0.4154, disc_loss: -2.8747\n",
      "Time for epoch 691 is 62.7515 sec\n",
      "Saved checkpoint for epoch 690: ./checkpoints/demo/comp3-690\n",
      "Epoch 692, gen_loss: -1.2244, disc_loss: -2.8545\n",
      "Time for epoch 692 is 64.5396 sec\n",
      "Saved checkpoint for epoch 691: ./checkpoints/demo/comp3-691\n",
      "Epoch 693, gen_loss: 19.1112, disc_loss: -2.9155\n",
      "Time for epoch 693 is 65.8407 sec\n",
      "Saved checkpoint for epoch 692: ./checkpoints/demo/comp3-692\n",
      "Epoch 694, gen_loss: 8.9797, disc_loss: -2.6124\n",
      "Time for epoch 694 is 66.8160 sec\n",
      "Saved checkpoint for epoch 693: ./checkpoints/demo/comp3-693\n",
      "Epoch 695, gen_loss: 11.9913, disc_loss: -2.9447\n",
      "Time for epoch 695 is 66.5082 sec\n",
      "Saved checkpoint for epoch 694: ./checkpoints/demo/comp3-694\n",
      "Epoch 696, gen_loss: -7.5904, disc_loss: -2.7675\n",
      "Time for epoch 696 is 65.4769 sec\n",
      "Saved checkpoint for epoch 695: ./checkpoints/demo/comp3-695\n",
      "Epoch 697, gen_loss: 7.4697, disc_loss: -2.6442\n",
      "Time for epoch 697 is 68.2319 sec\n",
      "Saved checkpoint for epoch 696: ./checkpoints/demo/comp3-696\n",
      "Epoch 698, gen_loss: -11.3391, disc_loss: -2.8241\n",
      "Time for epoch 698 is 65.0017 sec\n",
      "Saved checkpoint for epoch 697: ./checkpoints/demo/comp3-697\n",
      "Epoch 699, gen_loss: -1.2492, disc_loss: -2.7801\n",
      "Time for epoch 699 is 59.7566 sec\n",
      "Saved checkpoint for epoch 698: ./checkpoints/demo/comp3-698\n",
      "Epoch 700, gen_loss: 9.1202, disc_loss: -2.9279\n",
      "Time for epoch 700 is 66.9410 sec\n",
      "Saved checkpoint for epoch 699: ./checkpoints/demo/comp3-699\n",
      "Epoch 701, gen_loss: 11.6687, disc_loss: -2.9060\n",
      "Time for epoch 701 is 75.4680 sec\n",
      "Saved checkpoint for epoch 700: ./checkpoints/demo/comp3-700\n",
      "Epoch 702, gen_loss: 17.1007, disc_loss: -2.9793\n",
      "Time for epoch 702 is 61.9807 sec\n",
      "Saved checkpoint for epoch 701: ./checkpoints/demo/comp3-701\n",
      "Epoch 703, gen_loss: 3.3934, disc_loss: -2.9330\n",
      "Time for epoch 703 is 67.4237 sec\n",
      "Saved checkpoint for epoch 702: ./checkpoints/demo/comp3-702\n",
      "Epoch 704, gen_loss: 5.6869, disc_loss: -2.5992\n",
      "Time for epoch 704 is 64.8338 sec\n",
      "Saved checkpoint for epoch 703: ./checkpoints/demo/comp3-703\n",
      "Epoch 705, gen_loss: -4.0991, disc_loss: -2.7799\n",
      "Time for epoch 705 is 66.7940 sec\n",
      "Saved checkpoint for epoch 704: ./checkpoints/demo/comp3-704\n",
      "Epoch 706, gen_loss: 7.0779, disc_loss: -2.8256\n",
      "Time for epoch 706 is 59.9550 sec\n",
      "Saved checkpoint for epoch 705: ./checkpoints/demo/comp3-705\n",
      "Epoch 707, gen_loss: 7.3999, disc_loss: -2.8519\n",
      "Time for epoch 707 is 62.4713 sec\n",
      "Saved checkpoint for epoch 706: ./checkpoints/demo/comp3-706\n",
      "Epoch 708, gen_loss: 11.6412, disc_loss: -2.7927\n",
      "Time for epoch 708 is 65.3650 sec\n",
      "Saved checkpoint for epoch 707: ./checkpoints/demo/comp3-707\n",
      "Epoch 709, gen_loss: -2.5252, disc_loss: -2.9458\n",
      "Time for epoch 709 is 64.4954 sec\n",
      "Saved checkpoint for epoch 708: ./checkpoints/demo/comp3-708\n",
      "Epoch 710, gen_loss: -8.9637, disc_loss: -2.7302\n",
      "Time for epoch 710 is 81.9595 sec\n",
      "Saved checkpoint for epoch 709: ./checkpoints/demo/comp3-709\n",
      "Epoch 711, gen_loss: 15.3334, disc_loss: -2.7831\n",
      "Time for epoch 711 is 63.5862 sec\n",
      "Saved checkpoint for epoch 710: ./checkpoints/demo/comp3-710\n",
      "Epoch 712, gen_loss: 16.0775, disc_loss: -2.7247\n",
      "Time for epoch 712 is 59.6462 sec\n",
      "Saved checkpoint for epoch 711: ./checkpoints/demo/comp3-711\n",
      "Epoch 713, gen_loss: 12.4333, disc_loss: -2.9551\n",
      "Time for epoch 713 is 66.6996 sec\n",
      "Saved checkpoint for epoch 712: ./checkpoints/demo/comp3-712\n",
      "Epoch 714, gen_loss: 3.3779, disc_loss: -2.7577\n",
      "Time for epoch 714 is 74.7462 sec\n",
      "Saved checkpoint for epoch 713: ./checkpoints/demo/comp3-713\n",
      "Epoch 715, gen_loss: 20.5248, disc_loss: -2.9826\n",
      "Time for epoch 715 is 64.8445 sec\n",
      "Saved checkpoint for epoch 714: ./checkpoints/demo/comp3-714\n",
      "Epoch 716, gen_loss: -3.0575, disc_loss: -2.8747\n",
      "Time for epoch 716 is 82.0942 sec\n",
      "Saved checkpoint for epoch 715: ./checkpoints/demo/comp3-715\n",
      "Epoch 717, gen_loss: -0.8126, disc_loss: -2.7098\n",
      "Time for epoch 717 is 67.5321 sec\n",
      "Saved checkpoint for epoch 716: ./checkpoints/demo/comp3-716\n",
      "Epoch 718, gen_loss: 23.5823, disc_loss: -2.6990\n",
      "Time for epoch 718 is 66.4830 sec\n",
      "Saved checkpoint for epoch 717: ./checkpoints/demo/comp3-717\n",
      "Epoch 719, gen_loss: 10.2714, disc_loss: -2.6075\n",
      "Time for epoch 719 is 62.2116 sec\n",
      "Saved checkpoint for epoch 718: ./checkpoints/demo/comp3-718\n",
      "Epoch 720, gen_loss: 10.1213, disc_loss: -2.7727\n",
      "Time for epoch 720 is 59.9412 sec\n",
      "Saved checkpoint for epoch 719: ./checkpoints/demo/comp3-719\n",
      "Epoch 721, gen_loss: 29.4715, disc_loss: -2.7725\n",
      "Time for epoch 721 is 66.7883 sec\n",
      "Saved checkpoint for epoch 720: ./checkpoints/demo/comp3-720\n",
      "Epoch 722, gen_loss: 6.3278, disc_loss: -2.9587\n",
      "Time for epoch 722 is 65.4398 sec\n",
      "Saved checkpoint for epoch 721: ./checkpoints/demo/comp3-721\n",
      "Epoch 723, gen_loss: 18.2537, disc_loss: -2.6609\n",
      "Time for epoch 723 is 68.7931 sec\n",
      "Saved checkpoint for epoch 722: ./checkpoints/demo/comp3-722\n",
      "Epoch 724, gen_loss: 13.1118, disc_loss: -3.2545\n",
      "Time for epoch 724 is 64.7430 sec\n",
      "Saved checkpoint for epoch 723: ./checkpoints/demo/comp3-723\n",
      "Epoch 725, gen_loss: 29.8326, disc_loss: -2.6823\n",
      "Time for epoch 725 is 60.0475 sec\n",
      "Saved checkpoint for epoch 724: ./checkpoints/demo/comp3-724\n",
      "Epoch 726, gen_loss: 23.4582, disc_loss: -2.8136\n",
      "Time for epoch 726 is 66.4787 sec\n",
      "Saved checkpoint for epoch 725: ./checkpoints/demo/comp3-725\n",
      "Epoch 727, gen_loss: 19.3652, disc_loss: -2.7559\n",
      "Time for epoch 727 is 75.0924 sec\n",
      "Saved checkpoint for epoch 726: ./checkpoints/demo/comp3-726\n",
      "Epoch 728, gen_loss: 21.2430, disc_loss: -2.6689\n",
      "Time for epoch 728 is 65.1245 sec\n",
      "Saved checkpoint for epoch 727: ./checkpoints/demo/comp3-727\n",
      "Epoch 729, gen_loss: 14.1600, disc_loss: -2.7140\n",
      "Time for epoch 729 is 82.0495 sec\n",
      "Saved checkpoint for epoch 728: ./checkpoints/demo/comp3-728\n",
      "Epoch 730, gen_loss: 17.9271, disc_loss: -2.7393\n",
      "Time for epoch 730 is 68.6434 sec\n",
      "Saved checkpoint for epoch 729: ./checkpoints/demo/comp3-729\n",
      "Epoch 731, gen_loss: 14.8309, disc_loss: -2.8045\n",
      "Time for epoch 731 is 64.1842 sec\n",
      "Saved checkpoint for epoch 730: ./checkpoints/demo/comp3-730\n",
      "Epoch 732, gen_loss: 11.2614, disc_loss: -2.7258\n",
      "Time for epoch 732 is 59.6600 sec\n",
      "Saved checkpoint for epoch 731: ./checkpoints/demo/comp3-731\n",
      "Epoch 733, gen_loss: 9.7813, disc_loss: -2.7909\n",
      "Time for epoch 733 is 66.4880 sec\n",
      "Saved checkpoint for epoch 732: ./checkpoints/demo/comp3-732\n",
      "Epoch 734, gen_loss: 13.8748, disc_loss: -2.8917\n",
      "Time for epoch 734 is 75.4713 sec\n",
      "Saved checkpoint for epoch 733: ./checkpoints/demo/comp3-733\n",
      "Epoch 735, gen_loss: 15.2913, disc_loss: -2.7838\n",
      "Time for epoch 735 is 63.7348 sec\n",
      "Saved checkpoint for epoch 734: ./checkpoints/demo/comp3-734\n",
      "Epoch 736, gen_loss: -9.0110, disc_loss: -2.7764\n",
      "Time for epoch 736 is 65.3103 sec\n",
      "Saved checkpoint for epoch 735: ./checkpoints/demo/comp3-735\n",
      "Epoch 737, gen_loss: 0.3523, disc_loss: -2.5439\n",
      "Time for epoch 737 is 64.8649 sec\n",
      "Saved checkpoint for epoch 736: ./checkpoints/demo/comp3-736\n",
      "Epoch 738, gen_loss: 8.2388, disc_loss: -2.6030\n",
      "Time for epoch 738 is 69.1611 sec\n",
      "Saved checkpoint for epoch 737: ./checkpoints/demo/comp3-737\n",
      "Epoch 739, gen_loss: 4.4172, disc_loss: -2.7139\n",
      "Time for epoch 739 is 66.7991 sec\n",
      "Saved checkpoint for epoch 738: ./checkpoints/demo/comp3-738\n",
      "Epoch 740, gen_loss: 9.4043, disc_loss: -2.8848\n",
      "Time for epoch 740 is 65.3148 sec\n",
      "Saved checkpoint for epoch 739: ./checkpoints/demo/comp3-739\n",
      "Epoch 741, gen_loss: 12.1228, disc_loss: -2.7451\n",
      "Time for epoch 741 is 65.7921 sec\n",
      "Saved checkpoint for epoch 740: ./checkpoints/demo/comp3-740\n",
      "Epoch 742, gen_loss: 10.8798, disc_loss: -2.8550\n",
      "Time for epoch 742 is 67.3178 sec\n",
      "Saved checkpoint for epoch 741: ./checkpoints/demo/comp3-741\n",
      "Epoch 743, gen_loss: 8.9228, disc_loss: -2.8113\n",
      "Time for epoch 743 is 61.7956 sec\n",
      "Saved checkpoint for epoch 742: ./checkpoints/demo/comp3-742\n",
      "Epoch 744, gen_loss: 13.1240, disc_loss: -2.8505\n",
      "Time for epoch 744 is 63.9560 sec\n",
      "Saved checkpoint for epoch 743: ./checkpoints/demo/comp3-743\n",
      "Epoch 745, gen_loss: 10.4160, disc_loss: -3.1017\n",
      "Time for epoch 745 is 72.6513 sec\n",
      "Saved checkpoint for epoch 744: ./checkpoints/demo/comp3-744\n",
      "Epoch 746, gen_loss: 25.7000, disc_loss: -2.7739\n",
      "Time for epoch 746 is 67.5015 sec\n",
      "Saved checkpoint for epoch 745: ./checkpoints/demo/comp3-745\n",
      "Epoch 747, gen_loss: 18.9759, disc_loss: -2.5851\n",
      "Time for epoch 747 is 62.8928 sec\n",
      "Saved checkpoint for epoch 746: ./checkpoints/demo/comp3-746\n",
      "Epoch 748, gen_loss: 12.2614, disc_loss: -2.6403\n",
      "Time for epoch 748 is 64.4599 sec\n",
      "Saved checkpoint for epoch 747: ./checkpoints/demo/comp3-747\n",
      "Epoch 749, gen_loss: 13.2747, disc_loss: -2.7848\n",
      "Time for epoch 749 is 64.5051 sec\n",
      "Saved checkpoint for epoch 748: ./checkpoints/demo/comp3-748\n",
      "Epoch 750, gen_loss: 5.3920, disc_loss: -2.8788\n",
      "Time for epoch 750 is 66.2413 sec\n",
      "Saved checkpoint for epoch 749: ./checkpoints/demo/comp3-749\n",
      "Epoch 751, gen_loss: 0.3092, disc_loss: -2.8994\n",
      "Time for epoch 751 is 67.5476 sec\n",
      "Saved checkpoint for epoch 750: ./checkpoints/demo/comp3-750\n",
      "Epoch 752, gen_loss: -2.7489, disc_loss: -2.8311\n",
      "Time for epoch 752 is 65.3251 sec\n",
      "Saved checkpoint for epoch 751: ./checkpoints/demo/comp3-751\n",
      "Epoch 753, gen_loss: -4.3414, disc_loss: -2.8648\n",
      "Time for epoch 753 is 68.1764 sec\n",
      "Saved checkpoint for epoch 752: ./checkpoints/demo/comp3-752\n",
      "Epoch 754, gen_loss: -0.5770, disc_loss: -2.8235\n",
      "Time for epoch 754 is 64.0100 sec\n",
      "Saved checkpoint for epoch 753: ./checkpoints/demo/comp3-753\n",
      "Epoch 755, gen_loss: -0.0467, disc_loss: -2.8531\n",
      "Time for epoch 755 is 59.5974 sec\n",
      "Saved checkpoint for epoch 754: ./checkpoints/demo/comp3-754\n",
      "Epoch 756, gen_loss: 10.3404, disc_loss: -2.7608\n",
      "Time for epoch 756 is 81.9541 sec\n",
      "Saved checkpoint for epoch 755: ./checkpoints/demo/comp3-755\n",
      "Epoch 757, gen_loss: 0.8255, disc_loss: -2.7611\n",
      "Time for epoch 757 is 66.2443 sec\n",
      "Saved checkpoint for epoch 756: ./checkpoints/demo/comp3-756\n",
      "Epoch 758, gen_loss: -8.3713, disc_loss: -2.7808\n",
      "Time for epoch 758 is 74.9940 sec\n",
      "Saved checkpoint for epoch 757: ./checkpoints/demo/comp3-757\n",
      "Epoch 759, gen_loss: -0.1387, disc_loss: -2.8458\n",
      "Time for epoch 759 is 65.1699 sec\n",
      "Saved checkpoint for epoch 758: ./checkpoints/demo/comp3-758\n",
      "Epoch 760, gen_loss: 0.4070, disc_loss: -2.8773\n",
      "Time for epoch 760 is 62.8292 sec\n",
      "Saved checkpoint for epoch 759: ./checkpoints/demo/comp3-759\n",
      "Epoch 761, gen_loss: 5.4055, disc_loss: -2.9236\n",
      "Time for epoch 761 is 65.9365 sec\n",
      "Saved checkpoint for epoch 760: ./checkpoints/demo/comp3-760\n",
      "Epoch 762, gen_loss: 7.1670, disc_loss: -2.9764\n",
      "Time for epoch 762 is 65.2806 sec\n",
      "Saved checkpoint for epoch 761: ./checkpoints/demo/comp3-761\n",
      "Epoch 763, gen_loss: 23.2276, disc_loss: -2.8704\n",
      "Time for epoch 763 is 66.9219 sec\n",
      "Saved checkpoint for epoch 762: ./checkpoints/demo/comp3-762\n",
      "Epoch 764, gen_loss: 11.5999, disc_loss: -2.7747\n",
      "Time for epoch 764 is 66.0570 sec\n",
      "Saved checkpoint for epoch 763: ./checkpoints/demo/comp3-763\n",
      "Epoch 765, gen_loss: 7.0652, disc_loss: -2.9163\n",
      "Time for epoch 765 is 65.3704 sec\n",
      "Saved checkpoint for epoch 764: ./checkpoints/demo/comp3-764\n",
      "Epoch 766, gen_loss: 6.4693, disc_loss: -2.7444\n",
      "Time for epoch 766 is 66.0554 sec\n",
      "Saved checkpoint for epoch 765: ./checkpoints/demo/comp3-765\n",
      "Epoch 767, gen_loss: 7.6486, disc_loss: -2.4917\n",
      "Time for epoch 767 is 67.2947 sec\n",
      "Saved checkpoint for epoch 766: ./checkpoints/demo/comp3-766\n",
      "Epoch 768, gen_loss: 5.5788, disc_loss: -2.5711\n",
      "Time for epoch 768 is 60.4602 sec\n",
      "Saved checkpoint for epoch 767: ./checkpoints/demo/comp3-767\n",
      "Epoch 769, gen_loss: -4.7783, disc_loss: -2.6537\n",
      "Time for epoch 769 is 64.0403 sec\n",
      "Saved checkpoint for epoch 768: ./checkpoints/demo/comp3-768\n",
      "Epoch 770, gen_loss: 3.1385, disc_loss: -2.7636\n",
      "Time for epoch 770 is 72.7571 sec\n",
      "Saved checkpoint for epoch 769: ./checkpoints/demo/comp3-769\n",
      "Epoch 771, gen_loss: 12.6369, disc_loss: -2.7611\n",
      "Time for epoch 771 is 67.0902 sec\n",
      "Saved checkpoint for epoch 770: ./checkpoints/demo/comp3-770\n",
      "Epoch 772, gen_loss: -0.2990, disc_loss: -2.7125\n",
      "Time for epoch 772 is 61.4105 sec\n",
      "Saved checkpoint for epoch 771: ./checkpoints/demo/comp3-771\n",
      "Epoch 773, gen_loss: -1.8305, disc_loss: -2.7446\n",
      "Time for epoch 773 is 65.8410 sec\n",
      "Saved checkpoint for epoch 772: ./checkpoints/demo/comp3-772\n",
      "Epoch 774, gen_loss: 10.9486, disc_loss: -3.2048\n",
      "Time for epoch 774 is 64.8033 sec\n",
      "Saved checkpoint for epoch 773: ./checkpoints/demo/comp3-773\n",
      "Epoch 775, gen_loss: 6.0018, disc_loss: -2.6393\n",
      "Time for epoch 775 is 66.9337 sec\n",
      "Saved checkpoint for epoch 774: ./checkpoints/demo/comp3-774\n",
      "Epoch 776, gen_loss: 3.9528, disc_loss: -2.7830\n",
      "Time for epoch 776 is 67.9398 sec\n",
      "Saved checkpoint for epoch 775: ./checkpoints/demo/comp3-775\n",
      "Epoch 777, gen_loss: 3.7006, disc_loss: -2.7575\n",
      "Time for epoch 777 is 65.4832 sec\n",
      "Saved checkpoint for epoch 776: ./checkpoints/demo/comp3-776\n",
      "Epoch 778, gen_loss: 5.7015, disc_loss: -2.9346\n",
      "Time for epoch 778 is 65.5112 sec\n",
      "Saved checkpoint for epoch 777: ./checkpoints/demo/comp3-777\n",
      "Epoch 779, gen_loss: -6.4632, disc_loss: -2.8258\n",
      "Time for epoch 779 is 67.8875 sec\n",
      "Saved checkpoint for epoch 778: ./checkpoints/demo/comp3-778\n",
      "Epoch 780, gen_loss: -2.4312, disc_loss: -2.8640\n",
      "Time for epoch 780 is 63.3283 sec\n",
      "Saved checkpoint for epoch 779: ./checkpoints/demo/comp3-779\n",
      "Epoch 781, gen_loss: -3.2188, disc_loss: -2.8424\n",
      "Time for epoch 781 is 60.5182 sec\n",
      "Saved checkpoint for epoch 780: ./checkpoints/demo/comp3-780\n",
      "Epoch 782, gen_loss: 1.3607, disc_loss: -2.8751\n",
      "Time for epoch 782 is 69.2195 sec\n",
      "Saved checkpoint for epoch 781: ./checkpoints/demo/comp3-781\n",
      "Epoch 783, gen_loss: -2.8465, disc_loss: -2.8082\n",
      "Time for epoch 783 is 71.2072 sec\n",
      "Saved checkpoint for epoch 782: ./checkpoints/demo/comp3-782\n",
      "Epoch 784, gen_loss: 6.1995, disc_loss: -2.7707\n",
      "Time for epoch 784 is 60.9590 sec\n",
      "Saved checkpoint for epoch 783: ./checkpoints/demo/comp3-783\n",
      "Epoch 785, gen_loss: 5.4762, disc_loss: -2.7722\n",
      "Time for epoch 785 is 67.2933 sec\n",
      "Saved checkpoint for epoch 784: ./checkpoints/demo/comp3-784\n",
      "Epoch 786, gen_loss: 12.6718, disc_loss: -2.9117\n",
      "Time for epoch 786 is 65.1546 sec\n",
      "Saved checkpoint for epoch 785: ./checkpoints/demo/comp3-785\n",
      "Epoch 787, gen_loss: 19.9393, disc_loss: -2.7241\n",
      "Time for epoch 787 is 66.6072 sec\n",
      "Saved checkpoint for epoch 786: ./checkpoints/demo/comp3-786\n",
      "Epoch 788, gen_loss: 3.4272, disc_loss: -3.1589\n",
      "Time for epoch 788 is 66.6998 sec\n",
      "Saved checkpoint for epoch 787: ./checkpoints/demo/comp3-787\n",
      "Epoch 789, gen_loss: 0.6098, disc_loss: -2.9302\n",
      "Time for epoch 789 is 65.7378 sec\n",
      "Saved checkpoint for epoch 788: ./checkpoints/demo/comp3-788\n",
      "Epoch 790, gen_loss: -0.1844, disc_loss: -2.8110\n",
      "Time for epoch 790 is 65.3686 sec\n",
      "Saved checkpoint for epoch 789: ./checkpoints/demo/comp3-789\n",
      "Epoch 791, gen_loss: 3.0848, disc_loss: -2.7731\n",
      "Time for epoch 791 is 68.1935 sec\n",
      "Saved checkpoint for epoch 790: ./checkpoints/demo/comp3-790\n",
      "Epoch 792, gen_loss: -3.5939, disc_loss: -2.8212\n",
      "Time for epoch 792 is 60.7432 sec\n",
      "Saved checkpoint for epoch 791: ./checkpoints/demo/comp3-791\n",
      "Epoch 793, gen_loss: 2.4832, disc_loss: -2.8830\n",
      "Time for epoch 793 is 62.7782 sec\n",
      "Saved checkpoint for epoch 792: ./checkpoints/demo/comp3-792\n",
      "Epoch 794, gen_loss: -1.0219, disc_loss: -2.9708\n",
      "Time for epoch 794 is 71.6503 sec\n",
      "Saved checkpoint for epoch 793: ./checkpoints/demo/comp3-793\n",
      "Epoch 795, gen_loss: -4.6723, disc_loss: -2.8690\n",
      "Time for epoch 795 is 70.4828 sec\n",
      "Saved checkpoint for epoch 794: ./checkpoints/demo/comp3-794\n",
      "Epoch 796, gen_loss: 0.5537, disc_loss: -2.9071\n",
      "Time for epoch 796 is 64.6098 sec\n",
      "Saved checkpoint for epoch 795: ./checkpoints/demo/comp3-795\n",
      "Epoch 797, gen_loss: 7.3184, disc_loss: -2.8745\n",
      "Time for epoch 797 is 64.5148 sec\n",
      "Saved checkpoint for epoch 796: ./checkpoints/demo/comp3-796\n",
      "Epoch 798, gen_loss: 15.1918, disc_loss: -2.6515\n",
      "Time for epoch 798 is 64.7261 sec\n",
      "Saved checkpoint for epoch 797: ./checkpoints/demo/comp3-797\n",
      "Epoch 799, gen_loss: -0.6778, disc_loss: -2.7931\n",
      "Time for epoch 799 is 66.5845 sec\n",
      "Saved checkpoint for epoch 798: ./checkpoints/demo/comp3-798\n",
      "Epoch 800, gen_loss: 12.7583, disc_loss: -2.7894\n",
      "Time for epoch 800 is 67.1332 sec\n",
      "Saved checkpoint for epoch 799: ./checkpoints/demo/comp3-799\n",
      "Epoch 801, gen_loss: 1.9874, disc_loss: -2.7568\n",
      "Time for epoch 801 is 66.1822 sec\n",
      "Saved checkpoint for epoch 800: ./checkpoints/demo/comp3-800\n",
      "Epoch 802, gen_loss: 7.7577, disc_loss: -2.9119\n",
      "Time for epoch 802 is 67.8759 sec\n",
      "Saved checkpoint for epoch 801: ./checkpoints/demo/comp3-801\n",
      "Epoch 803, gen_loss: 2.9731, disc_loss: -2.7916\n",
      "Time for epoch 803 is 64.8556 sec\n",
      "Saved checkpoint for epoch 802: ./checkpoints/demo/comp3-802\n",
      "Epoch 804, gen_loss: 8.0595, disc_loss: -2.8886\n",
      "Time for epoch 804 is 61.3562 sec\n",
      "Saved checkpoint for epoch 803: ./checkpoints/demo/comp3-803\n",
      "Epoch 805, gen_loss: 7.7815, disc_loss: -2.9661\n",
      "Time for epoch 805 is 62.2946 sec\n",
      "Saved checkpoint for epoch 804: ./checkpoints/demo/comp3-804\n",
      "Epoch 806, gen_loss: 12.4237, disc_loss: -2.9212\n",
      "Time for epoch 806 is 69.4219 sec\n",
      "Saved checkpoint for epoch 805: ./checkpoints/demo/comp3-805\n",
      "Epoch 807, gen_loss: 12.0627, disc_loss: -2.5839\n",
      "Time for epoch 807 is 71.9528 sec\n",
      "Saved checkpoint for epoch 806: ./checkpoints/demo/comp3-806\n",
      "Epoch 808, gen_loss: 6.8235, disc_loss: -2.8041\n",
      "Time for epoch 808 is 61.9675 sec\n",
      "Saved checkpoint for epoch 807: ./checkpoints/demo/comp3-807\n",
      "Epoch 809, gen_loss: 8.5596, disc_loss: -2.8801\n",
      "Time for epoch 809 is 65.4505 sec\n",
      "Saved checkpoint for epoch 808: ./checkpoints/demo/comp3-808\n",
      "Epoch 810, gen_loss: 19.3221, disc_loss: -2.6144\n",
      "Time for epoch 810 is 64.6032 sec\n",
      "Saved checkpoint for epoch 809: ./checkpoints/demo/comp3-809\n",
      "Epoch 811, gen_loss: 11.6325, disc_loss: -2.8305\n",
      "Time for epoch 811 is 67.8426 sec\n",
      "Saved checkpoint for epoch 810: ./checkpoints/demo/comp3-810\n",
      "Epoch 812, gen_loss: 4.1991, disc_loss: -2.8624\n",
      "Time for epoch 812 is 66.6263 sec\n",
      "Saved checkpoint for epoch 811: ./checkpoints/demo/comp3-811\n",
      "Epoch 813, gen_loss: 0.2446, disc_loss: -2.7483\n",
      "Time for epoch 813 is 65.2836 sec\n",
      "Saved checkpoint for epoch 812: ./checkpoints/demo/comp3-812\n",
      "Epoch 814, gen_loss: -3.2824, disc_loss: -2.8469\n",
      "Time for epoch 814 is 67.5398 sec\n",
      "Saved checkpoint for epoch 813: ./checkpoints/demo/comp3-813\n",
      "Epoch 815, gen_loss: 5.2049, disc_loss: -2.6368\n",
      "Time for epoch 815 is 65.3852 sec\n",
      "Saved checkpoint for epoch 814: ./checkpoints/demo/comp3-814\n",
      "Epoch 816, gen_loss: 12.3827, disc_loss: -2.7388\n",
      "Time for epoch 816 is 61.5011 sec\n",
      "Saved checkpoint for epoch 815: ./checkpoints/demo/comp3-815\n",
      "Epoch 817, gen_loss: 1.3846, disc_loss: -2.9771\n",
      "Time for epoch 817 is 65.7756 sec\n",
      "Saved checkpoint for epoch 816: ./checkpoints/demo/comp3-816\n",
      "Epoch 818, gen_loss: 10.7691, disc_loss: -2.9642\n",
      "Time for epoch 818 is 74.7504 sec\n",
      "Saved checkpoint for epoch 817: ./checkpoints/demo/comp3-817\n",
      "Epoch 819, gen_loss: -1.8046, disc_loss: -2.8753\n",
      "Time for epoch 819 is 63.4822 sec\n",
      "Saved checkpoint for epoch 818: ./checkpoints/demo/comp3-818\n",
      "Epoch 820, gen_loss: 0.4582, disc_loss: -2.8886\n",
      "Time for epoch 820 is 64.0896 sec\n",
      "Saved checkpoint for epoch 819: ./checkpoints/demo/comp3-819\n",
      "Epoch 821, gen_loss: 7.2030, disc_loss: -3.2788\n",
      "Time for epoch 821 is 64.2883 sec\n",
      "Saved checkpoint for epoch 820: ./checkpoints/demo/comp3-820\n",
      "Epoch 822, gen_loss: -1.1248, disc_loss: -2.7495\n",
      "Time for epoch 822 is 64.5742 sec\n",
      "Saved checkpoint for epoch 821: ./checkpoints/demo/comp3-821\n",
      "Epoch 823, gen_loss: 8.1456, disc_loss: -2.3912\n",
      "Time for epoch 823 is 66.7441 sec\n",
      "Saved checkpoint for epoch 822: ./checkpoints/demo/comp3-822\n",
      "Epoch 824, gen_loss: 13.6538, disc_loss: -2.7731\n",
      "Time for epoch 824 is 66.8500 sec\n",
      "Saved checkpoint for epoch 823: ./checkpoints/demo/comp3-823\n",
      "Epoch 825, gen_loss: 14.9416, disc_loss: -2.8523\n",
      "Time for epoch 825 is 65.9062 sec\n",
      "Saved checkpoint for epoch 824: ./checkpoints/demo/comp3-824\n",
      "Epoch 826, gen_loss: 22.2648, disc_loss: -2.8930\n",
      "Time for epoch 826 is 68.0961 sec\n",
      "Saved checkpoint for epoch 825: ./checkpoints/demo/comp3-825\n",
      "Epoch 827, gen_loss: 2.8846, disc_loss: -2.6137\n",
      "Time for epoch 827 is 65.9536 sec\n",
      "Saved checkpoint for epoch 826: ./checkpoints/demo/comp3-826\n",
      "Epoch 828, gen_loss: 6.9696, disc_loss: -2.8173\n",
      "Time for epoch 828 is 60.6215 sec\n",
      "Saved checkpoint for epoch 827: ./checkpoints/demo/comp3-827\n",
      "Epoch 829, gen_loss: 6.2292, disc_loss: -2.6837\n",
      "Time for epoch 829 is 63.8056 sec\n",
      "Saved checkpoint for epoch 828: ./checkpoints/demo/comp3-828\n",
      "Epoch 830, gen_loss: 6.7159, disc_loss: -2.6055\n",
      "Time for epoch 830 is 73.0607 sec\n",
      "Saved checkpoint for epoch 829: ./checkpoints/demo/comp3-829\n",
      "Epoch 831, gen_loss: -0.7370, disc_loss: -2.7374\n",
      "Time for epoch 831 is 67.4180 sec\n",
      "Saved checkpoint for epoch 830: ./checkpoints/demo/comp3-830\n",
      "Epoch 832, gen_loss: 12.1604, disc_loss: -2.7006\n",
      "Time for epoch 832 is 82.1069 sec\n",
      "Saved checkpoint for epoch 831: ./checkpoints/demo/comp3-831\n",
      "Epoch 833, gen_loss: 8.3535, disc_loss: -2.8784\n",
      "Time for epoch 833 is 66.2003 sec\n",
      "Saved checkpoint for epoch 832: ./checkpoints/demo/comp3-832\n",
      "Epoch 834, gen_loss: 5.8079, disc_loss: -2.9141\n",
      "Time for epoch 834 is 66.3428 sec\n",
      "Saved checkpoint for epoch 833: ./checkpoints/demo/comp3-833\n",
      "Epoch 835, gen_loss: 3.1203, disc_loss: -2.7053\n",
      "Time for epoch 835 is 81.9630 sec\n",
      "Saved checkpoint for epoch 834: ./checkpoints/demo/comp3-834\n",
      "Epoch 836, gen_loss: 21.6005, disc_loss: -2.4993\n",
      "Time for epoch 836 is 68.1419 sec\n",
      "Saved checkpoint for epoch 835: ./checkpoints/demo/comp3-835\n",
      "Epoch 837, gen_loss: 12.0289, disc_loss: -2.7094\n",
      "Time for epoch 837 is 71.5352 sec\n",
      "Saved checkpoint for epoch 836: ./checkpoints/demo/comp3-836\n",
      "Epoch 838, gen_loss: 8.3927, disc_loss: -2.6630\n",
      "Time for epoch 838 is 61.8469 sec\n",
      "Saved checkpoint for epoch 837: ./checkpoints/demo/comp3-837\n",
      "Epoch 839, gen_loss: 2.2874, disc_loss: -2.6798\n",
      "Time for epoch 839 is 63.6438 sec\n",
      "Saved checkpoint for epoch 838: ./checkpoints/demo/comp3-838\n",
      "Epoch 840, gen_loss: -0.2239, disc_loss: -2.7150\n",
      "Time for epoch 840 is 60.9698 sec\n",
      "Saved checkpoint for epoch 839: ./checkpoints/demo/comp3-839\n",
      "Epoch 841, gen_loss: 8.1758, disc_loss: -2.8012\n",
      "Time for epoch 841 is 69.2508 sec\n",
      "Saved checkpoint for epoch 840: ./checkpoints/demo/comp3-840\n",
      "Epoch 842, gen_loss: 5.2019, disc_loss: -2.9121\n",
      "Time for epoch 842 is 71.4008 sec\n",
      "Saved checkpoint for epoch 841: ./checkpoints/demo/comp3-841\n",
      "Epoch 843, gen_loss: 6.5404, disc_loss: -2.9116\n",
      "Time for epoch 843 is 82.0949 sec\n",
      "Saved checkpoint for epoch 842: ./checkpoints/demo/comp3-842\n",
      "Epoch 844, gen_loss: 14.2847, disc_loss: -2.7765\n",
      "Time for epoch 844 is 67.0442 sec\n",
      "Saved checkpoint for epoch 843: ./checkpoints/demo/comp3-843\n",
      "Epoch 845, gen_loss: 9.3828, disc_loss: -2.6073\n",
      "Time for epoch 845 is 63.8115 sec\n",
      "Saved checkpoint for epoch 844: ./checkpoints/demo/comp3-844\n",
      "Epoch 846, gen_loss: 5.3003, disc_loss: -2.7844\n",
      "Time for epoch 846 is 63.6232 sec\n",
      "Saved checkpoint for epoch 845: ./checkpoints/demo/comp3-845\n",
      "Epoch 847, gen_loss: 14.0477, disc_loss: -2.8943\n",
      "Time for epoch 847 is 74.8870 sec\n",
      "Saved checkpoint for epoch 846: ./checkpoints/demo/comp3-846\n",
      "Epoch 848, gen_loss: 18.9046, disc_loss: -2.8242\n",
      "Time for epoch 848 is 65.3875 sec\n",
      "Saved checkpoint for epoch 847: ./checkpoints/demo/comp3-847\n",
      "Epoch 849, gen_loss: 0.2277, disc_loss: -3.0242\n",
      "Time for epoch 849 is 63.6464 sec\n",
      "Saved checkpoint for epoch 848: ./checkpoints/demo/comp3-848\n",
      "Epoch 850, gen_loss: 6.6623, disc_loss: -2.9038\n",
      "Time for epoch 850 is 65.0952 sec\n",
      "Saved checkpoint for epoch 849: ./checkpoints/demo/comp3-849\n",
      "Epoch 851, gen_loss: 16.6536, disc_loss: -2.7017\n",
      "Time for epoch 851 is 65.1188 sec\n",
      "Saved checkpoint for epoch 850: ./checkpoints/demo/comp3-850\n",
      "Epoch 852, gen_loss: 8.4330, disc_loss: -2.6231\n",
      "Time for epoch 852 is 66.8517 sec\n",
      "Saved checkpoint for epoch 851: ./checkpoints/demo/comp3-851\n",
      "Epoch 853, gen_loss: 8.0407, disc_loss: -2.9563\n",
      "Time for epoch 853 is 66.0248 sec\n",
      "Saved checkpoint for epoch 852: ./checkpoints/demo/comp3-852\n",
      "Epoch 854, gen_loss: 2.0275, disc_loss: -2.7733\n",
      "Time for epoch 854 is 65.3290 sec\n",
      "Saved checkpoint for epoch 853: ./checkpoints/demo/comp3-853\n",
      "Epoch 855, gen_loss: 20.4040, disc_loss: -2.6921\n",
      "Time for epoch 855 is 68.0460 sec\n",
      "Saved checkpoint for epoch 854: ./checkpoints/demo/comp3-854\n",
      "Epoch 856, gen_loss: 11.0248, disc_loss: -2.9443\n",
      "Time for epoch 856 is 64.0009 sec\n",
      "Saved checkpoint for epoch 855: ./checkpoints/demo/comp3-855\n",
      "Epoch 857, gen_loss: 18.9456, disc_loss: -2.6501\n",
      "Time for epoch 857 is 59.7717 sec\n",
      "Saved checkpoint for epoch 856: ./checkpoints/demo/comp3-856\n",
      "Epoch 858, gen_loss: 18.3699, disc_loss: -2.6920\n",
      "Time for epoch 858 is 60.6321 sec\n",
      "Saved checkpoint for epoch 857: ./checkpoints/demo/comp3-857\n",
      "Epoch 859, gen_loss: 21.8476, disc_loss: -2.6898\n",
      "Time for epoch 859 is 70.7443 sec\n",
      "Saved checkpoint for epoch 858: ./checkpoints/demo/comp3-858\n",
      "Epoch 860, gen_loss: 8.0340, disc_loss: -2.8323\n",
      "Time for epoch 860 is 67.0737 sec\n",
      "Saved checkpoint for epoch 859: ./checkpoints/demo/comp3-859\n",
      "Epoch 861, gen_loss: 2.9906, disc_loss: -2.6892\n",
      "Time for epoch 861 is 63.6405 sec\n",
      "Saved checkpoint for epoch 860: ./checkpoints/demo/comp3-860\n",
      "Epoch 862, gen_loss: 12.7794, disc_loss: -2.7455\n",
      "Time for epoch 862 is 65.0150 sec\n",
      "Saved checkpoint for epoch 861: ./checkpoints/demo/comp3-861\n",
      "Epoch 863, gen_loss: 6.0205, disc_loss: -2.8524\n",
      "Time for epoch 863 is 65.8539 sec\n",
      "Saved checkpoint for epoch 862: ./checkpoints/demo/comp3-862\n",
      "Epoch 864, gen_loss: 10.7263, disc_loss: -2.8924\n",
      "Time for epoch 864 is 66.3686 sec\n",
      "Saved checkpoint for epoch 863: ./checkpoints/demo/comp3-863\n",
      "Epoch 865, gen_loss: 25.1854, disc_loss: -2.7040\n",
      "Time for epoch 865 is 67.0571 sec\n",
      "Saved checkpoint for epoch 864: ./checkpoints/demo/comp3-864\n",
      "Epoch 866, gen_loss: 9.0592, disc_loss: -2.7473\n",
      "Time for epoch 866 is 65.3201 sec\n",
      "Saved checkpoint for epoch 865: ./checkpoints/demo/comp3-865\n",
      "Epoch 867, gen_loss: 23.7486, disc_loss: -2.8903\n",
      "Time for epoch 867 is 67.7849 sec\n",
      "Saved checkpoint for epoch 866: ./checkpoints/demo/comp3-866\n",
      "Epoch 868, gen_loss: 5.2791, disc_loss: -2.8956\n",
      "Time for epoch 868 is 66.2344 sec\n",
      "Saved checkpoint for epoch 867: ./checkpoints/demo/comp3-867\n",
      "Epoch 869, gen_loss: 1.1393, disc_loss: -2.8232\n",
      "Time for epoch 869 is 60.6320 sec\n",
      "Saved checkpoint for epoch 868: ./checkpoints/demo/comp3-868\n",
      "Epoch 870, gen_loss: 5.6566, disc_loss: -2.7830\n",
      "Time for epoch 870 is 62.9919 sec\n",
      "Saved checkpoint for epoch 869: ./checkpoints/demo/comp3-869\n",
      "Epoch 871, gen_loss: 17.8655, disc_loss: -2.6952\n",
      "Time for epoch 871 is 70.8514 sec\n",
      "Saved checkpoint for epoch 870: ./checkpoints/demo/comp3-870\n",
      "Epoch 872, gen_loss: 7.7573, disc_loss: -2.7796\n",
      "Time for epoch 872 is 71.0154 sec\n",
      "Saved checkpoint for epoch 871: ./checkpoints/demo/comp3-871\n",
      "Epoch 873, gen_loss: 4.4481, disc_loss: -2.7823\n",
      "Time for epoch 873 is 63.4213 sec\n",
      "Saved checkpoint for epoch 872: ./checkpoints/demo/comp3-872\n",
      "Epoch 874, gen_loss: -0.8314, disc_loss: -2.9545\n",
      "Time for epoch 874 is 64.1957 sec\n",
      "Saved checkpoint for epoch 873: ./checkpoints/demo/comp3-873\n",
      "Epoch 875, gen_loss: 10.0628, disc_loss: -2.5706\n",
      "Time for epoch 875 is 64.0209 sec\n",
      "Saved checkpoint for epoch 874: ./checkpoints/demo/comp3-874\n",
      "Epoch 876, gen_loss: 2.6399, disc_loss: -2.7568\n",
      "Time for epoch 876 is 67.2962 sec\n",
      "Saved checkpoint for epoch 875: ./checkpoints/demo/comp3-875\n",
      "Epoch 877, gen_loss: 10.7957, disc_loss: -2.8015\n",
      "Time for epoch 877 is 66.3701 sec\n",
      "Saved checkpoint for epoch 876: ./checkpoints/demo/comp3-876\n",
      "Epoch 878, gen_loss: 16.1796, disc_loss: -2.7092\n",
      "Time for epoch 878 is 65.8182 sec\n",
      "Saved checkpoint for epoch 877: ./checkpoints/demo/comp3-877\n",
      "Epoch 879, gen_loss: 5.2578, disc_loss: -2.8525\n",
      "Time for epoch 879 is 68.4835 sec\n",
      "Saved checkpoint for epoch 878: ./checkpoints/demo/comp3-878\n",
      "Epoch 880, gen_loss: 0.2688, disc_loss: -2.9983\n",
      "Time for epoch 880 is 63.9480 sec\n",
      "Saved checkpoint for epoch 879: ./checkpoints/demo/comp3-879\n",
      "Epoch 881, gen_loss: 5.6185, disc_loss: -2.8111\n",
      "Time for epoch 881 is 59.8542 sec\n",
      "Saved checkpoint for epoch 880: ./checkpoints/demo/comp3-880\n",
      "Epoch 882, gen_loss: 7.1643, disc_loss: -2.6673\n",
      "Time for epoch 882 is 66.5307 sec\n",
      "Saved checkpoint for epoch 881: ./checkpoints/demo/comp3-881\n",
      "Epoch 883, gen_loss: -8.7239, disc_loss: -2.7918\n",
      "Time for epoch 883 is 65.6422 sec\n",
      "Saved checkpoint for epoch 882: ./checkpoints/demo/comp3-882\n",
      "Epoch 884, gen_loss: 16.2326, disc_loss: -2.6124\n",
      "Time for epoch 884 is 66.7604 sec\n",
      "Saved checkpoint for epoch 883: ./checkpoints/demo/comp3-883\n",
      "Epoch 885, gen_loss: 10.7831, disc_loss: -2.6876\n",
      "Time for epoch 885 is 66.9655 sec\n",
      "Saved checkpoint for epoch 884: ./checkpoints/demo/comp3-884\n",
      "Epoch 886, gen_loss: 16.2339, disc_loss: -2.8259\n",
      "Time for epoch 886 is 62.8992 sec\n",
      "Saved checkpoint for epoch 885: ./checkpoints/demo/comp3-885\n",
      "Epoch 887, gen_loss: -3.0123, disc_loss: -2.7425\n",
      "Time for epoch 887 is 60.1699 sec\n",
      "Saved checkpoint for epoch 886: ./checkpoints/demo/comp3-886\n",
      "Epoch 888, gen_loss: 12.8233, disc_loss: -2.5779\n",
      "Time for epoch 888 is 66.8678 sec\n",
      "Saved checkpoint for epoch 887: ./checkpoints/demo/comp3-887\n",
      "Epoch 889, gen_loss: -0.0844, disc_loss: -2.7392\n",
      "Time for epoch 889 is 75.1014 sec\n",
      "Saved checkpoint for epoch 888: ./checkpoints/demo/comp3-888\n",
      "Epoch 890, gen_loss: 8.5639, disc_loss: -2.7843\n",
      "Time for epoch 890 is 63.4420 sec\n",
      "Saved checkpoint for epoch 889: ./checkpoints/demo/comp3-889\n",
      "Epoch 891, gen_loss: 7.3545, disc_loss: -2.7948\n",
      "Time for epoch 891 is 64.7074 sec\n",
      "Saved checkpoint for epoch 890: ./checkpoints/demo/comp3-890\n",
      "Epoch 892, gen_loss: -0.9161, disc_loss: -2.7668\n",
      "Time for epoch 892 is 65.0360 sec\n",
      "Saved checkpoint for epoch 891: ./checkpoints/demo/comp3-891\n",
      "Epoch 893, gen_loss: 10.1237, disc_loss: -2.9205\n",
      "Time for epoch 893 is 66.4586 sec\n",
      "Saved checkpoint for epoch 892: ./checkpoints/demo/comp3-892\n",
      "Epoch 894, gen_loss: 16.9745, disc_loss: -2.9262\n",
      "Time for epoch 894 is 67.5296 sec\n",
      "Saved checkpoint for epoch 893: ./checkpoints/demo/comp3-893\n",
      "Epoch 895, gen_loss: 5.7875, disc_loss: -2.6620\n",
      "Time for epoch 895 is 65.1659 sec\n",
      "Saved checkpoint for epoch 894: ./checkpoints/demo/comp3-894\n",
      "Epoch 896, gen_loss: 15.9089, disc_loss: -2.7688\n",
      "Time for epoch 896 is 82.0433 sec\n",
      "Saved checkpoint for epoch 895: ./checkpoints/demo/comp3-895\n",
      "Epoch 897, gen_loss: 8.3324, disc_loss: -3.0320\n",
      "Time for epoch 897 is 68.8933 sec\n",
      "Saved checkpoint for epoch 896: ./checkpoints/demo/comp3-896\n",
      "Epoch 898, gen_loss: 0.5242, disc_loss: -2.5939\n",
      "Time for epoch 898 is 63.6437 sec\n",
      "Saved checkpoint for epoch 897: ./checkpoints/demo/comp3-897\n",
      "Epoch 899, gen_loss: 1.4738, disc_loss: -2.7033\n",
      "Time for epoch 899 is 64.7480 sec\n",
      "Saved checkpoint for epoch 898: ./checkpoints/demo/comp3-898\n",
      "Epoch 900, gen_loss: 9.7862, disc_loss: -2.5232\n",
      "Time for epoch 900 is 64.5129 sec\n",
      "Saved checkpoint for epoch 899: ./checkpoints/demo/comp3-899\n",
      "Epoch 901, gen_loss: 8.0988, disc_loss: -2.5998\n",
      "Time for epoch 901 is 66.8821 sec\n",
      "Saved checkpoint for epoch 900: ./checkpoints/demo/comp3-900\n",
      "Epoch 902, gen_loss: 14.9647, disc_loss: -2.8270\n",
      "Time for epoch 902 is 66.6696 sec\n",
      "Saved checkpoint for epoch 901: ./checkpoints/demo/comp3-901\n",
      "Epoch 903, gen_loss: -10.2968, disc_loss: -2.8963\n",
      "Time for epoch 903 is 65.7704 sec\n",
      "Saved checkpoint for epoch 902: ./checkpoints/demo/comp3-902\n",
      "Epoch 904, gen_loss: 17.1942, disc_loss: -2.5823\n",
      "Time for epoch 904 is 66.9226 sec\n",
      "Saved checkpoint for epoch 903: ./checkpoints/demo/comp3-903\n",
      "Epoch 905, gen_loss: -3.6811, disc_loss: -2.7239\n",
      "Time for epoch 905 is 65.9533 sec\n",
      "Saved checkpoint for epoch 904: ./checkpoints/demo/comp3-904\n",
      "Epoch 906, gen_loss: -5.8501, disc_loss: -2.6363\n",
      "Time for epoch 906 is 60.7505 sec\n",
      "Saved checkpoint for epoch 905: ./checkpoints/demo/comp3-905\n",
      "Epoch 907, gen_loss: -0.7363, disc_loss: -2.7763\n",
      "Time for epoch 907 is 62.9568 sec\n",
      "Saved checkpoint for epoch 906: ./checkpoints/demo/comp3-906\n",
      "Epoch 908, gen_loss: 5.4261, disc_loss: -2.7688\n",
      "Time for epoch 908 is 70.1238 sec\n",
      "Saved checkpoint for epoch 907: ./checkpoints/demo/comp3-907\n",
      "Epoch 909, gen_loss: -2.6733, disc_loss: -2.7403\n",
      "Time for epoch 909 is 70.7392 sec\n",
      "Saved checkpoint for epoch 908: ./checkpoints/demo/comp3-908\n",
      "Epoch 910, gen_loss: 4.6911, disc_loss: -2.7614\n",
      "Time for epoch 910 is 61.7936 sec\n",
      "Saved checkpoint for epoch 909: ./checkpoints/demo/comp3-909\n",
      "Epoch 911, gen_loss: -1.0574, disc_loss: -2.9143\n",
      "Time for epoch 911 is 65.3935 sec\n",
      "Saved checkpoint for epoch 910: ./checkpoints/demo/comp3-910\n",
      "Epoch 912, gen_loss: 8.1194, disc_loss: -2.8101\n",
      "Time for epoch 912 is 64.2799 sec\n",
      "Saved checkpoint for epoch 911: ./checkpoints/demo/comp3-911\n",
      "Epoch 913, gen_loss: 6.3319, disc_loss: -3.1106\n",
      "Time for epoch 913 is 74.9465 sec\n",
      "Saved checkpoint for epoch 912: ./checkpoints/demo/comp3-912\n",
      "Epoch 914, gen_loss: 9.2182, disc_loss: -2.6307\n",
      "Time for epoch 914 is 65.1239 sec\n",
      "Saved checkpoint for epoch 913: ./checkpoints/demo/comp3-913\n",
      "Epoch 915, gen_loss: -6.7539, disc_loss: -2.8079\n",
      "Time for epoch 915 is 65.4214 sec\n",
      "Saved checkpoint for epoch 914: ./checkpoints/demo/comp3-914\n",
      "Epoch 916, gen_loss: 6.7216, disc_loss: -2.6374\n",
      "Time for epoch 916 is 68.1184 sec\n",
      "Saved checkpoint for epoch 915: ./checkpoints/demo/comp3-915\n",
      "Epoch 917, gen_loss: -4.1996, disc_loss: -2.7211\n",
      "Time for epoch 917 is 55.2382 sec\n",
      "Saved checkpoint for epoch 916: ./checkpoints/demo/comp3-916\n",
      "Epoch 918, gen_loss: -0.7791, disc_loss: -2.7957\n",
      "Time for epoch 918 is 67.4297 sec\n",
      "Saved checkpoint for epoch 917: ./checkpoints/demo/comp3-917\n",
      "Epoch 919, gen_loss: 15.5864, disc_loss: -2.6881\n",
      "Time for epoch 919 is 66.7314 sec\n",
      "Saved checkpoint for epoch 918: ./checkpoints/demo/comp3-918\n",
      "Epoch 920, gen_loss: 6.3627, disc_loss: -2.7010\n",
      "Time for epoch 920 is 65.2912 sec\n",
      "Saved checkpoint for epoch 919: ./checkpoints/demo/comp3-919\n",
      "Epoch 921, gen_loss: 5.5886, disc_loss: -3.0995\n",
      "Time for epoch 921 is 65.7177 sec\n",
      "Saved checkpoint for epoch 920: ./checkpoints/demo/comp3-920\n",
      "Epoch 922, gen_loss: 11.4582, disc_loss: -2.9244\n",
      "Time for epoch 922 is 67.2614 sec\n",
      "Saved checkpoint for epoch 921: ./checkpoints/demo/comp3-921\n",
      "Epoch 923, gen_loss: 14.3450, disc_loss: -2.7181\n",
      "Time for epoch 923 is 61.3414 sec\n",
      "Saved checkpoint for epoch 922: ./checkpoints/demo/comp3-922\n",
      "Epoch 924, gen_loss: 1.7821, disc_loss: -2.7849\n",
      "Time for epoch 924 is 60.9246 sec\n",
      "Saved checkpoint for epoch 923: ./checkpoints/demo/comp3-923\n",
      "Epoch 925, gen_loss: 7.3524, disc_loss: -2.7532\n",
      "Time for epoch 925 is 70.8841 sec\n",
      "Saved checkpoint for epoch 924: ./checkpoints/demo/comp3-924\n",
      "Epoch 926, gen_loss: 3.6901, disc_loss: -2.6038\n",
      "Time for epoch 926 is 69.8346 sec\n",
      "Saved checkpoint for epoch 925: ./checkpoints/demo/comp3-925\n",
      "Epoch 927, gen_loss: -6.0995, disc_loss: -3.0525\n",
      "Time for epoch 927 is 62.4779 sec\n",
      "Saved checkpoint for epoch 926: ./checkpoints/demo/comp3-926\n",
      "Epoch 928, gen_loss: -0.8236, disc_loss: -2.7425\n",
      "Time for epoch 928 is 64.4507 sec\n",
      "Saved checkpoint for epoch 927: ./checkpoints/demo/comp3-927\n",
      "Epoch 929, gen_loss: 8.2292, disc_loss: -2.8273\n",
      "Time for epoch 929 is 64.4399 sec\n",
      "Saved checkpoint for epoch 928: ./checkpoints/demo/comp3-928\n",
      "Epoch 930, gen_loss: -1.2095, disc_loss: -2.7555\n",
      "Time for epoch 930 is 66.7499 sec\n",
      "Saved checkpoint for epoch 929: ./checkpoints/demo/comp3-929\n",
      "Epoch 931, gen_loss: 10.6423, disc_loss: -2.8571\n",
      "Time for epoch 931 is 66.4904 sec\n",
      "Saved checkpoint for epoch 930: ./checkpoints/demo/comp3-930\n",
      "Epoch 932, gen_loss: 8.8749, disc_loss: -2.7452\n",
      "Time for epoch 932 is 65.1786 sec\n",
      "Saved checkpoint for epoch 931: ./checkpoints/demo/comp3-931\n",
      "Epoch 933, gen_loss: 8.9462, disc_loss: -2.7674\n",
      "Time for epoch 933 is 66.9780 sec\n",
      "Saved checkpoint for epoch 932: ./checkpoints/demo/comp3-932\n",
      "Epoch 934, gen_loss: 8.2328, disc_loss: -2.7670\n",
      "Time for epoch 934 is 66.0018 sec\n",
      "Saved checkpoint for epoch 933: ./checkpoints/demo/comp3-933\n",
      "Epoch 935, gen_loss: 3.3432, disc_loss: -2.9706\n",
      "Time for epoch 935 is 61.7093 sec\n",
      "Saved checkpoint for epoch 934: ./checkpoints/demo/comp3-934\n",
      "Epoch 936, gen_loss: -1.9946, disc_loss: -2.7587\n",
      "Time for epoch 936 is 61.1555 sec\n",
      "Saved checkpoint for epoch 935: ./checkpoints/demo/comp3-935\n",
      "Epoch 937, gen_loss: 20.9818, disc_loss: -2.8954\n",
      "Time for epoch 937 is 70.8149 sec\n",
      "Saved checkpoint for epoch 936: ./checkpoints/demo/comp3-936\n",
      "Epoch 938, gen_loss: 6.6144, disc_loss: -2.7534\n",
      "Time for epoch 938 is 68.5364 sec\n",
      "Saved checkpoint for epoch 937: ./checkpoints/demo/comp3-937\n",
      "Epoch 939, gen_loss: -4.0178, disc_loss: -2.7807\n",
      "Time for epoch 939 is 62.3998 sec\n",
      "Saved checkpoint for epoch 938: ./checkpoints/demo/comp3-938\n",
      "Epoch 940, gen_loss: 22.5509, disc_loss: -2.8609\n",
      "Time for epoch 940 is 65.5074 sec\n",
      "Saved checkpoint for epoch 939: ./checkpoints/demo/comp3-939\n",
      "Epoch 941, gen_loss: 11.6666, disc_loss: -2.6309\n",
      "Time for epoch 941 is 65.3040 sec\n",
      "Saved checkpoint for epoch 940: ./checkpoints/demo/comp3-940\n",
      "Epoch 942, gen_loss: 11.6607, disc_loss: -2.6501\n",
      "Time for epoch 942 is 66.7275 sec\n",
      "Saved checkpoint for epoch 941: ./checkpoints/demo/comp3-941\n",
      "Epoch 943, gen_loss: 9.6592, disc_loss: -2.8987\n",
      "Time for epoch 943 is 66.4584 sec\n",
      "Saved checkpoint for epoch 942: ./checkpoints/demo/comp3-942\n",
      "Epoch 944, gen_loss: 7.7650, disc_loss: -2.6688\n",
      "Time for epoch 944 is 65.2191 sec\n",
      "Saved checkpoint for epoch 943: ./checkpoints/demo/comp3-943\n",
      "Epoch 945, gen_loss: 3.6710, disc_loss: -2.8949\n",
      "Time for epoch 945 is 67.7324 sec\n",
      "Saved checkpoint for epoch 944: ./checkpoints/demo/comp3-944\n",
      "Epoch 946, gen_loss: -4.5361, disc_loss: -2.7547\n",
      "Time for epoch 946 is 66.3067 sec\n",
      "Saved checkpoint for epoch 945: ./checkpoints/demo/comp3-945\n",
      "Epoch 947, gen_loss: -0.1103, disc_loss: -2.6469\n",
      "Time for epoch 947 is 61.3828 sec\n",
      "Saved checkpoint for epoch 946: ./checkpoints/demo/comp3-946\n",
      "Epoch 948, gen_loss: 11.6711, disc_loss: -2.7955\n",
      "Time for epoch 948 is 61.3379 sec\n",
      "Saved checkpoint for epoch 947: ./checkpoints/demo/comp3-947\n",
      "Epoch 949, gen_loss: 9.1733, disc_loss: -2.8014\n",
      "Time for epoch 949 is 70.3253 sec\n",
      "Saved checkpoint for epoch 948: ./checkpoints/demo/comp3-948\n",
      "Epoch 950, gen_loss: 13.3200, disc_loss: -2.8500\n",
      "Time for epoch 950 is 68.5380 sec\n",
      "Saved checkpoint for epoch 949: ./checkpoints/demo/comp3-949\n",
      "Epoch 951, gen_loss: 19.4357, disc_loss: -2.6566\n",
      "Time for epoch 951 is 65.6142 sec\n",
      "Saved checkpoint for epoch 950: ./checkpoints/demo/comp3-950\n",
      "Epoch 952, gen_loss: 15.9618, disc_loss: -2.8638\n",
      "Time for epoch 952 is 63.9215 sec\n",
      "Saved checkpoint for epoch 951: ./checkpoints/demo/comp3-951\n",
      "Epoch 953, gen_loss: 11.4943, disc_loss: -2.5829\n",
      "Time for epoch 953 is 64.2626 sec\n",
      "Saved checkpoint for epoch 952: ./checkpoints/demo/comp3-952\n",
      "Epoch 954, gen_loss: -2.1078, disc_loss: -2.6702\n",
      "Time for epoch 954 is 66.5832 sec\n",
      "Saved checkpoint for epoch 953: ./checkpoints/demo/comp3-953\n",
      "Epoch 955, gen_loss: 2.8963, disc_loss: -2.7677\n",
      "Time for epoch 955 is 59.7300 sec\n",
      "Saved checkpoint for epoch 954: ./checkpoints/demo/comp3-954\n",
      "Epoch 956, gen_loss: 13.0263, disc_loss: -2.8221\n",
      "Time for epoch 956 is 64.6608 sec\n",
      "Saved checkpoint for epoch 955: ./checkpoints/demo/comp3-955\n",
      "Epoch 957, gen_loss: 15.7272, disc_loss: -2.7669\n",
      "Time for epoch 957 is 64.2949 sec\n",
      "Saved checkpoint for epoch 956: ./checkpoints/demo/comp3-956\n",
      "Epoch 958, gen_loss: -3.8404, disc_loss: -2.7284\n",
      "Time for epoch 958 is 64.5866 sec\n",
      "Saved checkpoint for epoch 957: ./checkpoints/demo/comp3-957\n",
      "Epoch 959, gen_loss: 9.4873, disc_loss: -2.8536\n",
      "Time for epoch 959 is 66.6634 sec\n",
      "Saved checkpoint for epoch 958: ./checkpoints/demo/comp3-958\n",
      "Epoch 960, gen_loss: 17.4286, disc_loss: -2.7529\n",
      "Time for epoch 960 is 65.8411 sec\n",
      "Saved checkpoint for epoch 959: ./checkpoints/demo/comp3-959\n",
      "Epoch 961, gen_loss: 14.3592, disc_loss: -2.7740\n",
      "Time for epoch 961 is 65.8297 sec\n",
      "Saved checkpoint for epoch 960: ./checkpoints/demo/comp3-960\n",
      "Epoch 962, gen_loss: 6.9229, disc_loss: -2.5532\n",
      "Time for epoch 962 is 67.8081 sec\n",
      "Saved checkpoint for epoch 961: ./checkpoints/demo/comp3-961\n",
      "Epoch 963, gen_loss: 4.0266, disc_loss: -2.7938\n",
      "Time for epoch 963 is 63.8766 sec\n",
      "Saved checkpoint for epoch 962: ./checkpoints/demo/comp3-962\n",
      "Epoch 964, gen_loss: 9.2864, disc_loss: -2.6949\n",
      "Time for epoch 964 is 52.4773 sec\n",
      "Saved checkpoint for epoch 963: ./checkpoints/demo/comp3-963\n",
      "Epoch 965, gen_loss: 10.9881, disc_loss: -2.7212\n",
      "Time for epoch 965 is 51.5212 sec\n",
      "Saved checkpoint for epoch 964: ./checkpoints/demo/comp3-964\n",
      "Epoch 966, gen_loss: 14.4812, disc_loss: -2.8175\n",
      "Time for epoch 966 is 51.9420 sec\n",
      "Saved checkpoint for epoch 965: ./checkpoints/demo/comp3-965\n",
      "Epoch 967, gen_loss: 3.3037, disc_loss: -2.7778\n",
      "Time for epoch 967 is 51.9575 sec\n",
      "Saved checkpoint for epoch 966: ./checkpoints/demo/comp3-966\n",
      "Epoch 968, gen_loss: 1.8337, disc_loss: -2.6520\n",
      "Time for epoch 968 is 51.6752 sec\n",
      "Saved checkpoint for epoch 967: ./checkpoints/demo/comp3-967\n",
      "Epoch 969, gen_loss: 9.3488, disc_loss: -2.7823\n",
      "Time for epoch 969 is 51.7296 sec\n",
      "Saved checkpoint for epoch 968: ./checkpoints/demo/comp3-968\n",
      "Epoch 970, gen_loss: 0.3797, disc_loss: -2.6330\n",
      "Time for epoch 970 is 51.9031 sec\n",
      "Saved checkpoint for epoch 969: ./checkpoints/demo/comp3-969\n",
      "Epoch 971, gen_loss: 14.2297, disc_loss: -2.6711\n",
      "Time for epoch 971 is 51.8463 sec\n",
      "Saved checkpoint for epoch 970: ./checkpoints/demo/comp3-970\n",
      "Epoch 972, gen_loss: -6.4158, disc_loss: -2.6125\n",
      "Time for epoch 972 is 51.8768 sec\n",
      "Saved checkpoint for epoch 971: ./checkpoints/demo/comp3-971\n",
      "Epoch 973, gen_loss: -2.1733, disc_loss: -2.8454\n",
      "Time for epoch 973 is 51.6351 sec\n",
      "Saved checkpoint for epoch 972: ./checkpoints/demo/comp3-972\n",
      "Epoch 974, gen_loss: 2.8809, disc_loss: -2.8624\n",
      "Time for epoch 974 is 51.6219 sec\n",
      "Saved checkpoint for epoch 973: ./checkpoints/demo/comp3-973\n",
      "Epoch 975, gen_loss: 5.2463, disc_loss: -2.8202\n",
      "Time for epoch 975 is 51.5365 sec\n",
      "Saved checkpoint for epoch 974: ./checkpoints/demo/comp3-974\n",
      "Epoch 976, gen_loss: 2.3711, disc_loss: -2.7404\n",
      "Time for epoch 976 is 52.0072 sec\n",
      "Saved checkpoint for epoch 975: ./checkpoints/demo/comp3-975\n",
      "Epoch 977, gen_loss: 2.4997, disc_loss: -2.7755\n",
      "Time for epoch 977 is 51.9321 sec\n",
      "Saved checkpoint for epoch 976: ./checkpoints/demo/comp3-976\n",
      "Epoch 978, gen_loss: 10.8005, disc_loss: -2.6520\n",
      "Time for epoch 978 is 51.5057 sec\n",
      "Saved checkpoint for epoch 977: ./checkpoints/demo/comp3-977\n",
      "Epoch 979, gen_loss: 6.6397, disc_loss: -2.8563\n",
      "Time for epoch 979 is 51.7944 sec\n",
      "Saved checkpoint for epoch 978: ./checkpoints/demo/comp3-978\n",
      "Epoch 980, gen_loss: 4.9726, disc_loss: -2.6891\n",
      "Time for epoch 980 is 51.5914 sec\n",
      "Saved checkpoint for epoch 979: ./checkpoints/demo/comp3-979\n",
      "Epoch 981, gen_loss: 7.0534, disc_loss: -2.9366\n",
      "Time for epoch 981 is 51.6857 sec\n",
      "Saved checkpoint for epoch 980: ./checkpoints/demo/comp3-980\n",
      "Epoch 982, gen_loss: -7.9427, disc_loss: -2.7795\n",
      "Time for epoch 982 is 51.7355 sec\n",
      "Saved checkpoint for epoch 981: ./checkpoints/demo/comp3-981\n",
      "Epoch 983, gen_loss: 8.3435, disc_loss: -2.8333\n",
      "Time for epoch 983 is 51.6798 sec\n",
      "Saved checkpoint for epoch 982: ./checkpoints/demo/comp3-982\n",
      "Epoch 984, gen_loss: 12.0442, disc_loss: -2.8982\n",
      "Time for epoch 984 is 51.6593 sec\n",
      "Saved checkpoint for epoch 983: ./checkpoints/demo/comp3-983\n",
      "Epoch 985, gen_loss: -1.6033, disc_loss: -3.1209\n",
      "Time for epoch 985 is 51.6899 sec\n",
      "Saved checkpoint for epoch 984: ./checkpoints/demo/comp3-984\n",
      "Epoch 986, gen_loss: 3.5520, disc_loss: -2.6888\n",
      "Time for epoch 986 is 51.9051 sec\n",
      "Saved checkpoint for epoch 985: ./checkpoints/demo/comp3-985\n",
      "Epoch 987, gen_loss: 1.9410, disc_loss: -2.8446\n",
      "Time for epoch 987 is 51.7615 sec\n",
      "Saved checkpoint for epoch 986: ./checkpoints/demo/comp3-986\n",
      "Epoch 988, gen_loss: 16.8313, disc_loss: -2.6785\n",
      "Time for epoch 988 is 51.7031 sec\n",
      "Saved checkpoint for epoch 987: ./checkpoints/demo/comp3-987\n",
      "Epoch 989, gen_loss: 12.2118, disc_loss: -2.8331\n",
      "Time for epoch 989 is 51.5505 sec\n",
      "Saved checkpoint for epoch 988: ./checkpoints/demo/comp3-988\n",
      "Epoch 990, gen_loss: 16.0350, disc_loss: -2.7109\n",
      "Time for epoch 990 is 51.6680 sec\n",
      "Saved checkpoint for epoch 989: ./checkpoints/demo/comp3-989\n",
      "Epoch 991, gen_loss: 3.2967, disc_loss: -2.6389\n",
      "Time for epoch 991 is 52.0535 sec\n",
      "Saved checkpoint for epoch 990: ./checkpoints/demo/comp3-990\n",
      "Epoch 992, gen_loss: 4.3433, disc_loss: -2.5514\n",
      "Time for epoch 992 is 51.7821 sec\n",
      "Saved checkpoint for epoch 991: ./checkpoints/demo/comp3-991\n",
      "Epoch 993, gen_loss: 5.2459, disc_loss: -2.6949\n",
      "Time for epoch 993 is 51.7708 sec\n",
      "Saved checkpoint for epoch 992: ./checkpoints/demo/comp3-992\n",
      "Epoch 994, gen_loss: 0.3234, disc_loss: -2.7353\n",
      "Time for epoch 994 is 51.7255 sec\n",
      "Saved checkpoint for epoch 993: ./checkpoints/demo/comp3-993\n",
      "Epoch 995, gen_loss: 1.0154, disc_loss: -2.4795\n",
      "Time for epoch 995 is 51.6286 sec\n",
      "Saved checkpoint for epoch 994: ./checkpoints/demo/comp3-994\n",
      "Epoch 996, gen_loss: 1.9637, disc_loss: -2.5748\n",
      "Time for epoch 996 is 51.8817 sec\n",
      "Saved checkpoint for epoch 995: ./checkpoints/demo/comp3-995\n",
      "Epoch 997, gen_loss: -1.3363, disc_loss: -2.7262\n",
      "Time for epoch 997 is 51.8140 sec\n",
      "Saved checkpoint for epoch 996: ./checkpoints/demo/comp3-996\n",
      "Epoch 998, gen_loss: 2.4539, disc_loss: -2.7798\n",
      "Time for epoch 998 is 51.5105 sec\n",
      "Saved checkpoint for epoch 997: ./checkpoints/demo/comp3-997\n",
      "Epoch 999, gen_loss: 5.7310, disc_loss: -2.7155\n",
      "Time for epoch 999 is 51.7249 sec\n",
      "Saved checkpoint for epoch 998: ./checkpoints/demo/comp3-998\n",
      "Epoch 1000, gen_loss: 10.9202, disc_loss: -2.4998\n",
      "Time for epoch 1000 is 51.6928 sec\n",
      "Saved checkpoint for epoch 999: ./checkpoints/demo/comp3-999\n"
     ]
    }
   ],
   "source": [
    "train(dataset, hparas['N_EPOCH'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Dataset\n",
    "If you change anything during preprocessing of training dataset, you must make sure same operations have be done in testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_data_generator(embedding, index):\n",
    "    embedding = tf.cast(embedding, tf.float32)\n",
    "    return embedding, index\n",
    "\n",
    "def testing_dataset_generator(data, batch_size, data_generator):\n",
    "    embeddings = data['embeddings'].values\n",
    "    embedding = []\n",
    "    for i in range(len(embeddings)):\n",
    "        embedding.append(embeddings[i])\n",
    "    embedding = np.asarray(embedding)\n",
    "    embedding = embedding.astype(np.float32)\n",
    "\n",
    "    index = data['ID'].values\n",
    "    index = np.asarray(index)\n",
    "    \n",
    "    assert embedding.shape[0] == index.shape[0]\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((embedding, index))\n",
    "    dataset = dataset.map(data_generator, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.repeat().batch(batch_size)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx2word_test(indices_list):\n",
    "    indices_list = [indices_list]\n",
    "    results_list = []\n",
    "    for indices in indices_list:\n",
    "        string = ''\n",
    "        length_of_string = 0\n",
    "        for idx in indices:\n",
    "            if idx == '5428':\n",
    "                string = string + ''\n",
    "            elif idx == '5427':\n",
    "                break\n",
    "            else:\n",
    "                string = string + id2word_dict[idx] + ' '\n",
    "        results_list.append(string.strip())\n",
    "    return results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('./dataset/testData.pkl')\n",
    "data['texts'] = data['Captions'].apply(lambda x: idx2word_test(x))\n",
    "data['texts'] = data['texts'].apply(lambda x: remove_empty_string(x))\n",
    "data['embeddings'] = data['texts'].apply(lambda x : turn_to_bert_embedding(x))\n",
    "captions = data['Captions'].values\n",
    "\n",
    "NUM_TEST = len(captions)\n",
    "EPOCH_TEST = int(NUM_TEST / hparas['BATCH_SIZE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataset = testing_dataset_generator(data, hparas['BATCH_SIZE'], testing_data_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(dataset):\n",
    "    # hidden = text_encoder.initialize_hidden_state()\n",
    "    hidden = None\n",
    "    sample_size = hparas['BATCH_SIZE']\n",
    "    sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, hparas['Z_DIM'])).astype(np.float32)\n",
    "    \n",
    "    step = 0\n",
    "    start = time.time()\n",
    "    for embedding, idx in dataset:\n",
    "        if step > EPOCH_TEST:\n",
    "            break\n",
    "        \n",
    "        fake_image = test_step(embedding, sample_seed, hidden)\n",
    "        step += 1\n",
    "        for i in range(hparas['BATCH_SIZE']):\n",
    "            plt.imsave('./inference/demo/inference_{:04d}.jpg'.format(idx[i]), fake_image[i].numpy()*0.5 + 0.5)\n",
    "            \n",
    "    print('Time for inference is {:.4f} sec'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint.restore(checkpoint_dir + '/ckpt-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for inference is 0.9761 sec\n"
     ]
    }
   ],
   "source": [
    "inference(testing_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output score.csv\n",
    "CAUTION: \n",
    "* Please modify GPU setting in <i>inception_score.py</i> if need.\n",
    "* Please run the below cmd in command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd testing\n",
    "!python inception_score.py ../inference/demo ../score_demo.csv 39 # BATCH_SIZE=39 is available using GTX 1080 Ti (need 9441MB memory)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5c5b210ffa015f2312f69f2248e3163602cc860559c1494ea467ed1fecf0f25e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
